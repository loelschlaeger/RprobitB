\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% additional packages
\usepackage{amssymb,amsmath}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}



%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Lennart Oelschl\"ager \\Bielefeld University \And Dietmar Bauer\\Bielefeld University}
\Plainauthor{Lennart Oelschl\"ager, Dietmar Bauer}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{\pkg{RprobitB}: Bayes Estimation of Discrete Choice Behavior Heterogeneity via Probit Models in \proglang{R}}
\Plaintitle{RprobitB: Bayes Estimation of Discrete Choice Behavior Heterogeneity via Probit Models in R}
\Shorttitle{RprobitB}

%% - \Abstract{} almost as usual
\Abstract{
\pkg{RprobitB} is an \proglang{R} package for Bayes estimation of probit models with a special focus on modeling choice behavior heterogeneity. In comparison to competing packages it places a focus on approximating the mixing distribution via a latent mixture of Gaussian distributions and thereby providing a classification of deciders. It provides tools for data management, model estimation via Markov Chain Monte Carlo Simulation, diagnostics tools for the Gibbs sampling and a prediction function. This paper demonstrates the functionalities of \pkg{RprobitB} on known choice datasets and compares estimation results across packages.
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{discrete choice, probit models, heterogeneity, Bayes estimation, \proglang{R}}
\Plainkeywords{discrete choice, probit models, heterogeneity, Bayes estimation, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Lennart Oelschl\"ager, Dietmar Bauer\\
  Department of Business Administration and Economics\\
  Bielefeld University\\
  Postfach 10 01 31\\
  E-mail: \email{lennart.oelschlaeger@uni-bielefeld.de}, \email{dietmar.bauer@uni-bielefeld.de}
}

\begin{document}
%% I have no idea what this does. Maybe we need this in the future.
%% \SweaveOpts{concordance=TRUE}

%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, \fct{} and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).

\section{Introduction}
\label{sec:introduction}

% Opening
Many applied research areas seek to understand decision maker's choices among a discrete set of alternatives, for example between different brands (marketing) or options to commute (transportation). Of central interest is heterogeneity in choice behavior: do deciders weight choice attributes like product price or travel time differently? If yes, to what extend? And what groups of deciders share similar preferences? Answering questions of this type is the motivation behind the presented statistical software \pkg{RprobitB} \citep{Oelschlaeger:2021}. The package name is a portmanteau of \proglang{R} (the programming language), the probit model class, and the Bayesian estimation framework.

% The mixed probit model
The probit model is one of the most widely-used statistical models to explain discrete choices.  Heterogeneity in choice behavior can be modeled using mixing distributions for the coefficients. Recently, \cite{Oelschlaeger:2020} proposed a new instrument for approximating the underlying mixing distribution that combines Bayes estimation and semi-parametric methods. This paper presents the implementation of the methodology in the \proglang{R} package \pkg{RprobitB}.

%% Overview RUMs
Traditionally, discrete choice models are interpreted as random utility models, including the multinomial logit (MNL) and the multinomial probit (MNP) model as the most prominent members. The MNL model affords straightforward analysis but suffers from the well-known independence of irrelevant alternatives assumption. In contrast, the MNP model avoids this assumption, which however comes at the price of more complex parameter estimation, cf. \cite{Train:2009}. In their basic form, these models often fail to take into account heterogeneity of individual deciders, cf. \cite{Train:2009}, Chapter 6, or \cite{Train:2016}. A concrete example of heterogeneous preferences is constituted by the value of travel time, cf. \cite{Cirillo:2006}. Modeling heterogeneity in preferences is indispensable in such cases and has been elaborated in both the MNL and the MNP model by imposing mixing distributions on the coefficients, cf. \cite{Train:2009} and \cite{Bhat:2011}.

% How are the mixing distribution specified currently?
Specifying these mixing distributions is an important part of the model selection. In absence of alternatives, it has been common practice so far to try different types of standard parametric distributions (including the normal, log-normal, uniform and tent distribution) and to perform a likelihood value-based model selection, cf. \cite{Train:2009}, Chapter 6. Aiming to capture correlation patterns across parameters, \cite{Fountas:2018} and \cite{Fountas:2019} apply multivariate normal mixing distributions in their probit models, which however comes at the price of imposing the rather strong normality assumption on their parameters.

In order to alleviate these restrictions \cite{Train:2016} proposes a non-parametric approach based on grid methods. Building on the ideas of \cite{Train:2016} and \cite{Bhat:2018} recently \cite{Bauer:2019} introduced procedures for non-parametrically estimating latent class mixed multinomial probit models where the number of classes is chosen iteratively in the algorithm. These procedures have been demonstrated to be useful in reasonable sized cross-sectional data sets. However, for large panel data sets with a significant number of choice occasions per person, the approach is numerically extremely demanding in particular due to its non-parametric nature and has to deal with the curse of dimensionality.

%What is the potential benefit of Bayesian estimation?
In the Bayesian framework \cite{Scaccia:2010} presents the idea to estimate latent class logit models with a fixed prespecified number of Gaussian components. This approach does not require the maximization of the likelihood while at the same time it allows for approximation of the underlying mixing distribution. The same idea has also been applied to probit models, cf. \cite{Xiong:2013} for an analysis of adolescent driver-injury data. In both cases however, the specification of the number of latent classes is based only on a trial-and-error strategy.

%What approach do we suggest to specify the mixing distributions?
Oelschlaeger and Bauer presents a more flexible approach that combines the ideas of a Bayesian framework, approximating the mixing distribution through a mixture of normal distributions and updates on the number of latent classes within the algorithm analogously to \cite{Bauer:2019}. As a consequence, the procedure unites the benefits of a reduced numerical complexity for the estimation compared to the non-parametric likelihood maximization approach and the ability to approximate any mixing distribution. Presenting simulation results on artificial test cases, it is shown that the approach is capable of approximating the underlying mixing distributions and thereby guiding the specification of mixing distributions for real-world applications.

% Comparison to other packages
This packages adds to the line of discrete choice software packages in R in the following way: Its focus is entirely on Bayesian estimation, thereby it differs from the packages Rchoice. Furthermore, it places a focus on modeling choice behaviour heterogeneity by approximating the underlying mixing distribution through a latent mixture of normal distributions. The method is explained in detail in Oelschlaeger and Bauer.

\begin{table}[!h]
\centering
\begin{tabular}{l|cccccccc}
               & Probit      & Logit      & Bayes      & ML         & Ord.       & Mix.       & LC           & upd. LC    \\ \hline
\pkg{Rchoice}  & \checkmark  & \checkmark &            & \checkmark & \checkmark & \checkmark &              &            \\
\pkg{mlogit}   & \checkmark  & \checkmark &            & \checkmark & \checkmark & \checkmark &              &            \\
\pkg{Biogeme}  & \checkmark  & \checkmark &            & \checkmark & \checkmark & \checkmark & \checkmark   &            \\
\pkg{apollo}   & \checkmark  & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark   &            \\
\pkg{bayesm}   & \checkmark  & \checkmark & \checkmark &            & \checkmark & \checkmark &              &            \\
\pkg{MNP}      & \checkmark  &            & \checkmark &            & \checkmark &            &              &            \\
\pkg{RprobitB} & \checkmark  &            & \checkmark &            &            & \checkmark & \checkmark   & \checkmark \\
\end{tabular}
\label{tab:pkg_overview}
\caption{Overview of packages for discrete choice modeling.}
\end{table}

% Examples
\begin{table}[!h]
\centering
\begin{tabular}{l|p{10cm}}
Example                     & Illustrated package functionalities \\ \hline
1: Train                    & \fct{prepare\_data}, \fct{fit\_model}, \fct{predict}, \fct{model\_selection} \\
2: Simulated choices        & \fct{simulate\_choices}, estimation and weight-based update of latent classes \\
3: Electricity              & estimation and interpretation of random effects \\
4: Online chess strategy    & Dirichlet process-based update of latent classes, preference classification \\
\end{tabular}
\label{tab:example_overview}
\caption{Overview of examples.}
\end{table}

%% Article overview
In this article we present the methodology, give an overview over the functionality of the package and apply the package to data sets. Some of them were already analysed and we aim to reconstruct their findings. In addition, we added two datasets that are especially appropriate for \pkg{RprobitB} in modeling choice behaviour heterogeneits. The first one is a dataset of contraception choice from the German family panel pairfam. It contains repeated observations of males and femals over several years having different social demographics and relationship status choosing different means of contraception. This choice a priori can be considered to be very hetereogenous and dependent on factors not directly observable by the researcher. The second application deals with the opening choice of chess players depending on their and their openents playing strenght measured in the popular measure system Elo, their gender and nationality. Like the contraception example, this choice a priori can be considers to depend on psychological factors that are not directly observable by the researcher. By applying the functionality of this package we demonstrate how we are able to classify the players into different categories of playing style.

1. With \pkg{RprobitB}, you can model the choices made by deciders among a discrete set of alternatives. For example, think of tourists that want to book a flight to their holiday destination. The knowledge why they prefer a certain route over another is of great value for airlines, especially the customer's willingness to pay for say a faster or more comfortable flight alternative.

2. Different deciders value different choice attributes differently. For example, it is imaginable that business people place a higher value on flight time and are willing to pay more for a faster route alternative than vacationers. Such choice behavior heterogeneity can be addressed by \pkg{RprobitB}. Furthermore, the package enables to identify groups of deciders that share similar preferences.

3. Finally, the package enables prediction of choice behavior when certain choice attributes change, for example the proportion of customers who will choose the competitor's product in the event of a price increase.

The functions of \pkg{RprobitB} can be grouped into ones for data management, model fitting, and model evaluation, see the flowchart below. The package can be used for two different purposes: (a) estimation of a model for given data and (b) estimation of a model for simulated data. Simulation typically serves to assess the properties of estimation algorithms either for research or in a bootstrap like fashion. \pkg{RprobitB} supports these functions.

\begin{figure}[t!]
  \includegraphics{flowchart.png}
  \caption{Package flowchart with main functions as rectangles and objects as ovals.}
  \label{fig:flowchart}
\end{figure}


\section{The probit model} \label{sec:probit_model}

The probit model is a regression-type model where the dependent variable takes a finite number of values and the error term is normally distributed \citep{Agresti:2015}. Its most popular application are discrete choice scenarios. The dependent variable in this case is one of finitely many and mutually exclusive alternatives, and explanatory variables typically are characteristics of the deciders or the alternatives. This section defines the probit model based on the concept of latent utilities, outlines a model extension for incorporating choice behavior heterogeneity, and discusses required normalization for parameter identification.

\subsection{Latent utilities} \label{subsec:latent_utilities}

Assume that we know the choices of $N$ deciders choosing between $J \geq 2$ alternatives at each of $T$ choice occasions.\footnote{For notational simplicity, the number of choice occasions $T$ is assumed to be the same for each decision maker here. However, \pkg{RprobitB} allows for unbalanced panels, i.e. varying $T$. Of course, the cross-sectional case $T = 1$ is possible.} Specific to each decision maker, alternative and choice occasion, we furthermore observe $P$ covariates. We seek to explain the choices by a linear combination of those. However, the linear combination is continuous and cannot be linked directly to the discrete choices. Hence, we extend the model by latent variables $U_{ntj}$ for each decider $n$, choice occasion $t$, and alternative $j$. In the discrete choice setting, these variables can be interpreted as the decider's utility of a certain alternative at a certain choice occasion. Formally:

\begin{equation}
  \label{eq:utility}
  U_{ntj} = X_{ntj}'\beta + \epsilon_{ntj}
\end{equation}

for $n=1,\dots,N$, $t=1,\dots,T$ and $j=1,\dots,J$. Here, $X_{ntj}$ is a (column) vector of $P$ characteristics of $j$ as faced by $n$ at $t$, $\beta \in {\mathbb R}^{P}$ is a vector of coefficients, and $(\epsilon_{nt:}) = (\epsilon_{nt1},\dots,\epsilon_{ntJ})' \sim \text{MVN}_{J} (0,\Sigma)$ is the model's error term vector for $n$ at $t$, which in the probit model is assumed to be multivariate normally distributed with zero mean and covariance matrix $\Sigma$.\footnote{The assumption about the error term distribution distinguishes the probit from the logit model. In the latter, each $\epsilon_{ntj}$ is assumed to be independently extreme value distributed.}

Assuming utility maximizing behavior of the decision makers\footnote{We note that utility maximizing behavior is a common assumption in econometric models. However, many studies have shown that humans do not decide in this rational sense in general, see for example \cite{Hewig:2011}.}, we link the latent utilities to the choices via
\begin{align*}
   y_{nt} = \operatorname*{argmax}_{j = 1,\dots,J} U_{ntj}.
\end{align*}
Here, $y_{nt}=j$ denotes the event that decider $n$ chooses $j$ at occasion $t$.

\subsection{Choice behavior heterogeneity} \label{subsec:heterogeneity}

Note that the coefficient vector $\beta$ in equation \eqref{eq:utility} is constant across decision makers (a so-called fixed effect). This assumption is too restrictive for many applications\footnote{As an example, consider the choice of a means of transportation: It is easily imaginable that business people and pensioners do not share the same sensitivities towards cost and time.} and can be relaxed by imposing a distribution on $\beta$, a so-called mixing distribution \citep{Train:2009}. Now, each decider $n$ can have their own sensitivities $\beta_n$ as a realization from the underlying mixing distribution. To allow for a combination of such random effects next to fixed effects, we replace $\beta$ in in equation \eqref{eq:utility} by $\beta = (\alpha, \beta_n)$, where $\alpha$ are $P_f$ coefficients that are constant across deciders and $\beta_n$ are $P_r$ decider specific coefficients, $P_f + P_r = P$. Now if $P_r>0$, $\beta_n$ is distributed according to some $P_r$-variate mixing distribution.

Choosing an appropriate mixing distribution is a notoriously difficult task of the model specification. In many applications, different types of standard parametric distributions (including the normal, log-normal, uniform and tent distribution) are tried in conjunction with a likelihood value-based model selection \citep{Train:2009}. Instead, \pkg{RprobitB} implements the approach of \cite{Oelschlaeger:2020} to approximate any underlying mixing distribution by a mixture of (multivariate) Gaussian densities. More precisely, the underlying mixing distribution for the random coefficients $(\beta_n)_{n}$ is approximated by a mixture of $P_r$-variate normal densities $\phi_{P_r}$ with mean vectors $b=(b_c)_{c}$ and covariance matrices $\Omega=(\Omega_c)_{c}$ using $C$ components:

\begin{align*}
\beta_n\mid b,\Omega \sim \sum_{c=1}^{C} s_c \phi_{P_r} (\cdot \mid b_c,\Omega_c).
\end{align*}

Here, $(s_c)_{c}$ are weights satisfying $0 < s_c\leq 1$ for $c=1,\dots,C$ and $\sum_c s_c=1$. One interpretation of the latent class model is obtained by introducing variables $z=(z_n)_n$, allocating each decision maker $n$ to class $c$ with probability $s_c$, i.e.

\begin{align*}
\text{Prob}(z_n=c)=s_c \land \beta_n \mid z,b,\Omega \sim \phi_{P_r}(\cdot \mid b_{z_n},\Omega_{z_n}).
\end{align*}

This interpretation enables a classification of the deciders in terms of their preferences, see Section \ref{subsec:dp_update} for an example.

\subsection{Model normalization} \label{subsec:normalization}

Any utility model is invariant towards the level and the scale of utility, as \cite{Train:2009} points out. For identification of the model parameters, we therefore normalize the model by considering only utility differences and fixing one error term variance. Formally, equation \eqref{eq:utility} is transformed to

\begin{align*}
\tilde{U}_{ntj} = \tilde{X}_{ntj}' \beta + \tilde{\epsilon}_{ntj},
\end{align*}

$n=1,\dots,N$, $t=1,\dots,T$, and $j=1,\dots,J-1$. Here (choosing $J$ as the reference alternative), $\tilde{U}_{ntj} = U_{ntj} - U_{ntJ}$, $\tilde{X}_{ntj} = X_{ntj} - X_{ntJ}$, and $\tilde{\epsilon}_{ntj} = \epsilon_{ntj} - \epsilon_{ntJ}$, where $(\tilde{\epsilon}_{nt:}) = (\tilde{\epsilon}_{nt1},...,\tilde{\epsilon}_{nt(J-1)})' \sim \text{MVN}_{J-1} (0,\tilde{\Sigma})$ and $\tilde{\Sigma}$ denotes a covariance matrix with one diagonal element fixed to a positive number.\footnote{Restricting an element of $\tilde{\Sigma}$ determines the utility scale. Fixing one fixed effect $\alpha$ serves the same purpose. Both alternatives are implemented in \pkg{RprobitB}, see Section \ref{subsec:estimation_routine}.}

\section{Choice data} \label{sec:choice_data}

Choice data typically consist of the observed choices in conjunction with choice characteristics.\footnote{We only consider unordered alternatives (that is, alternatives cannot be ranked). Every decider may take one or repeated choices (called choice occasions).} \pkg{RprobitB} requests that data sets are (a) of class \class{data.frame} and (b) in wide format (that means each row provides the full information for one choice occasion), (c) contain a column with unique identifiers for each decision maker\footnote{Additionally, it can contain a column with identifier for each choice occasion of each decider.}, (d) contain a column with the observed choices (required for model fitting but not for prediction), and (e) contain columns for the values of alternative and/or decider specific covariates. The underlying set of choice alternatives is assumed to be mutually exclusive (one can choose one and only one alternative that are all different), exhaustive (the alternatives do not leave other options open), and finite \citep{Train:2009}.

This section introduces the formula framework of \pkg{RprobitB} for specifying the set of covariates entering a model. The framework is adapted from \pkg{mlogit}, which is flexible enough to allow for different types of covariates: covariates that are constant across alternatives (e.g. the decider's age), covariates that are alternative specific (e.g. the alternative's price), covariates with a generic coefficient (e.g. paying the same amount of money for train company A versus B should make no difference), and covariates that have alternative specific coefficients (e.g. spending time in a crowded train versus a private jet makes a difference). Subsequently, we demonstrate how to pass such a formula to the functions \fct{prepare\_data} for preparing empirical data for estimation and \fct{simulate\_choices} for simulating choice data.

\subsection{Formula framework} \label{subsec:formula}

We generalize equation \eqref{eq:utility} to allow for different types of covariates:

\begin{align}
  \label{eq:utility_gen}
  U_{ntj} = A_{ntj} \beta_1 + B_{nt} \beta_{2j} + C_{ntj} \beta_{3j} + \epsilon_{ntj}.
\end{align}

Here, the covariates $A$ and $C$ depend on the alternative, whereas $B$ is only choice occasion specific. The coefficient $\beta_1$ is generic (i.e. the same for each alternative), whereas $\beta_{2j}$ and $\beta_{3j}$ are alternative specific. Note that the full collection $(\beta_{2j})_{j=1,\dots,J}$ is not identified. This is because we took utility differences for model normalization, see Section \ref{subsec:normalization}. We therefore fix $\beta_{2k}$ to 0 for one alternative $k$ (the so-called base alternative). The coefficients $(\beta_{2j})_{j\neq k}$ have to be interpreted with respect to alternative $k$.

Equation \eqref{eq:utility_gen} can be entered into \proglang{R} via specifying the \class{formula} object \code{choice ~ A | B | C}, where \code{choice} is the name of the dependent variable (the discrete choice we aim to explain). By default, alternative specific constants (ASCs)\footnote{ASCs capture the average effect on utility of all factors that are not included in the model. We cannot estimate ASCs for all the alternatives due to identifiability. Therefore, they are added for all except for the base alternative.} are added to the model. They can be removed by adding \code{+ 0} in the second spot, e.g. \code{choice ~ A | B + 0 | C}. To exclude covariates of the backmost categories, use either \code{0}, e.g. \code{choice ~ A | B | 0} or just leave this part out and write \code{choice ~ A | B}. However, to exclude covariates of front categories, we have to use \code{0}, e.g. \code{choice ~ 0 | B}. To include more than one covariate of the same category, use \code{+}, e.g. \code{choice ~ A1 + A2 | B}. If we don't want to include any covariates of the second category but want to estimate ASCs, add \code{1} in the second spot, e.g. \code{choice ~ A | 1 | C}. The expression \code{choice ~ A | 0 | C} is interpreted as no covariates of the second category and no alternative specific constants.

\subsection{Preparing data for estimation} \label{subsec:prepare_data}

Before model estimation, any choice data set \code{choice\_data} must pass the \fct{prepare\_data} function together with a formula object \code{form} introduced above:

\begin{Schunk}
\begin{Sinput}
> data <- prepare_data(form = form, choice_data = choice_data)
\end{Sinput}
\end{Schunk}

The function performs compatibility checks and data transformations and returns an object of class \class{RprobitB\_data} that can be fed into the estimation routine \fct{fit\_model} (introduced in Section \ref{sec:model_fitting}). The following arguments of \fct{prepare\_data} are optional:
\begin{itemize}
  \item \code{re}: A character vector of covariate names in \code{form} with random effects (see Section \ref{subsec:heterogeneity}). Per default \code{re = NULL}, i.e. no random effects. To have random effects for the ASCs, include \code{"ASC"} in \code{re}.
  \item \code{alternatives}: A character vector of alternative names, defining the choice set. If not specified, all alternatives chosen in the data set are considered.
  \item \code{base\_alternative}: One element of \code{alternatives} specifying the base alternative (see Section \ref{subsec:formula}). Per default, \code{base\_alternative} is the last element of \code{alternatives}.
  \item \code{id} and \code{idc}: The names of the columns in \code{choice_data} that contain unique identifier for each decision maker and for each choice occasion, respectively. Per default, \code{id = "id"} and \code{idc = NULL}, in which case the choice occasion identifier are generated by the appearance of the choices in the data set.
  \item \code{standardize}: A character vector of variable names of \code{form} that get standardized (i.e. rescaled to have a mean of 0 and a standard deviation of 1). Per default, no covariate is standardized.
  \item \code{impute}: A character, specifying how to handle missing data entries. Options are \code{"complete\_cases"} (removing rows that contain missing entries, which is the default behavior), \code{"zero\_out"} (replacing missing entries by 0), and \code{"mean"} (imputing missing entries by the covariate mean).
\end{itemize}

\paragraph{Example 1: Train.}

The \pkg{mlogit} package contains the choice data set \code{Train}, which we use to demonstrate the \fct{prepare\_data} function. The example is continued in Section \ref{subsec:estimation_routine} for a demonstration of the estimation routine. The data set contains 2929 stated choices of 235 deciders between two fictional train trip alternatives \code{A} and \code{B}. The trip alternatives are characterized by their \code{price}, the travel \code{time}, the level of \code{comfort} (the lower the value the higher the comfort), and the number of \code{change}s. The data is in wide format; the columns \code{id} and \code{choiceid} identify the deciders and the choice occasions, respectively; the column \code{choice} provides the choices. For convenience, we transform \code{time} from minutes to hours and \code{price} from guilders to euros:

\begin{Schunk}
\begin{Sinput}
> data("Train", package = "mlogit")
> Train$price_A <- Train$price_A / 100 * 2.20371
> Train$price_B <- Train$price_B / 100 * 2.20371
> Train$time_A <- Train$time_A / 60
> Train$time_B <- Train$time_B / 60
> str(Train)
\end{Sinput}
\begin{Soutput}
'data.frame':	2929 obs. of  11 variables:
 $ id       : int  1 1 1 1 1 1 1 1 1 1 ...
 $ choiceid : int  1 2 3 4 5 6 7 8 9 10 ...
 $ choice   : Factor w/ 2 levels "A","B": 1 1 1 2 2 2 2 2 1 1 ...
 $ price_A  : num  52.9 52.9 52.9 88.1 52.9 ...
 $ time_A   : num  2.5 2.5 1.92 2.17 2.5 ...
 $ change_A : num  0 0 0 0 0 0 0 0 0 0 ...
 $ comfort_A: num  1 1 1 1 1 0 1 1 0 1 ...
 $ price_B  : num  88.1 70.5 88.1 70.5 70.5 ...
 $ time_B   : num  2.5 2.17 1.92 2.5 2.5 ...
 $ change_B : num  0 0 0 0 0 0 0 0 0 0 ...
 $ comfort_B: num  1 1 0 0 0 0 1 0 1 0 ...
\end{Soutput}
\end{Schunk}

The naming convention \code{<covariate>\_<alternative>} in the \code{Train} data set for alternative specific covariates is the required format for \pkg{RprobitB}. Say we want to include all of them in our probit model, connect them to generic coefficients, and exclude ASCs. Then we would specify the formula

\begin{Schunk}
\begin{Sinput}
> form <- choice ~ price + time + comfort + change | 0
\end{Sinput}
\end{Schunk}

Passing \code{form} to \fct{prepare\_data} returns an \class{RprobitB\_data} object. The object can be inspected via its \fct{summary} and \fct{plot} methods:

\begin{Schunk}
\begin{Sinput}
> data_train <- prepare_data(
+    form = form,
+    choice_data = Train,
+    id = "id",
+    idc = "choiceid"
+    )
> summary(data_train)
\end{Sinput}
\begin{Soutput}
  number deciders choice occasions choices total
1             235     5 to 19 each          2929

  alternative frequency
1           A      1474
2           B      1455

   effect as_value as_coef random
1   price     TRUE   FALSE  FALSE
2    time     TRUE   FALSE  FALSE
3 comfort     TRUE   FALSE  FALSE
4  change     TRUE   FALSE  FALSE
\end{Soutput}
\end{Schunk}

Above, \code{as\_value}, \code{as\_coef}, and \code{random} are booleans for each effect, indicating whether the covariate is alternative specific, the coefficient is alternative specific, and the effect is random, respectively.

\begin{Schunk}
\begin{Sinput}
> plot(data_train)
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-train-data}

\subsection{Simulating choice data} \label{subsec:simulate_choice_data}

The \fct{simulate\_choices} function simulates discrete choice data from a probit model. Say we want to simulate the choices of \code{N} deciders in \code{T} choice occasions\footnote{\code{T} can be either a positive number, representing a fixed number of choice occasions for each decision maker, or a vector of length \code{N} with decision maker specific numbers of choice occasions} among \code{J} alternatives. Together with a model formula \code{form}, we would have to call

\begin{Schunk}
\begin{Sinput}
> data <- simulate_choices(form = form, N = N, T = T, J = J)
\end{Sinput}
\end{Schunk}

The function \fct{simulate\_choices} has the following optional arguments:
\begin{itemize}
  \item \code{re}: Analogue to \fct{prepare\_data}.
  \item \code{alternatives}: A character vector of length \code{J} with the names of the choice alternatives. If not specified, the alternatives are labeled by the first \code{J} upper-case letters of the Roman alphabet.
  \item \code{base\_alternative}: Analogue to \fct{prepare\_data}.
  \item \code{covariates}: A named list of covariate values. Each element must be a vector of length equal to the number of choice occasions and named according to a covariate, or follow the naming convention for alternative specific covariates, i.e. \code{<covariate>\_<alternative>}. Covariates for which no values are specified are drawn from a standard normal distribution.
  \item \code{standardize}: Analogue to \fct{prepare\_data}.
  \item \code{seed}: Optionally set a seed for the simulation.
\end{itemize}

True model parameters are set at random per default or can be specified via the function's ellipsis argument:
\begin{itemize}
  \item a numeric vector \code{alpha} with the fixed effects,
  \item an integer \code{C}, the number (greater or equal 1) of latent classes of decision makers (\code{C = 1} per default),
  \item a numeric vector \code{s} of length \code{C} with the class weights,
  \item a matrix \code{b} with the class means as columns,
  \item a matrix \code{Omega} with the class covariance matrices as columns,
  \item a matrix \code{Sigma}, the differenced error term covariance matrix, or \code{Sigma\_full}, the full error term covariance matrix,
  \item a matrix \code{beta} with the decision-maker specific coefficient vectors as columns,
  \item a numeric vector \code{z} of length \code{N} with elements in \code{1:C}, representing the class allocations.
\end{itemize}

\paragraph{Example 2: Simulated choices.} For illustration, we simulate the choices of \code{N = 100} deciders at \code{T = 30} choice occasions between the fictitious alternatives \code{alt1} and \code{alt2}. The choices are explained by the alternative specific covariates \code{var1} and \code{var3}. Covariate \code{var2} is only choice occasion specific but connected to a random effect, as well as the ASCs:

\begin{Schunk}
\begin{Sinput}
> N <- 100
> T <- 30
> alternatives <- c("alt1", "alt2")
> base_alternative <- "alt2"
> form <- choice ~ var1 | var2 | var3
> re <- c("ASC","var2")
\end{Sinput}
\end{Schunk}

\pkg{RprobitB} provides the function \fct{overview\_effects}, which can be used to get an overview of the effects for which parameters can be specified:

\begin{Schunk}
\begin{Sinput}
> overview_effects(
+    form = form,
+    re = re,
+    alternatives = alternatives,
+    base_alternative = base_alternative
+  )
\end{Sinput}
\begin{Soutput}
     effect as_value as_coef random
1      var1     TRUE   FALSE  FALSE
2 var3_alt1     TRUE    TRUE  FALSE
3 var3_alt2     TRUE    TRUE  FALSE
4 var2_alt1    FALSE    TRUE   TRUE
5  ASC_alt1    FALSE    TRUE   TRUE
\end{Soutput}
\end{Schunk}

The model has three fixed effects (\code{random = FALSE}), hence the vector \code{alpha} must be of length 3, where the elements 1 to 3 correspond to \code{var1}, \code{var3\_alt1}, and \code{var3\_alt2}, respectively. Additionally, the model has two fixed effects (\code{random = TRUE}), hence the matrix \code{b} must be of dimension \code{2 x C}, where row 1 and 2 correspond to \code{var2\_alt1} and \code{ASC\_alt1}, respectively. We specify \code{C = 2} latent classes in the data generating process with the intention, that this example will serve as an illustration of the estimation of latent classes in Section \ref{subsec:weight_update}.

\begin{Schunk}
\begin{Sinput}
> data_sim <- simulate_choices(
+    form = form,
+    N = N,
+    T = T,
+    J = 2,
+    re = re,
+    alternatives = alternatives,
+    base_alternative = base_alternative,
+    seed = 1,
+    alpha = c(-1,0,1),
+    C = 2,
+    s = c(0.7,0.3),
+    b = matrix(c(2,-0.5,1,1), ncol = 2),
+    Sigma = 1
+  )
\end{Sinput}
\end{Schunk}

The \fct{plot} method of \class{RprobitB\_data} object has the optional argument \code{by\_choice}. Setting \code{by\_choice = TRUE} visualizes the (randomly drawn) covariates grouped by the chosen alternatives:

\begin{Schunk}
\begin{Sinput}
> plot(data_sim, by_choice = TRUE)
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-sim-data}

The graphic is consistent with our model specification: for example, covariate \code{var1} was specified to have a negative effect on \code{alt1}, because the coefficient of \code{var1} (the first value of \code{alpha}) is negative $(-1)$. Hence higher values of \code{var1\_alt1} correspond more frequently to choice \code{alt2} (upper-right panel).

\subsection{Train and test data set} \label{subsec:train_test}

\pkg{RprobitB} provides the function \fct{train\_test} that can split an \class{RprobitB\_data} object (i.e. the output of \fct{prepare\_data} or \fct{simulate\_choices}) in two parts: a train subset and a test subset. The model then is typically estimated on the train subset and applied to explain the choices in the test subset. This procedure can be useful for evaluating how the model generalizes to unseen data. For example, the following call puts 70\% of deciders from \code{data_sim} into the train subsample and 30\% of deciders into the test subsample:

\begin{Schunk}
\begin{Sinput}
> train_test(data_sim, test_proportion = 0.3, by = "N")
\end{Sinput}
\begin{Soutput}
$train
Simulated data of 2100 choices.

$test
Simulated data of 900 choices.
\end{Soutput}
\end{Schunk}

Alternatively, the following line puts 2 randomly chosen choice occasions per decider from \code{data\_sim} into the test subsample, the rest goes into the train subsample:

\begin{Schunk}
\begin{Sinput}
> train_test(data_sim, test_number = 2, by = "T", random = TRUE, seed = 1)
\end{Sinput}
\begin{Soutput}
$train
Simulated data of 2800 choices.

$test
Simulated data of 200 choices.
\end{Soutput}
\end{Schunk}

\section{Model fitting} \label{sec:model_fitting}

\pkg{RprobitB} estimates the probit model in a Bayesian framework. The framework builds upon the work of \cite{McCulloch:1994}, \cite{Nobile:1998}, \cite{Allenby:1998}, and \cite{Imai:2005}. A key ingredient is the concept of data augmentation \citep{Albert:1993}, which treats the latent utilities in model equation \eqref{eq:utility} as additional parameters. Then, conditional on these parameters, the probit model constitutes a standard Bayesian linear regression set-up. Its posterior distribution can be approximated by iteratively drawing and updating each model parameter conditional on the other parameters (the so-called Gibbs sampling approach).

This section lists the prior distributions for the model parameters and formulates the conditional posterior distributions. We subsequently introduce the estimation routine \fct{fit\_model} and apply it to the two examples from the previous section. The remainder of this section presents how \pkg{RprobitB} estimates latent classes and updates their number either weight-based or via a Dirichlet process.

\subsection{Prior and posterior distributions} \label{subsec:prior_and_posterior}

This section follows closely \cite{Oelschlaeger:2020}. We a priori assume the following (conjugate) parameter distributions:
\begin{itemize}
  \item $(s_1,\dots,s_C)\sim D_C(\delta)$, where $D_C(\delta)$ denotes the $C$-dimensional Dirichlet distribution with concentration parameter vector $\delta = (\delta_1,\dots,\delta_C)$,
  \item $\alpha\sim \text{MVN}_{P_f}(\psi,\Psi)$, where $\text{MVN}_{P_f}$ denotes the $P_f$-dimensional normal distribution with mean $\psi$ and covariance $\Psi$,
  \item $b_c \sim \text{MVN}_{P_r}(\xi,\Xi)$, independent for all $c$,
  \item $\Omega_c \sim W^{-1}_{P_r}(\nu,\Theta)$, independent for all $c$, where $W^{-1}_{P_r}(\nu,\Theta)$ denotes the $P_r$-dimensional inverse Wishart distribution with $\nu$ degrees of freedom and scale matrix $\Theta$,
  \item and $\Sigma \sim W^{-1}_{J-1}(\kappa,\Lambda)$.
\end{itemize}

These priors imply the following conditional posterior distributions:
\begin{itemize}
  \item The class weights are drawn from the Dirichlet distribution
  \begin{align*}
  (s_1,\dots,s_C)\mid \delta,z \sim D_C(\delta_1+m_1,\dots,\delta_C+m_C),
  \end{align*}
  where for $c=1,\dots,C$, $m_c=\#\{n:z_n=c\}$ denotes the current absolute class size. Mind that the model is invariant to permutations of the class labels $1,\dots,C$. For that reason, we accept an update only if the ordering $s_1>\dots>s_C$ holds, thereby ensuring a unique labeling of the classes.
  \item Independently for all $n$, we update the allocation variables $(z_n)_n$ from their conditional distribution
  \begin{align*}
  \text{Prob}(z_n=c\mid s,\beta,b,\Omega )=\frac{s_c\phi_{P_r}(\beta_n\mid b_c,\Omega_c)}{\sum_c s_c\phi_{P_r}(\beta_n\mid b_c,\Omega_c)}.
  \end{align*}
  \item The class means $(b_c)_c$ are updated independently for all $c$ via
  \begin{align*}
  b_c\mid \Xi,\Omega,\xi,z,\beta \sim\text{MVN}_{P_r}\left( \mu_{b_c}, \Sigma_{b_c}  \right),
  \end{align*}
  where $\mu_{b_c}=(\Xi^{-1}+m_c\Omega_c^{-1})^{-1}(\Xi^{-1}\xi +m_c\Omega_c^{-1}\bar{b}_c)$, $\Sigma_{b_c}=(\Xi^{-1}+m_c\Omega_c^{-1})^{-1}$, and $\bar{b}_c=m_c^{-1}\sum_{n:z_n=c} \beta_n$.
    \item The class covariance matrices $(\Omega_c)_c$ are updated independently for all $c$ via
  \begin{align*}
  \Omega_c \mid \nu,\Theta,z,\beta,b \sim W^{-1}_{P_r}(\mu_{\Omega_c},\Sigma_{\Omega_c}),
  \end{align*}
  where $\mu_{\Omega_c}=\nu+m_c$ and $\Sigma_{\Omega_c}=\Theta^{-1} + \sum_{n:z_n=c} (\beta_n-b_c)(\beta_n-b_c)'$.
  \item Independently for all $n$ and $t$ and conditionally on the other components, the utility vectors $(U_{nt:})$ follow a $J-1$-dimensional truncated multivariate normal distribution, where the truncation points are determined by the choices $y_{nt}$. To sample from a truncated multivariate normal distribution, we apply a sub-Gibbs sampler, following the approach of \cite{Geweke:1998}:
  \begin{align*}
  U_{ntj} \mid U_{nt(-j)},y_{nt},\Sigma,W,\alpha,X,\beta
  \sim \mathcal{N}(\mu_{U_{ntj}},\Sigma_{U_{ntj}}) \cdot \begin{cases}
  1(U_{ntj}>\max(U_{nt(-j)},0) ) & \text{if}~ y_{nt}=j\\
  1(U_{ntj}<\max(U_{nt(-j)},0) ) & \text{if}~ y_{nt}\neq j
  \end{cases},
  \end{align*}
  where $U_{nt(-j)}$ denotes the vector $(U_{nt:})$ without the element $U_{ntj}$, $\mathcal{N}$ denotes the univariate normal distribution, $\Sigma_{U_{ntj}} = 1/(\Sigma^{-1})_{jj}$ and
  \begin{align*}
  \mu_{U_{ntj}} = W_{ntj}'\alpha + X_{ntj}'\beta_n - \Sigma_{U_{ntj}} (\Sigma^{-1})_{j(-j)}   (U_{nt(-j)} - W_{nt(-j)}'\alpha - X_{nt(-j)}' \beta_n ),
  \end{align*}
  where $(\Sigma^{-1})_{jj}$ denotes the $(j,j)$-th element of $\Sigma^{-1}$, $(\Sigma^{-1})_{j(-j)}$ the $j$-th row without the $j$-th entry, $W_{nt(-j)}$ and $X_{nt(-j)}$ the coefficient matrices $W_{nt}$ and $X_{nt}$, respectively, without the $j$-th column.
  \item Updating the fixed coefficient vector $\alpha$ is achieved by applying the formula for Bayesian linear regression of the regressors $W_{nt}$ on the regressands $(U_{nt:})-X_{nt}'\beta_n$, i.e.
  \begin{align*}
  \alpha \mid \Psi,\psi,W,\Sigma,U,X,\beta \sim \text{MVN}_{P_f}(\mu_\alpha,\Sigma_\alpha),
  \end{align*}
  with the definitions $\mu_\alpha = \Sigma_\alpha (\Psi^{-1}\psi + \sum_{n=1,t=1}^{N,T} W_{nt} \Sigma^{-1} ((U_{nt:})-X_{nt}'\beta_n) )$ and $\Sigma_\alpha = (\Psi^{-1} + \sum_{n=1,t=1}^{N,T} W_{nt}\Sigma^{-1} W_{nt}^{'} )^{-1}$.
  \item Analogously to $\alpha$, the random coefficients $(\beta_n)_n$ are updated independently via
  \begin{align*}
  \beta_n \mid \Omega,b,X,\Sigma,U,W,\alpha \sim \text{MVN}_{P_r}(\mu_{\beta_n},\Sigma_{\beta_n}),
  \end{align*}
  $\mu_{\beta_n} = \Sigma_{\beta_n} (\Omega_{z_n}^{-1}b_{z_n} + \sum_{t=1}^{T} X_{nt} \Sigma^{-1} (U_{nt}-W_{nt}'\alpha) )$ and $\Sigma_{\beta_n} = (\Omega_{z_n}^{-1} + \sum_{t=1}^{T} X_{nt}\Sigma^{-1} X_{nt}^{'} )^{-1}$.
    \item The error term covariance matrix $\Sigma$ is updated by means of
  \begin{align*}
  \Sigma \mid \kappa,\Lambda,U,W,\alpha,X,\beta \sim W^{-1}_{J-1}(\kappa+NT,\Lambda+S),
  \end{align*}
  where $S = \sum_{n=1,t=1}^{N,T} \varepsilon_{nt} \varepsilon_{nt}'$ and $\varepsilon_{nt} = (U_{nt:}) - W_{nt}'\alpha - X_{nt}'\beta_n$.
\end{itemize}

Samples obtained from the updating scheme described above lack identification (except for $s$ and $z$ draws), compare Section \ref{subsec:normalization}. Therefore, subsequent to the sampling, we apply the following normalization for the $i$-th update in each iteration $i$:
\begin{itemize}
  \item $\alpha^{(i)} \cdot \omega^{(i)}$,
  \item $b_c^{(i)} \cdot \omega^{(i)}$, $c=1,\dots,C$,
  \item $U_{nt}^{(i)} \cdot \omega^{(i)}$, $n = 1,\dots,N$, $t = 1,\dots,T$,
  \item $\beta_n^{(i)} \cdot \omega^{(i)}$, $n = 1,\dots,N$,
  \item $\Omega_c^{(i)} \cdot (\omega^{(i)})^2$, $c=1,\dots,C$, and
  \item $\Sigma^{(i)} \cdot (\omega^{(i)})^2$,
\end{itemize}
where either $\omega^{(i)} = \sqrt{\text{const} / (\Sigma^{(i)})_{jj}}$ with $(\Sigma^{(i)})_{jj}$ the $j$-th diagonal element of $\Sigma^{(i)}$, $1\leq j \leq J-1$, or alternatively $\omega^{(i)} = \text{const} / \alpha^{(i)}_p$ for some coordinate $1\leq p \leq P_f$ of the $i$-th draw for the coefficient vector $\alpha$. Here, $\text{const}$ is any positive constant (typically 1). The preferences will be flipped if $\omega^{(i)} < 0$, which only is the case if $\alpha^{(i)}_p < 0$.

\subsection{The estimation routine} \label{subsec:estimation_routine}

The Gibbs sampling scheme described above can be executed by applying the function

\begin{Schunk}
\begin{Sinput}
> fit_model(data = data)
\end{Sinput}
\end{Schunk}

where \code{data} is an \class{RprobitB\_data} object (see Section \ref{sec:choice_data}). The function has the following optional arguments:

\begin{itemize}
  \item \code{scale}: A formula object, which determines the utility scale (cf. Section \ref{subsec:normalization}). It is of the form \code{<parameter> ~ <value>}, where \code{<parameter>} is either the name of a fixed effect or \code{Sigma\_<j>} for the \code{<j>}-th diagonal element of \code{Sigma}, and \code{<value>} is the value of the fixed parameter (i.e. $\text{const}$ introduced above). Per default \code{scale = Sigma\_1 ~ 1}, i.e. the first error-term variance is fixed to 1.
  \item \code{R}: The number of iterations of the Gibbs sampler. The default is \code{R = 10000}.
  \item \code{B}: The length of the burn-in period\footnote{The theory behind Gibbs sampling constitutes that the sequence of samples produced by the
updating scheme is a Markov chain with stationary distribution equal to the desired joint posterior distribution. It takes a certain number of iterations for that stationary distribution to be approximated reasonably well. Therefore, it is common practice to discard the first \code{B} out of \code{R} samples (the so-called burn-in period).}, i.e. a non-negative number of samples to be discarded. The default is \code{B = R/2}.
  \item \code{Q}: The thinning factor for the Gibbs samples\footnote{In order to obtain independent Gibbs samples, we consider only every \code{Q}-th sample when computing parameter statistics.}, i.e. only every \code{Q}-th sample is kept. The default is \code{Q = 1}.
  \item \code{print\_progress}: A boolean, determining whether to print the Gibbs sampler progress.
  \item \code{prior}: A named list of parameters for the prior distributions (their default values are documented in the \fct{check\_prior} function, see \code{help(check\_prior, package = "RprobitB")}):
  \begin{itemize}
    \item \code{eta}: The mean vector of length \code{P\_f} of the normal prior for \code{alpha}.
    \item \code{Psi}: The covariance matrix of dimension \code{P\_f x P\_f} of the normal prior for \code{alpha}.
    \item \code{delta}: The concentration parameter of length 1 of the Dirichlet prior for \code{s}.
    \item \code{xi}: The mean vector of length \code{P\_r} of the normal prior for each \code{b\_c}.
    \item \code{D}: The covariance matrix of dimension \code{P\_r} x \code{P\_r} of the normal prior for each \code{b\_c}.
    \item \code{nu}: The degrees of freedom (a natural number greater than \code{P\_r}) of the Inverse Wishart prior for each \code{Omega\_c}.
    \item \code{Theta}: The scale matrix of dimension \code{P\_r} x \code{P\_r} of the Inverse Wishart prior for each \code{Omega\_c}.
    \item \code{kappa}: The degrees of freedom (a natural number greater than \code{J-1}) of the Inverse Wishart prior for \code{Sigma}.
    \item \code{E}: The scale matrix of dimension \code{J-1} x \code{J-1} of the Inverse Wishart prior for \code{Sigma}.
  \end{itemize}
  \item \code{latent\_classes}: A list of parameters specifying the number and the updating scheme of latent classes, see Section \ref{subsec:normal_mix}.
\end{itemize}

\paragraph{Example 1: Train (cont.).}

Recall the \code{Train} data set of stated train trip alternatives, characterized by their \code{price}, \code{time}, number of \code{change}s, and level of \code{comfort}. From this data, we previously build the \class{RprobitB\_data} object \code{data\_train}, which we now pass to the estimation routine \fct{fit\_model}. For model normalization, we fix the coefficient of the \code{price} effect to \code{-1}, which has the advantage that we can interpret the other coefficients as monetary values:

\begin{Schunk}
\begin{Sinput}
> model_train <- fit_model(data = data_train, scale = price ~ -1)
\end{Sinput}
\end{Schunk}

The model is saved in \pkg{RprobitB} and can be accessed via

\begin{Schunk}
\begin{Sinput}
> data(model_train, package = "RprobitB")
\end{Sinput}
\end{Schunk}

The estimated coefficients (using the mean of the Gibbs samples as a point estimate) can be printed via

\begin{Schunk}
\begin{Sinput}
> coef(model_train)
\end{Sinput}
\begin{Soutput}
           Estimate   (sd)
1   price     -1.00 (0.00)
2    time    -25.89 (2.21)
3  change     -4.94 (0.88)
4 comfort    -14.45 (0.86)
\end{Soutput}
\end{Schunk}

and visualized via

\begin{Schunk}
\begin{Sinput}
> plot(coef(model_train), sd = 3)
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-coef-model-train}

The results indicate that the deciders value one hour travel time by about 26 euros, an additional change by 5 euros, and a more comfortable class by 14 euros.\footnote{We note that these results are consistent with the ones that are presented in a vignette of the \pkg{mlogit} package on the same data set but using the logit model.} The Gibbs samples are saved in list form in the \class{RprobitB\_fit} object at the entry \code{"gibbs\_samples"}, i.e.

\begin{Schunk}
\begin{Sinput}
> str(model_train$gibbs_samples, max.level = 2, give.attr = FALSE)
\end{Sinput}
\begin{Soutput}
List of 2
 $ gibbs_samples_raw:List of 2
  ..$ alpha: num [1:10000, 1:4] -0.00239 -0.02179 -0.02986 -0.03343 -0.03584 ...
  ..$ Sigma: num [1:10000, 1] 0.978 0.949 0.862 0.864 0.911 ...
 $ gibbs_samples_nbt:List of 2
  ..$ alpha: num [1:500, 1:4] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
  ..$ Sigma: num [1:500, 1] 649 541 662 691 623 ...
\end{Soutput}
\end{Schunk}

This object contains 2 elements: \code{gibbs\_samples\_raw} is a list of the raw samples from the Gibbs sampler and \code{gibbs\_samples\_nbt} are the transformed Gibbs samples used for parameter estimates (i.e. the normalized and thinned Gibbs samples after the burn-in).

Calling the \fct{summary} method on the estimated \class{RprobitB\_fit} object yields additional information about the (transformed) Gibbs samples. The method receives a list \code{FUN} of functions that compute any point estimate of the Gibbs samples, for example
\begin{itemize}
  \item \fct{mean} for the arithmetic mean,
  \item \fct{stats::sd} for the standard deviation,
  \item \fct{R\_hat} for the Gelman-Rubin statistic \citep{Gelman:1992}\footnote{A Gelman-Rubin statistic close to 1 indicates that the chain of Gibbs samples converged to the stationary distribution.},
  \item or custom statistics like the absolute difference between the median and the mean as an indicator for the symmetry of the posterior distribution.
\end{itemize}

\begin{Schunk}
\begin{Sinput}
> summary(model_train,
+          FUN = c("mean" = mean, "sd" = stats::sd, "R^" = RprobitB::R_hat,
+                  "custom_stat" = function(x) abs(mean(x) - median(x))))
\end{Sinput}
\begin{Soutput}
Probit model
choice ~ price + time + change + comfort | 0 
R: 10000 
B: 5000 
Q: 10 

Utility normalization
Level: Utility differences with respect to alternative 'B'.
Scale: Coefficient of effect 'price' (alpha_1) fixed to -1.

Gibbs sample statistics
               mean           sd           R^  custom_stat
 alpha
                                                          
     1        -1.00         0.00         1.00         0.00
     2       -25.89         2.21         1.00         0.02
     3        -4.94         0.88         1.01         0.01
     4       -14.45         0.86         1.00         0.02

 Sigma
                                                          
   1,1       655.56        65.79         1.00         7.28
\end{Soutput}
\end{Schunk}

Calling the \fct{plot} method with the additional argument \code{type = "trace"} plots the trace of the transformed and thinned Gibbs samples after the burn-in:

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1,2))
> plot(model_train, type = "trace")
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-model-train-trace}

Additionally, we can visualize the serial correlation of the Gibbs samples via the argument \code{type = "acf"} (here exemplary for \code{alpha\_4} and \code{Sigma\_1,1}):

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1,2))
> plot(model_train, type = "acf", ignore = c("alpha_1", "alpha_2", "alpha_3"))
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-model-train-acf}

The boxes in the top-right corner state the total sample size TSS, given by (\code{R} - \code{B}) / \code{Q}, the effective sample size ESS, and the factor by which TSS is larger than ESS. The effective sample size is the value $\text{TSS} / (1 + 2\sum_{k\geq 1} \rho_k)$, where $\rho_k$ is the $k$-th order auto correlation of the Gibbs samples \citep{Marin:2014}. The auto correlations are estimated via the \fct{stats::acf} function.

In our example, the presented diagnosing tools (Gelman-Rubin statistic, trace plot, autocorrelation plot) indicate a satisfying quality of the Gibbs samples. If that would not the case, the \fct{transform} method can be used to increase the length of the burn-in (via \code{transform(model\_train, B = B\_new)}) or to increase the thinning factor (via \code{transform(model\_train, Q = Q\_new})).\footnote{The \fct{transform} function can also be used to change the model normalization \code{scale}, for example \code{transform(model\_train, scale = Sigma\_1 ~ 1)}.}

\subsection{Estimating a joint normal mixing distribution} \label{subsec:normal_mix}

This section demonstrates how to estimate a joint normal mixing distribution in \pkg{RprobitB} using another real-data example.

\paragraph{Example 3: Electricity.}

The \pkg{mlogit} package \citep{Croissant:2020} contains the data set \code{Electricity}, in which residential electricity customers were asked to decide between four contract offers of hypothetical electricity suppliers. Heterogeneity in choice behavior is expected here, because customers might value contract characteristics differently based on their living conditions. In particular, the contracts differed in 6 characteristics:

\begin{enumerate}
  \item their fixed price \code{pf} per kilowatt hour,
  \item their contract length \code{cf},
  \item a boolean \code{loc}, indicating whether the supplier is a local company,
  \item a boolean \code{wk}, indicating whether the supplier is a well known company,
  \item a boolean \code{tod}, indicating whether the supplier offers a time-of-day electricity price (which is higher during the day and lower during the night), and
  \item a boolean \code{seas}, indicating whether the supplier's price is seasonal dependent.
\end{enumerate}

The following lines prepare the \code{Electricity} data set for estimation. We first use the convenience function \fct{as\_cov\_names} that relabels the data columns for alternative specific covariates into the required format \code{"<covariate>\_<alternative>"}:

\begin{Schunk}
\begin{Sinput}
> data("Electricity", package = "mlogit")
> Electricity <- as_cov_names(
+    choice_data = Electricity,
+    cov = c("pf","cl","loc","wk","tod","seas"),
+    alternatives = 1:4
+  )
\end{Sinput}
\end{Schunk}

Via the \code{re = c("cl","loc","wk","tod","seas")} argument, we specify that we want to model random effects for all but the price coefficient, which we will fix to \code{-1} to interpret the other estimates as monetary values:

\begin{Schunk}
\begin{Sinput}
> data_elec <- prepare_data(
+    form = choice ~ pf + cl + loc + wk + tod + seas | 0,
+    choice_data = Electricity,
+    re = c("cl","loc","wk","tod","seas")
+  )
> model_elec <- fit_model(data_elec, R = 5000, scale = pf ~ -1)
\end{Sinput}
\end{Schunk}

The estimated model is saved in \pkg{RprobitB} and can be accessed via:

\begin{Schunk}
\begin{Sinput}
> data(model_elec, package = "RprobitB")
\end{Sinput}
\end{Schunk}

Calling the \fct{coef} method on the estimated model returns a table of the average effects and the estimated (marginal) variances of the mixing distribution:

\begin{Schunk}
\begin{Sinput}
> coef(model_elec)
\end{Sinput}
\begin{Soutput}
        Estimate   (sd) Variance   (sd)
1   pf     -1.00 (0.00)       NA   (NA)
2   cl     -0.25 (0.03)     0.23 (0.04)
3  loc      2.77 (0.24)     6.74 (1.19)
4   wk      2.02 (0.19)     3.48 (0.69)
5  tod     -9.70 (0.23)    10.88 (1.80)
6 seas     -9.87 (0.19)     5.90 (1.06)
\end{Soutput}
\end{Schunk}

We can for example deduce, that a longer contract length has a negative effect on average (\code{-0.25}). However, our model shows that 30\% of the customers still prefer to have a longer contract length. This share is estimated by computing the proportion under the mixing distribution that yields a positive coefficient for \code{cl}:

\begin{Schunk}
\begin{Sinput}
> cl_mu <- coef(model_elec)["cl","mean"]
> cl_sd <- sqrt(coef(model_elec)["cl","var"])
> pnorm(cl_mu / cl_sd)
\end{Sinput}
\begin{Soutput}
[1] 0.2996708
\end{Soutput}
\end{Schunk}

Furthermore, the estimated joint mixing distribution allows to infer correlations between effects. They can be extracted via the \fct{cov\_mix} function (setting \code{cor = FALSE} would return the covariances). For example, we see a high correlation between \code{loc} and \code{wk} (deciders that prefer local suppliers also prefer well known companies):

\begin{Schunk}
\begin{Sinput}
> round(cov_mix(model_elec, cor = TRUE), 2)
\end{Sinput}
\begin{Soutput}
        cl  loc    wk   tod  seas
cl    1.00 0.12  0.09 -0.06 -0.14
loc   0.12 1.00  0.81  0.12  0.02
wk    0.09 0.81  1.00  0.11 -0.02
tod  -0.06 0.12  0.11  1.00  0.54
seas -0.14 0.02 -0.02  0.54  1.00
\end{Soutput}
\end{Schunk}

\subsection{Estimating latent classes} \label{subsec:latent_classes}

\pkg{RprobitB} allows to specify a Gaussian mixture $\sum_{c=1}^C \text{MVN} (b_c,\Omega_c)$ as the mixing distribution (cf. Section \ref{subsec:heterogeneity}). This specification allows for a) a better approximation of the true underlying mixing distribution and b) a preference based classification of the deciders. To estimate such a latent mixture, pass the list \code{latent\_classes = list("C" = C)} to \fct{fit\_model}, with \code{C} being the number (greater or equal 1) of latent classes (set to 1 per default).\footnote{We here assume that $C$ is known and fixed. The following Sections \ref{subsec:weight_update} and \ref{subsec:dp_update} present two updating schemes in which $C$ does not need to be pre-specified.}

\paragraph{Example 2: Simulation (cont.).}

We previously simulated the \class{RprobitB\_data} object \code{data\_sim} from a probit model with two latent classes. We now aim to reproduce the model parameters from the data generating process:

\begin{Schunk}
\begin{Sinput}
> model_sim <- fit_model(
+    data = data_sim,
+    R = 1000,
+    latent_classes = list("C" = 2),
+    seed = 1
+  )
> summary(model_sim)
\end{Sinput}
\begin{Soutput}
Probit model
choice ~ var1 | var2 | var3 
R: 1000 
B: 500 
Q: 1 

Utility normalization
Level: Utility differences with respect to alternative 'alt2'.
Scale: Coefficient of the 1. error term variance fixed to 1.

Latent classes
C = 2 

Gibbs sample statistics
          true    mean      sd      R^
 alpha
                                      
     1   -1.00   -0.99    0.09    1.20
     2    0.00   -0.03    0.04    1.03
     3    1.00    0.93    0.09    1.07

 s
                                      
     1    0.70    0.70    0.09    1.01
     2    0.30    0.30    0.09    1.01

 b
                                      
   1.1    2.00    2.04    0.21    1.06
   1.2   -0.50   -0.51    0.28    1.00
   2.1    1.00    0.74    0.41    1.05
   2.2    1.00    1.20    0.32    1.00

 Omega
                                      
 1.1,1    0.31    0.23    0.14    1.71
 1.1,2    0.71    0.37    0.25    1.52
 1.2,2    4.67    4.33    1.16    1.04
 2.1,1    1.67    1.18    0.51    1.11
 2.1,2   -1.20   -0.71    0.35    1.03
 2.2,2    0.87    0.66    0.31    1.01

 Sigma
                                      
   1,1    1.00    1.00    0.00    1.00
\end{Soutput}
\end{Schunk}

\subsection{Weight-based update of the latent classes} \label{subsec:weight_update}

Adding \code{"wp_update" = TRUE} to the list \code{latent\_classes} above executes the following weight-based updating scheme (analogue to \cite{Bauer:2019}):

\begin{itemize}
  \item Class $c$ is removed, if $s_c<\varepsilon_{\text{min}}$, i.e. if the class weight $s_c$ drops below some threshold $\varepsilon_{\text{min}}$. This case indicates that class $c$ has a negligible impact on the mixing distribution.
  \item Class $c$ is splitted into two classes $c_1$ and $c_2$, if $s_c>\varepsilon_\text{max}$. This case indicates that class $c$ has a high influence on the mixing distribution whose approximation can potentially be improved by increasing the resolution in directions of high variance. Therefore, the class means $b_{c_1}$ and $b_{c_2}$ of the new classes $c_1$ and $c_2$ are shifted in opposite directions from the class mean $b_c$ of the old class $c$ in the direction of the highest variance.
  \item Classes $c_1$ and $c_2$ are joined to one class $c$, if $\lVert b_{c_1} - b_{c_2} \rVert<\varepsilon_{\text{distmin}}$, i.e. if the euclidean distance between the class means $b_{c_1}$ and $b_{c_2}$  drops below some threshold $\varepsilon_{\text{distmin}}$. This case indicates location redundancy which should be repealed. The parameters of $c$ are assigned by adding the values of $s$ from $c_1$ and $c_2$ and averaging the values for $b$ and $\Omega$.
\end{itemize}

The values for $\varepsilon_{\text{min}}$, $\varepsilon_{\text{max}}$ and $\varepsilon_{\text{distmin}}$ can be specified. Per default, \pkg{RprobitB} sets  \code{epsmin = 0.01}, \code{epsmax = 0.99}, and \code{distmin = 0.1}.

\begin{Schunk}
\begin{Sinput}
> model_sim <- fit_model(
+    data = data_sim,
+    R = 1000,
+    latent_classes = list(
+      "C" = 10,
+      "weight_update" = TRUE,
+      "buffer" = 5
+      ),
+    seed = 1
+  )
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
> plot(model_sim, type = "class_seq")
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-model-sim-class-seq}

\subsection{Dirichlet process-based update of the latent classes} \label{subsec:dp_update}

As an alternative to the weight-based updating scheme, \pkg{RprobitB} implemented the Dirichlet process. A similar approach in the context of discrete choice can be found in \cite{Burda:2008}, where the Dirichlet process is applied to estimate a mixed logit-probit model. The Dirichlet Process is a Bayesian nonparametric method, where nonparametric means that the number of model parameters can theoretically grow to infinity. The method allows to add more mixture components to the mixing distribution if needed for a better approximation, see \cite{Neal:2000} for a documentation of the general case. The literature offers many representations of the method, including the Chinese Restaurant Process \citep{Aldous:1985}, the stick-braking metaphor \citep{Sethuraman:1994}, and the Polya Urn model \citep{Blackwell:1973}.

In our case, we face the situation to find a distribution $g$ that explains the decider specific coefficients $(\beta_n)_{n = 1,\dots,N}$, where $g$ is supposed to be a mixture of an unknown number $C$ of Gaussian densities, i.e. $g = \sum_{c = 1,\dots,C} s_c \text{MVN}(b_c, \Omega_c)$.

Let $z_n \in \{1,\dots,C\}$ denote the class membership of $\beta_n$. A priori, the mixture weights $(s_c)_c$ are given a Dirichlet prior with concentration parameter $\delta/C$, i.e. $(s_c)_c \mid \delta \sim \text{D}_C(\delta/C,\dots,\delta/C)$. \cite{Rasmussen:2000} shows that

$$ \Pr((z_n)_n\mid \delta) = \frac{\Gamma(\delta)}{\Gamma(N+\delta)} \prod_{c=1}^C \frac{\Gamma(m_c + \delta/C)}{\Gamma(\delta/C)}, $$
where $\Gamma(\cdot)$ denotes the gamma function and $m_c = \#\{n:z_n = c\}$ the number of elements that are currently allocated to class $c$. Crucially, the last equation is independent of the class weights $(s_c)_c$, yet it still depends on the finite number $C$ of latent classes. However, \cite{Li:2019} shows that

$$ \Pr(z_n = c \mid z_{-n}, \delta) = \frac{m_{c,-n} + \delta/C}{N-1+\delta},$$
where the notation $-n$ means excluding the $n$-th element. We can let $C$ approach infinity to derive:

$$ \Pr(z_n = c \mid z_{-n}, \delta) \to \frac{m_{c,-n}}{N-1+\delta}. $$

Note that the allocation probabilities do not sum to 1, instead

$$ \sum_{c = 1}^C \frac{m_{c,-n}}{N-1+\delta} = \frac{N-1}{N-1+\delta}. $$

The difference to 1 equals

$$ \Pr(z_n \neq z_m ~ \forall ~ m \neq n \mid z_{-n}, \delta) = \frac{\delta}{N-1+\delta} $$

and constitutes the probability that a new cluster for observation $n$ is created. \cite{Neal:2000} points out that this probability is proportional to the prior parameter $\delta$: A greater value for $\delta$ encourages the creation of new clusters, a smaller value for $\delta$ increases the probability of an allocation to an already existing class.

In summary, the Dirichlet process updates the allocation of each $\beta$ coefficient vector one at a time, dependent on the other allocations. The number of clusters can theoretically rise to infinity, however, as we delete unoccupied clusters, $C$ is bounded by $N$. As a final step after the allocation update, we update the class means $b_c$ and covariance matrices $\Omega_c$ by means of their posterior predictive distribution. The mean and covariance matrix for a new generated cluster is drawn from the prior predictive distribution. The corresponding formulas are given in \cite{Li:2019}.

The Dirichlet process directly integrates into our existing Gibbs sampler. Given $\beta$ values, it updated the class means $b_c$ and class covariance matrices $\Omega_c$. The Dirichlet process updating scheme is implemented in the function \fct{update\_classes\_dp}. In the following, we give a small example in the bivariate case \code{P\_r = 2}. We sample true class means \code{b\_true} and class covariance matrices \code{Omega\_true} for \code{C\_true = 3} true latent classes. The true (unbalanced) class sizes are given by the vector \code{N}, and \code{z_true} denotes the true allocations.

\begin{Schunk}
\begin{Sinput}
> set.seed(1)
> P_r <- 2
> C_true <- 3
> N <- c(100,70,30)
> (b_true <- matrix(replicate(C_true, rnorm(P_r)), nrow = P_r, ncol = C_true))
\end{Sinput}
\begin{Soutput}
           [,1]       [,2]       [,3]
[1,] -0.6264538 -0.8356286  0.3295078
[2,]  0.1836433  1.5952808 -0.8204684
\end{Soutput}
\begin{Sinput}
> (Omega_true <- matrix(replicate(C_true, rwishart(P_r + 1, 0.1*diag(P_r))$W, simplify = TRUE),
+                        nrow = P_r*P_r, ncol = C_true))
\end{Sinput}
\begin{Soutput}
          [,1]        [,2]       [,3]
[1,] 0.3093652  0.14358543  0.2734617
[2,] 0.1012729 -0.07444148 -0.1474941
[3,] 0.1012729 -0.07444148 -0.1474941
[4,] 0.2648235  0.05751780  0.2184029
\end{Soutput}
\begin{Sinput}
> beta <- c()
> for(c in 1:C_true) for(n in 1:N[c])
+    beta <- cbind(beta, rmvnorm(mu = b_true[,c,drop=F], Sigma = matrix(Omega_true[,c,drop=F], ncol = P_r)))
> z_true <- rep(1:3, times = N)
\end{Sinput}
\end{Schunk}

We specify the following prior parameters (for their definition see the vignette on model fitting):

\begin{Schunk}
\begin{Sinput}
> delta <- 0.1
> xi <- numeric(P_r)
> D <- diag(P_r)
> nu <- P_r + 2
> Theta <- diag(P_r)
\end{Sinput}
\end{Schunk}

Initially, we start with \code{C = 1} latent classes. The class mean \code{b} is set to zero, the covariance matrix \code{Omega} to the identity matrix:

\begin{Schunk}
\begin{Sinput}
> z <- rep(1, ncol(beta))
> C <- 1
> b <- matrix(0, nrow = P_r, ncol = C)
> Omega <- matrix(rep(diag(P_r), C), nrow = P_r*P_r, ncol = C)
\end{Sinput}
\end{Schunk}

The following call to \fct{update\_classes\_dp} updates the latent classes in \code{100} iterations. Note that we specify the arguments \code{Cmax} and \code{s\_desc}. The former denotes the maximum number of latent classes. This specification is not a requirement for the Dirichlet process per se, but rather for its implementation. Knowing the maximum possible class number, we can allocate the required memory space, which leads to a speed improvement. We later can verify that we won't exceed the number of \code{Cmax = 10} latent classes at any point of the Dirichlet process. Setting \code{s\_desc = TRUE} ensures that the classes are ordered by their weights in a descending order to ensure identifiability.

\begin{Schunk}
\begin{Sinput}
> for(r in 1:100){
+    dp <- RprobitB:::update_classes_dp(
+      Cmax = 10, beta, z, b, Omega, delta, xi, D, nu, Theta, s_desc = TRUE
+      )
+    z <- dp$z
+    b <- dp$b
+    Omega <- dp$Omega
+  }
\end{Sinput}
\end{Schunk}

The Dirichlet process was able to infer the true number \code{C\_true = 3} of latent classes:

\begin{Schunk}
\begin{Sinput}
> par(mfrow = c(1,2))
> plot(t(beta), xlab = bquote(beta[1]), ylab = bquote(beta[2]), pch = 19)
> RprobitB:::plot_class_allocation(beta, z, b, Omega, r = 100, perc = 0.95)
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-dp-example}

\paragraph{Example 4: Online chess strategy.}

\begin{Schunk}
\begin{Sinput}
> data(model_berserk, package = "RprobitB")
\end{Sinput}
\end{Schunk}

The estimated pairwise mixing distributions can be visualized via calling the \fct{plot} method with the additional argument \code{type = mixture}:

\begin{Schunk}
\begin{Sinput}
> plot(model_berserk, type = "mixture")
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-model-berserk-mixture}

\section{Choice prediction} \label{sec:choice_prediction}

\pkg{RprobitB} provides a \fct{predict} method for in-sample and out-of-sample prediction. The former case refers to reproducing the observed choices on the basis of the covariates and the fitted model and subsequently using the deviations between prediction and reality as an indicator for the model performance. The latter means forecasting choice behavior for changes in the choice attributes. For illustration, we revisit our probit model of travelers deciding between two fictional train route alternatives.

\paragraph{Example 1: Train (cont.).}

Per default, the \fct{predict} method returns a confusion matrix, which gives an overview of the in-sample prediction performance:

\begin{Schunk}
\begin{Sinput}
> predict(model_train)
\end{Sinput}
\begin{Soutput}
    predicted
true    A    B
   A 1024  450
   B  438 1017
\end{Soutput}
\end{Schunk}

By setting the argument \code{overview = FALSE}, the method instead returns predictions on the level of individual choice occasions:\footnote{Incorrect predictions can be analyzed via the convenience function \fct{get\_cov}, which extracts the characteristics of a particular choice situation.}

\begin{Schunk}
\begin{Sinput}
> pred <- predict(model_train, overview = FALSE)
> head(pred, n = 5)
\end{Sinput}
\begin{Soutput}
  id choiceid         A          B true predicted correct
1  1        1 0.9157601 0.08423988    A         A    TRUE
2  1        2 0.6373833 0.36261675    A         A    TRUE
3  1        3 0.7918504 0.20814963    A         A    TRUE
4  1        4 0.1799070 0.82009305    B         B    TRUE
5  1        5 0.5494634 0.45053656    B         A   FALSE
\end{Soutput}
\end{Schunk}

Apart from the prediction accuracy, the model performance can be evaluated more nuanced in terms of sensitivity and specificity, for example via a receiver operating characteristic (ROC) curve \citep{Fawcett:2006}, using the \pkg{plotROC} package \citep{Sachs:2017}:

\begin{Schunk}
\begin{Sinput}
> library(plotROC)
> ggplot(data = pred, aes(m = A, d = ifelse(true == "A", 1, 0))) +
+    geom_roc(n.cuts = 20, labels = FALSE) +
+    style_roc(theme = theme_grey)
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-roc-example}

The \fct{predict} method has an additional \code{data} argument. Per default, \code{data = NULL}, which results into the in-sample case outlined above. Alternatively, \code{data} can be either an \class{RprobitB\_data} object (for example the test subsample derived from the \fct{train\_test} function, or a data frame of custom choice characteristics.

We demonstrate the second case in the following. Assume that a train company wants to anticipate the effect of a price increase on their market share. By our model, increasing the ticket price from 100 euros to 110 euros (ceteris paribus) draws 15\% of the customers to the competitor who does not increase their prices.

\begin{Schunk}
\begin{Sinput}
> predict(
+    model_train,
+    data = data.frame("price_A" = c(100,110),
+                      "price_B" = c(100,100)),
+    overview = FALSE)
\end{Sinput}
\begin{Soutput}
  id choiceid         A         B prediction
1  1        1 0.5000000 0.5000000          A
2  2        1 0.3480596 0.6519404          B
\end{Soutput}
\end{Schunk}

However, offering a better comfort class compensates for the higher price and even results in a gain of 7\% market share:

\begin{Schunk}
\begin{Sinput}
> predict(
+    model_train,
+    data = data.frame("price_A"   = c(100,110),
+                      "comfort_A" = c(1,0),
+                      "price_B"   = c(100,100),
+                      "comfort_B" = c(1,1)),
+    overview = FALSE)
\end{Sinput}
\begin{Soutput}
  id choiceid         A         B prediction
1  1        1 0.5000000 0.5000000          A
2  2        1 0.5689424 0.4310576          A
\end{Soutput}
\end{Schunk}

\section{Model selection} \label{sec:model_selection}

The task of model selection targets the question: If there are several competing models, how do I choose the most appropriate one? This section outlines the model selection tools implemented in \pkg{RprobitB}. For illustration, we revisit the probit model of travelers deciding between two fictional train route alternatives:

\begin{Schunk}
\begin{Sinput}
> data("model_train", package = "RprobitB")
> model_train
\end{Sinput}
\begin{Soutput}
Probit model 'choice ~ price + time + change + comfort | 0'.
\end{Soutput}
\end{Schunk}

As a competing model, we consider explaining the choices only by the alternative's price, i.e. the probit model with the formula \code{choice ~ price | 0`}. (This model is also saved in \pkg{RprobitB} and can be accessed via \code{data("model\_train\_sparse", package = "RprobitB")}.)

\begin{Schunk}
\begin{Sinput}
> model_train_sparse <- nested_model(model_train, form = choice ~ price | 0)
\end{Sinput}
\end{Schunk}


The \fct{nested\_model} function helps to estimate a new version of \code{model\_train} with new specifications. Here, only \code{form} has been changed.

\paragraph{The model selection function:}

\pkg{RprobitB} provides the convenience function \fct{model\_selection}, which takes an arbitrary number of \class{RprobitB\_fit} objects and returns a matrix of model selection criteria:

\begin{Schunk}
\begin{Sinput}
> model_selection(model_train, model_train_sparse,
+                  criteria = c("npar", "LL", "AIC", "BIC", "WAIC", "MMLL", "BF", "pred_acc"))
\end{Sinput}
\begin{Soutput}
                         model_train model_train_sparse
npar                               4                  1
LL                          -1727.70           -1865.86
AIC                          3463.41            3733.73
BIC                          3487.34            3739.71
WAIC                         3463.74            3733.87
se(WAIC)                        0.19               0.07
pWAIC                           4.37               1.13
MMLL                        -1731.13           -1867.26
BF(*,model_train)                  1             < 0.01
BF(*,model_train_sparse)       > 100                  1
pred_acc                      69.68%             63.37%
\end{Soutput}
\end{Schunk}

Specifying \code{criteria} is optional. Per default, \code{criteria = c("npar", "LL", "AIC", "BIC")}. The available model selection criteria are explained in the following.

\paragraph{npar:}

\code{"npar"} yields the number of model parameters, which is computed by the \fct{npar} method:

\begin{Schunk}
\begin{Sinput}
> npar(model_train, model_train_sparse)
\end{Sinput}
\begin{Soutput}
[1] 4 1
\end{Soutput}
\end{Schunk}

Here, \code{model\_train} has 4 parameters (a coefficient for price, time, change, and comfort, respectively), while \code{model\_train\_sparse} has only a single price coefficient.

\paragraph{LL:}

If \code{"LL"} is included in \code{criteria}, \fct{model\_selection} returns the model's log-likelihood values. They can also be directly accessed via the \fct{logLik} method. The log-likelihood values are per default computed at the point estimates derived from the Gibbs sample means. \fct{logLik} has the argument \code{par\_set}, where alternative statistics for the point estimate can be specified.

\begin{Schunk}
\begin{Sinput}
> logLik(model_train)
\end{Sinput}
\begin{Soutput}
[1] -1727.704
\end{Soutput}
\begin{Sinput}
> logLik(model_train_sparse)
\end{Sinput}
\begin{Soutput}
[1] -1865.863
\end{Soutput}
\end{Schunk}

\paragraph{AIC:}

Including \code{"AIC"} yields the Akaike's Information Criterion \citep{Akaike:1974}, which is computed as $$-2 \cdot \text{LL} + k \cdot \text{npar},$$
where $\text{LL}$ is the model's log-likelihood value, $k$ is the penalty per parameter with $k = 2$ per default for the classical AIC, and $\text{npar}$ is the number of parameters in the fitted model.

Alternatively, the \fct{AIC} method also returns the AIC values:

\begin{Schunk}
\begin{Sinput}
> AIC(model_train, model_train_sparse, k = 2)
\end{Sinput}
\begin{Soutput}
[1] 3463.408 3733.725
\end{Soutput}
\end{Schunk}

The AIC quantifies the trade-off between over- and under-fitting, where smaller values are preferred. Here, the increase in goodness of fit justifies the additional 3 parameters of \code{model\_train}.

\paragraph{BIC:}

Similar to the AIC, \code{"BIC"} yields the Bayesian Information Criterion \citep{Schwarz:1978}, which is defined as $$-2 \cdot \text{LL} + \log{(\text{nobs})} \cdot \text{npar},$$
where $\text{LL}$ is the model's log-likelihood value, $\text{nobs}$ is the number of data points (which can be accessed via the \fct{nobs} method), and $\text{npar}$ is the number of parameters in the fitted model. The interpretation is analogue to AIC.

\pkg{RprobitB} also provided a method for the BIC value:

\begin{Schunk}
\begin{Sinput}
> BIC(model_train, model_train_sparse)
\end{Sinput}
\begin{Soutput}
[1] 3487.338 3739.708
\end{Soutput}
\end{Schunk}

\paragraph{WAIC:}

WAIC is short for Widely Applicable (or Watanabe-Akaike) Information Criterion \citep{Watanabe:2010}. As for AIC and BIC, the smaller the WAIC value the better the model. Including \code{"WAIC"} in \code{criteria} yields the WAIC value, its standard error \code{se(WAIC)}, and the effective number of parameters \code{pWAIC}, see below.

The WAIC is defined as

$$-2  \cdot \text{lppd} + 2\cdot p_\text{WAIC},$$

where $\text{lppd}$ stands for log pointwise predictive density and $p_\text{WAIC}$ is a penalty term proportional to the variance in the posterior distribution that is sometimes called effective number of parameters, see \cite{McElreath:2016} p. 220 for a reference.

The $\text{lppd}$ is approximated as follows. Let $$p_{si} = \Pr(y_i\mid \theta_s)$$ be the probability of observation $y_i$ (here the single choices) given the $s$-th set $\theta_s$ of parameter samples from the posterior. Then

$$\text{lppd} = \sum_i \log \left( S^{-1} \sum_s p_{si} \right).$$
The penalty term is computed as the sum over the variances in log-probability for each observation:
$$p_\text{WAIC} = \sum_i \mathbb{V}_{\theta}  \log (p_{si}) . $$
The $\text{WAIC}$ has a standard error of
$$\sqrt{n \cdot \mathbb{V}_i \left[-2 \left(\text{lppd} - \mathbb{V}_{\theta}  \log (p_{si})  \right)\right]},$$
where $n$ is the number of choices.

Before computing the WAIC of an \code{RprobitB\_fit} object, the probabilities $p_{si}$ must be computed via the \fct{compute\_p\_si} function:

\begin{Schunk}
\begin{Sinput}
> model_train <- compute_p_si(model_train)
\end{Sinput}
\end{Schunk}

Afterwards, the WAIC can be accessed as follows, where the number in brackets is the standard error:

\begin{Schunk}
\begin{Sinput}
> WAIC(model_train)
> WAIC(model_train_sparse)
\end{Sinput}
\end{Schunk}

You can visualize the convergence of the WAIC as follows:

\begin{Schunk}
\begin{Sinput}
> plot(WAIC(model_train))
> plot(WAIC(model_train_sparse))
\end{Sinput}
\end{Schunk}

Here, both approximations look satisfactory. If the WAIC value does not seem to have converged, use more Gibbs samples by increasing \code{R} in \fct{fit\_model} or decreasing \code{B} or \code{Q} via \fct{transform}, see the vignette on model fitting.

\paragraph{MMLL:}

\code{"MMLL"} in \code{criteria} stands for marginal model log-likelihood. The model's marginal likelihood $\Pr(y\mid M)$ for a model $M$ and data $y$ is required for the computation of Bayes factors, see below. In general, the term has no closed form and must be approximated numerically.

\pkg{RprobitB} uses the posterior Gibbs samples derived from the \fct{fit\_model} function to approximate the model's marginal likelihood via the posterior harmonic mean estimator \citep{Newton:1994}: Let $S$ denote the number of available posterior samples $\theta_1,\dots,\theta_S$. Then,
$$\Pr(y\mid M) = \left(\mathbb{E}_\text{posterior} 1/\Pr(y\mid \theta,M) \right)^{-1} \approx \left( \frac{1}{S} \sum_s 1/\Pr(y\mid \theta_s,M) \right) ^{-1} = \tilde{\Pr}(y\mid M).$$

By the law of large numbers, $\tilde{\Pr}(y\mid M) \to \Pr(y\mid M)$ almost surely as $S \to \infty$.

As for the WAIC, computing the MMLL relies on the probabilities $p_{si} = \Pr(y_i\mid \theta_s)$, which must first be computed via the \fct{compute\_p\_si} function. Afterwards, the \fct{mml} function can be called with an \class{RprobitB\_fit} object as input. The function returns the \class{RprobitB\_fit} object, where the marginal likelihood value is saved as the entry \code{"mml"} and the marginal log-likelihood value as the attribute \code{"mmll"}. Note that the marginal likelihood value is very small. The given representation is required so that the value is not rounded to 0 by the computer.

\begin{Schunk}
\begin{Sinput}
> model_train <- mml(model_train)
> model_train$mml
\end{Sinput}
\begin{Soutput}
9.72e-117 * exp(-1464)
\end{Soutput}
\begin{Sinput}
> attr(model_train$mml, "mmll")
\end{Sinput}
\begin{Soutput}
[1] -1731.128
\end{Soutput}
\end{Schunk}

Analogue to the WAIC value, the computation of the MMLL is an approximation that improves with rising (posterior) sample size. The convergence can again be verified visually via the \fct{plot} method:

\begin{Schunk}
\begin{Sinput}
> plot(model_train$mml, log = TRUE)
\end{Sinput}
\end{Schunk}
\includegraphics{rprobitb_oelschlaeger_bauer-mmltrace}

There are two options for improving the approximation: As for the WAIC, you can use more posterior samples. Alternatively, you can combine the posterior harmonic mean estimate with the prior arithmetic mean estimator \citep{Hammersley:1964}: For this approach, $S$ samples $\theta_1,\dots,\theta_S$ are drawn from the model's prior distribution. Then,

$$\Pr(y\mid M) = \mathbb{E}_\text{prior} \Pr(y\mid \theta,M) \approx \frac{1}{S} \sum_s \Pr(y\mid \theta_s,M) = \tilde{\Pr}(y\mid M).$$

Again, it hols by the law of large numbers, that $\tilde{\Pr}(y\mid M) \to \Pr(y\mid M)$ almost surely as $S \to \infty$. The final approximation of the model's marginal likelihood than is a weighted sum of the posterior harmonic mean estimate and the prior arithmetic mean estimate, where the weights are determined by the sample sizes.

To use the prior arithmetic mean estimator, call the \fct{mml} function with a specification of the number of prior draws \code{S} and set \code{recompute = TRUE}:

\begin{Schunk}
\begin{Sinput}
> model_train <- mml(model_train, S = 1000, recompute = TRUE)
\end{Sinput}
\end{Schunk}

Note that the prior arithmetic mean estimator works well if the prior and posterior distribution have a similar shape and strong overlap, as \cite{Gronau:2017} points out. Otherwise, most of the sampled prior values result in a likelihood value close to zero, thereby contributing only marginally to the approximation. In this case, a very large number \code{S} of prior samples is required.

\paragraph{Bayes factor:}

The Bayes factor is an index of relative posterior model plausibility of one model over another \citep{Marin:2014}. Given data $\texttt{y}$ and two models $\texttt{mod0}$ and $\texttt{mod1}$, it is defined as

$$
BF(\texttt{mod0},\texttt{mod1}) = \frac{\Pr(\texttt{mod0} \mid \texttt{y})}{\Pr(\texttt{mod1} \mid \texttt{y})} = \frac{\Pr(\texttt{y} \mid \texttt{mod0} )}{\Pr(\texttt{y} \mid \texttt{mod1})} / \frac{\Pr(\texttt{mod0})}{\Pr(\texttt{mod1})}.
$$

The ratio $\Pr(\texttt{mod0}) / \Pr(\texttt{mod1})$ expresses the factor by which $\texttt{mod0}$ a priori is assumed to be the correct model. Per default, \pkg{RprobitB} sets $\Pr(\texttt{mod0}) = \Pr(\texttt{mod1}) = 0.5$. The front part $\Pr(\texttt{y} \mid \texttt{mod0} ) / \Pr(\texttt{y} \mid \texttt{mod1})$ is the ratio of the marginal model likelihoods. A value of $BF(\texttt{mod0},\texttt{mod1}) > 1$ means that the model $\texttt{mod0}$ is more strongly supported by the data under consideration than $\texttt{mod1}$.

Adding \code{"BF"} to the \code{criteria} argument of \fct{model\_selection} yields the Bayes factors. We see a decisive evidence \citep{Jeffreys:1998} in favor of \class{model\_train}.

\begin{Schunk}
\begin{Sinput}
> model_selection(model_train, model_train_sparse, criteria = c("BF"))
\end{Sinput}
\end{Schunk}

\paragraph{pred acc:}

Finally, adding \code{"pred_acc"} to the \code{criteria} argument for the \fct{model\_selection} function returns the share of correctly predicted choices. From the output of \fct{model\_selection} above (or alternatively the one in the following) we deduce that \class{model\_train} correctly predicts about 6\% of the choices more than \class{model\_train\_sparse}:

\begin{Schunk}
\begin{Sinput}
> pred_acc(model_train, model_train_sparse)
\end{Sinput}
\begin{Soutput}
[1] 0.6968249 0.6336634
\end{Soutput}
\end{Schunk}

\section{Conclusion} \label{sec:conclusion}

%% The problem
This paper addressed the problem of specifying mixing distributions in the multinomial probit model with a panel data setting, constituting an important part of the model selection for which the literature does not provide much guidance so far. In the absence of alternatives, many applications of the mixed multinomial probit model rely on different types of standard parametric distributions for modelling heterogeneity in preferences in combination with a likelihood-value based model selection. This course of action is very restrictive and imposes strong assumptions on the distribution of the model parameters that could potentially lead to misspecification and biased estimates.

%% Our solution
We proposed a new approach that improves the current specification strategies in several ways: First, our approach does not require any distributional assumption, since the latent class setting is flexible enough to approximate practically any distribution shape and allowing for any correlation pattern. Second, the weight-based updating scheme ensures that the number of latent classes does not need to be prespecified. Third, the imposed Bayesian estimation framework avoids many numerical problems that occur in the frequentist approach. Most notably, no likelihood function has to be evaluated nor approximated. Comparing the numerical estimation speed to the non-parametric frequentist approach of \cite{Bauer:2019}, we found that our implementation of the Bayesian approach is at least 10 times faster. The improvement becomes more distinct for panel data settings with a high number of choice occasions. This is due to the fact that for given total sample size $NT$ a large $T$ is beneficial for the Bayesian approach as then the number of vectors $\beta_n,~ n = 1,...,N$ is comparatively small, while in the frequentist approach calculating the likelihood becomes more challenging for increasing the number $T$ of choice situations faced by each of the $N$ individuals. On the other hand, the grid based frequentist approach of \cite{Bauer:2019} can potentially achieve a better approximation (especially of point masses) due to the relatively high number of latent classes. However, this approach requires that a suitable grid is set prior to the estimation with a specification of upper bounds for the coefficients. Additionally, the curse of dimensionality plays a crucial role, which is less of a burden in the Bayesian approach. Note that for a fully specified parametric structure these concerns do not play such a big role also for the frequentist approach.

%% Planned extensions
Our simulation results verified that the proposed approach achieves good approximations of the mixing distribution in common choice modelling situations, where the underlying heterogeneity cannot be captured by standard parametric distributions. It would be interesting to apply the approach also to empirical data in the future. Additionally,
further research on how to properly address sign-restricted coefficients is required.


\section*{Computational details}

The results in this paper were obtained using
\proglang{R}~4.1.0 with the
\pkg{RprobitB}~1.0.0.9000 package. \proglang{R} itself
and all packages used are available from the Comprehensive
\proglang{R} Archive Network (CRAN) at \url{https://CRAN.R-project.org/}.


\section*{Acknowledgments}

This work has been financed partly by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Projektnummer 356500581 which is gratefully acknowledged.

\bibliography{ref}


\end{document}
