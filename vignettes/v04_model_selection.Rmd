---
title: "Model selection"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Model selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: ref.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

This vignette explains model selection in RprobitB. We target the question: If there are several possible models, how do I choose the most appropriate one?

```{r setup}
library(RprobitB)
```

## Model selection with the `model_selection()` function

RprobitB provides a function called `model_selection()` which guides through the process of model selection. As input the function takes an arbitrary number of `RprobitB_model` objects.

For illustration, we fit three different models with increasing complexity to the [Train dataset from the mlogit package](https://cran.r-project.org/package=mlogit).

```{r fit models}
data("Train", package = "mlogit")

### Using only the alternative's price as covariate
d1 <- prepare_data(
  form = choice ~ price,
  choice_data = Train
)
m1 <- mcmc(d1, print_progress = FALSE)

### Using all available covariates
d2 <- prepare_data(
  form = choice ~ price | 0 | time + comfort + change,
  choice_data = Train
)
m2 <- mcmc(d2, print_progress = FALSE)

### We additionally impose a mixing distribution on the price coefficient
d3 <- prepare_data(
  form = choice ~ price | 0 | time + comfort + change,
  choice_data = Train, 
  re = "price"
)
m3 <- mcmc(d3, print_progress = FALSE)
```

The `model_selection()` function returns the following information:

```{r model selection}
model_selection(m1, m2, m3)
```

The output is explained in detail below.

## Information criteria

### `npar`

Simply the number of estimated model parameters.

### `LL`

The model's log-likelihood value at the estimated parameters.

### `AIC`

### `BIC`

### `WAIC`

WAIC is short for *Widely Applicable (or Watanabe-Akaike) Information Criterion*. As for AIC and BIC, the smaller the WAIC value the better the model. Its definition is

$$\text{WAIC} = -2 ( \text{lppd} - p_\text{WAIC} ),$$
where $\text{lppd}$ stands for *log pointwise predictive density* and $p_\text{WAIC}$ is a penalty term proportional to the variance in the posterior distribution that is sometimes called *effective number of parameters*, see @McElreath:16 page 220.

The $\text{lppd}$ is computed as follows. Let $$p_{is} = \Pr(y_i\mid \theta_s)$$ be the probability of observation $y_i$ given the $s$-th set $\theta_s$ of parameter samples from the posterior. Then

$$\text{lppd} = \sum_i \log S^{-1} \sum_s p_{si}.$$
The penalty term is computed as the sum over the variances in log-probability for each observation:
$$p_\text{WAIC} = \sum_i \mathbb{V}_{\theta} \left[ \log p_{si} \right]. $$
Since the calculation of $\text{WAIC}$ is pointwise over the $n$ observations $y_i$, it has an (approximate) standard error $\text{SE}$, which is computed as 
$$\text{SE} = \sqrt{n \cdot \mathbb{V}_i \left[-2 \left(\text{lppd} - \mathbb{V}_{\theta} \left[ \log p_{si} \right] \right)\right]}.$$
The internal function `waic()` provides all the components given an `RprobitB_model` object. 

## Bayes factors

## Prediction accuracy

## Likelihood ratio test
