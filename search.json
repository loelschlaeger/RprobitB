[{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":null,"dir":"","previous_headings":"","what":"RprobitB TODOs","title":"RprobitB TODOs","text":"following tasks agenda tackled time permits.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"gibbs-sampler","dir":"","previous_headings":"Functionality","what":"Gibbs sampler","title":"RprobitB TODOs","text":"Make setting scale mcmc intuitive. Use default progress bar Gibbs sampling show least ETA. Check document R_hat() implementation (chain splitted?). Fixed parameters: update, set fixed parameters. Use transform() keep Gibbs sampling, .e. change R. Don’t save Gibbs samples RprobitB_fit raw ones. Show average covariates classes classification via preference_classification().","code":""},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"data","dir":"","previous_headings":"Functionality","what":"Data","title":"RprobitB TODOs","text":"Variable choice set Function generates covariates. example lagged terms transformations.","code":""},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"ordered-probit","dir":"","previous_headings":"Functionality","what":"Ordered Probit","title":"RprobitB TODOs","text":"Implement ordered probit model.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"console-outputs","dir":"","previous_headings":"Outputs","what":"Console outputs","title":"RprobitB TODOs","text":"Print model formula check_form().","code":""},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"visualization","dir":"","previous_headings":"Outputs","what":"Visualization","title":"RprobitB TODOs","text":"Exclude parameter wrt model normalized.","code":""},{"path":[]},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"bugs","dir":"","previous_headings":"Performance","what":"Bugs","title":"RprobitB TODOs","text":"gibbs_sampling() low data fails “Evaluation error: positive probabilities”. simulate_choices(), specifying s without seed fails (due partial match ellipsis argument?).","code":""},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"tests","dir":"","previous_headings":"Performance","what":"Tests","title":"RprobitB TODOs","text":"Add tests update_classes_dp().","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"demos","dir":"","previous_headings":"References","what":"Demos","title":"RprobitB TODOs","text":"rpb() demonstration console feedback.","code":""},{"path":"https://loelschlaeger.de/RprobitB/TODO.html","id":"function-documentation","dir":"","previous_headings":"References","what":"Function documentation","title":"RprobitB TODOs","text":"Document Dirichlet Process update_classes_dp(). Document RprobitB_latent_classes.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/RprobitB.html","id":"the-package-purpose","dir":"Articles","previous_headings":"","what":"The package purpose","title":"Introduction","text":"{RprobitB}, can model choices made deciders among discrete set alternatives. example, think tourists want book flight holiday destination. knowledge prefer certain route another great value airlines, especially customer’s willingness pay say faster comfortable flight alternative. Different deciders value different choice attributes differently. example, imaginable business people place higher value flight time willing pay faster route alternative vacationers. choice behavior heterogeneity can addressed {RprobitB}. Furthermore, package enables identify groups deciders share similar preferences. Finally, package enables prediction choice behavior certain choice attributes change, example proportion customers choose competitor’s product event price increase.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/RprobitB.html","id":"the-workflow","dir":"Articles","previous_headings":"","what":"The workflow","title":"Introduction","text":"functions {RprobitB} can grouped ones data management, model fitting, model evaluation, see flowchart . package can used two different purposes: () estimation model given data (b) estimation model simulated data. Simulation typically serves assess properties estimation algorithms either research bootstrap like fashion. {RprobitB} supports functions. typical workflow follows: Prepare choice data set via prepare_data() function simulate data via simulate_choices() (true model parameters can set RprobitB_parameter object). functions return RprobitB_data object can fed estimation routine. train_test() allows optionally split data estimation validation part. See vignette choice data details. estimation routine called mcmc() (Markov chain Monte Carlo) returns RprobitB_fit object. transform_fit() function allows change normalization model model fitted. See vignettes model fitting modeling heterogeneity. RprobitB_fit object can fed coef() show covariate effects choices predict() compute choice probabilities forecast choice behavior choice characteristics change, see vignette choice prediction. function model_selection() compares arbitrary many competing RprobitB_fit objects computing different model selection criteria, see vignette model selection. flowchart {RprobitB}.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/RprobitB.html","id":"printing-the-progress-of-computations","dir":"Articles","previous_headings":"","what":"Printing the progress of computations","title":"Introduction","text":"Per default, function {RprobitB} takes seconds evaluation time prints progress computations console. hide progress messages, set options(\"RprobitB_progress\" = FALSE). show , set options(\"RprobitB_progress\" = TRUE).","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v01_model_definition.html","id":"the-probit-model","dir":"Articles","previous_headings":"","what":"The probit model","title":"Model definition","text":"probit model2 regression-type model dependent variable takes finite number values error term normally distributed (Agresti 2015). purpose estimate probability dependent variable takes certain, discrete value. common application discrete choice scenarios. dependent variable one finitely many mutually exclusive alternatives (ordered), explanatory variables typically characteristics deciders alternatives. concrete, assume possess data \\(N\\) decision makers choose \\(J \\geq 2\\) alternatives3 \\(T\\) choice occasions4. Specific decision maker, alternative choice occasion, furthermore observe \\(P\\) choice attributes use explain choices. continuous choice attributes linked directly discrete choices must take detour latent variable. discrete choice setting, variable can interpreted decider’s utility certain alternative. Decider \\(n\\)’s utility \\(U_{ntj}\\) alternative \\(j\\) choice occasion \\(t\\) modeled \\[\\begin{equation}   U_{ntj} = X_{ntj}'\\beta + \\epsilon_{ntj} \\end{equation}\\] \\(n=1,\\dots,N\\), \\(t=1,\\dots,T\\) \\(j=1,\\dots,J\\), \\(X_{ntj}\\) (column) vector \\(P\\) characteristics \\(j\\) faced \\(n\\) \\(t\\), \\(\\beta \\{\\mathbb R}^{P}\\) vector coefficients, \\((\\epsilon_{nt:}) = (\\epsilon_{nt1},\\dots,\\epsilon_{ntJ})' \\sim \\text{MVN}_{J} (0,\\Sigma)\\) model’s error term vector \\(n\\) \\(t\\), probit model assumed multivariate normally distributed zero mean covariance matrix \\(\\Sigma\\). Now let \\(y_{nt}=j\\) denote event decision maker \\(n\\) chooses alternative \\(j\\) choice occasion \\(t\\). Assuming utility maximizing behavior decision makers5, decisions linked utilities via \\[\\begin{equation} y_{nt} = {\\arg \\max}_{j = 1,\\dots,J} U_{ntj}. \\end{equation}\\]","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v01_model_definition.html","id":"choice-behavior-heterogeneity","dir":"Articles","previous_headings":"","what":"Choice behavior heterogeneity","title":"Model definition","text":"Note coefficient vector \\(\\beta\\) definition constant across decision makers. assumption restrictive many applications.6 Heterogeneity choice behavior can modeled imposing distribution \\(\\beta\\) decider can preferences. Formally, define \\(\\beta = (\\alpha, \\beta_n)\\), \\(\\alpha\\) \\(P_f\\) coefficients constant across deciders \\(\\beta_n\\) \\(P_r\\) decider-specific coefficients. Consequently, \\(P = P_f + P_r\\). Now \\(P_r>0\\), \\(\\beta_n\\) distributed according \\(P_r\\)-variate distribution, -called mixing distribution. Choosing appropriate mixing distribution notoriously difficult task model specification. many applications, different types standard parametric distributions (including normal, log-normal, uniform tent distribution) tried conjunction likelihood value-based model selection, cf. Train (2009), Chapter 6. Instead, {RprobitB} implements approach (Oelschläger Bauer 2020) approximate underlying mixing distribution mixture (multivariate) Gaussian densities. precisely, underlying mixing distribution \\(g_{P_r}\\) random coefficients \\((\\beta_n)_{n}\\) approximated mixture \\(P_r\\)-variate normal densities \\(\\phi_{P_r}\\) mean vectors \\(b=(b_c)_{c}\\) covariance matrices \\(\\Omega=(\\Omega_c)_{c}\\) using \\(C\\) components, .e. \\[\\begin{equation} \\beta_n\\mid b,\\Omega \\sim \\sum_{c=1}^{C} s_c \\phi_{P_r} (\\cdot \\mid b_c,\\Omega_c). \\end{equation}\\] , \\((s_c)_{c}\\) weights satisfying \\(0 < s_c\\leq 1\\) \\(c=1,\\dots,C\\) \\(\\sum_c s_c=1\\). One interpretation latent class model obtained introducing variables \\(z=(z_n)_n\\), allocating decision maker \\(n\\) class \\(c\\) probability \\(s_c\\), .e. \\[\\begin{equation} \\text{Prob}(z_n=c)=s_c \\land \\beta_n \\mid z,b,\\Omega \\sim \\phi_{P_r}(\\cdot \\mid b_{z_n},\\Omega_{z_n}). \\end{equation}\\] call resulting model latent class mixed multinomial probit model. Note model collapses (normally) mixed multinomial probit model \\(P_r>0\\) \\(C=1\\), multinomial probit model \\(P_r=0\\) binary probit model additionally \\(J=2\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v01_model_definition.html","id":"model-normalization","dir":"Articles","previous_headings":"","what":"Model normalization","title":"Model definition","text":"well known, utility model needs normalized respect level scale order identified (Train 2009). Therefore, consider transformed model \\[\\begin{equation} \\tilde{U}_{ntj} = \\tilde{X}_{ntj}' \\beta + \\tilde{\\epsilon}_{ntj}, \\end{equation}\\] \\(n=1,\\dots,N\\), \\(t=1,\\dots,T\\) \\(j=1,\\dots,J-1\\), (choosing \\(J\\) reference alternative) \\(\\tilde{U}_{ntj} = U_{ntj} - U_{ntJ}\\), \\(\\tilde{X}_{ntj} = X_{ntj} - X_{ntJ}\\), \\(\\tilde{\\epsilon}_{ntj} = \\epsilon_{ntj} - \\epsilon_{ntJ}\\), \\((\\tilde{\\epsilon}_{nt:}) = (\\tilde{\\epsilon}_{nt1},...,\\tilde{\\epsilon}_{nt(J-1)})' \\sim \\text{MVN}_{J-1} (0,\\tilde{\\Sigma})\\) \\(\\tilde{\\Sigma}\\) denotes covariance matrix top-left element restricted one.7","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v01_model_definition.html","id":"parameter-labels-in-rprobitb","dir":"Articles","previous_headings":"","what":"Parameter labels in {RprobitB}","title":"Model definition","text":"{RprobitB}, probit model parameters saved RprobitB_parameter object. labels consistent definition vignette. example: Mind matrix Sigma_full unique can matrix results Sigma differencing, see non-exported function RprobitB:::undiff_Sigma().","code":"RprobitB:::RprobitB_parameter(   P_f   = 1,              P_r   = 2,   J     = 3,   N     = 10,   C     = 2,          # the number of latent classes   alpha = c(1),       # the fixed coefficient vector of length P_f   s     = c(0.6,0.4), # the vector of class weights of length C   b     = matrix(c(-1,1,1,2), nrow = 2, ncol = 2),                                  # the matrix of class means as columns of dimension P_r x C   Omega = matrix(c(diag(2),0.1*diag(2)), nrow = 4, ncol = 2),                              # the matrix of class covariance matrices as columns of dimension P_r^2 x C   Sigma = diag(2),    # the differenced error term covariance matrix of dimension (J-1) x (J-1)                       # the undifferenced error term covariance matrix is labeled 'Sigma_full'   z     = rep(1:2,5)  # the vector of the allocation variables of length N ) #> RprobitB model parameter #>  #> alpha : 1 #>  #> C : 2 #>  #> s : double vector of length 2  #>  #> 0.6 0.4 #>  #> b : 2 x 2 matrix of doubles  #>  #>      [,1] [,2] #> [1,]   -1    1 #> [2,]    1    2 #>  #>  #> Omega : 4 x 2 matrix of doubles  #>  #>      [,1] [,2] #> [1,]    1  0.1 #> [2,]    0  0.0 #> [3,]    0  0.0 #> [4,]    1  0.1 #>  #>  #> Sigma : 2 x 2 matrix of doubles  #>  #>      [,1] [,2] #> [1,]    1    0 #> [2,]    0    1 #>  #>  #> Sigma_full : 3 x 3 matrix of doubles  #>  #>      [,1] [,2] [,3] #> [1,]    2    1    1 #> [2,]    1    2    1 #> [3,]    1    1    1 #>  #>  #> beta : 2 x 10 matrix of doubles  #>  #>         [,1]   [,2]    [,3] ...  [,10] #> [1,] -1.5165 1.8942 -1.9415 ... 1.0058 #> [2,]  1.8753 2.2062  1.1762 ... 2.2632 #>  #>  #> z : integer vector of length 10  #>  #> 1 2 1 ... 2"},{"path":"https://loelschlaeger.de/RprobitB/articles/v01_model_definition.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Model definition","text":"Agresti, . 2015. Foundations Linear Generalized Linear Models. Wiley. Bliss, C. . 1934. “Method Probits.” Science 79 (2037): 38–39. https://doi.org/10.1126/science.79.2037.38. Hewig, J., N. Kretschmer, R. H. Trippe, H. Hecht, M. G. H. Coles, C. B. Holroyd, W. H. R. Miltner. 2011. “Humans Deviate Rational Choice.” Psychophysiology 48 (4): 507–14. https://doi.org/10.1111/j.1469-8986.2010.01081.x. Oelschläger, L., D. Bauer. 2020. “Bayes Estimation Latent Class Mixed Multinomial Probit Models.” TRB Annual Meeting 2021. Train, K. 2009. Discrete Choice Methods Simulation. 2. ed. Cambridge Univ. Press.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"requirements-to-choice-data","dir":"Articles","previous_headings":"","what":"Requirements to choice data","title":"Choice data","text":"{RprobitB} helps modeling choice individual deciders one alternative finite set choice alternatives. choice set fulfill three properties (Train 2009, 16): Choices need mutually exclusive (one can choose one one alternative different), exhaustive (alternatives leave options open), finitely many. {RprobitB} unordered alternatives (, alternatives ranked) modeled. Every decider may take one repeated choices (called choice occasions). data set thus contains information identifier decider (optionally choice situation), choices, alternative decider specific covariates. Additionally, {RprobitB} asks following formal requirements: data set must “wide” format, means row provides full information one choice occasion.2 must contain column unique identifiers decision maker. Additionally, can contain column identifier choice situation decider. information missing, identifier generated automatically appearance choices data set.3 can contain column observed choices. column required model fitting prediction. must contain columns values alternative specific covariate alternative decider specific covariate.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"example","dir":"Articles","previous_headings":"Requirements to choice data","what":"Example","title":"Choice data","text":"Train data set contains 2929 stated choices 235 Dutch individuals deciding two virtual train trip options based price, travel time, level comfort, number changes. fulfills requirements: row represents one choice occasion, columns id choiceid identify deciders choice occasions, respectively. column choice gives observed choices. Four alternative-specific covariates available, namely price, time, change, comfort. values given alternative.4","code":"data(\"Train\", package = \"mlogit\") str(Train) #> 'data.frame':    2929 obs. of  11 variables: #>  $ id       : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ choiceid : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ choice   : Factor w/ 2 levels \"A\",\"B\": 1 1 1 2 2 2 2 2 1 1 ... #>  $ price_A  : num  2400 2400 2400 4000 2400 4000 2400 2400 4000 2400 ... #>  $ time_A   : num  150 150 115 130 150 115 150 115 115 150 ... #>  $ change_A : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ comfort_A: num  1 1 1 1 1 0 1 1 0 1 ... #>  $ price_B  : num  4000 3200 4000 3200 3200 2400 3200 3200 3200 4000 ... #>  $ time_B   : num  150 130 115 150 150 130 115 150 130 115 ... #>  $ change_B : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ comfort_B: num  1 1 0 0 0 0 1 0 1 0 ..."},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"the-model-formula","dir":"Articles","previous_headings":"","what":"The model formula","title":"Choice data","text":"inform {RprobitB} covariates want include model via specifying formula object. Say want model utility \\(U_{n,t,j}\\) decider \\(n\\) choice occasion \\(t\\) alternative \\(j\\) via linear equation \\[U_{n,t,j} = A_{n,t,j} \\beta_1 + B_{n,t} \\beta_{2,j} + C_{n,t,j} \\beta_{3,j} + \\epsilon_{n,tj}.\\] , \\(\\) \\(C\\) alternative choice situation specific covariates, whereas \\(B\\) choice situation specific. coefficient \\(\\beta_1\\) generic (.e. alternative), whereas \\(\\beta_{2,j}\\) \\(\\beta_{3,j}\\) alternative specific. represent structure, formula object form (analogously {mlogit}) choice ~ | B | C, choice dependent variable (discrete choice aim explain), alternative choice situation specific covariates generic coefficient (call covariates type 1), B choice situation specific covariates alternative specific coefficients5 (call covariates type 2), C alternative choice situation specific covariates alternative specific coefficients (call covariates type 3). Specifying formula object {RprobitB} must consistent following rules: default, alternative specific constants (ASCs)6 added model. can removed adding + 0 second spot, e.g. choice ~ | B + 0 | C. exclude covariates backmost categories, use either 0, e.g. choice ~ | B | 0 just leave part write choice ~ | B. However, exclude covariates front categories, use 0, e.g. choice ~ 0 | B. include one covariate category, use +, e.g. choice ~ A1 + A2 | B. don’t want include covariates second category want estimate alternative specific constants, add 1 second spot, e.g. choice ~ | 1. expression choice ~ | 0 interpreted covariates second category alternative specific constants. impose random effects specific variables, need define character vector re corresponding variable names. random effects alternative specific constants, include \"ASC\" re.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"example-1","dir":"Articles","previous_headings":"The model formula","what":"Example","title":"Choice data","text":"specify model formula Train data set. Say want include covariates price, time, comfort, change, alternative specific (, contain potentially different value alternative, different prices B), either type 1 type 3. difference type 1 type 3 former case, estimate generic coefficient (.e. coefficient constant across alternatives), whereas latter case, estimate alternative specific coefficients. Deciding type 1 type 3 covariates belongs topic model selection, provide separate vignette. now, go type 1 covariates remove ASCs: Additionally, specify random effects price time (typically expect heterogeneity ): {RprobitB} provides function check_form() can used check form re correctly interpreted:","code":"form <- choice ~ price + time + comfort + change | 0 re <- c(\"price\",\"time\") check_form(form = form, re = re) #> choice ~ price + time + comfort + change | 0 #> - dependent variable: choice  #> - type 1 covariate(s): price, time, comfort, change  #> - type 2 covariate(s):   #> - type 3 covariate(s):   #> - random effects: price, time  #> - ASC: FALSE"},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"the-prepare_data-function","dir":"Articles","previous_headings":"","what":"The prepare_data() function","title":"Choice data","text":"model estimation {RprobitB}, empirical choice data set choice_data must pass prepare_data() function: function performs compatibility checks data transformations returns object class RprobitB_data can fed estimation routine mcmc. following arguments optional: re: character vector variable names form random effects. re = NULL per default, .e. random effects. alternatives: may want consider alternatives choice_data. case, can specify character vector alternatives selected names alternatives. specified, choice set defined observed choices. id: character (single string), name column choice_data contains unique identifier decision maker. default \"id\". idc: character, name column choice_data contains unique identifier choice situation given decision maker. Per default, identifier generated appearance choices data set. standardize: character vector variable names form get standardized. Covariates type 1 3 addressed <covariate>_<alternative>. standardize = \"\", covariates get standardized. Per default, covariate standardized. impute: Specifies handle missing entries (NA, NaN, -Inf, Inf) choice_data. following options available: \"complete_cases\", removes rows containing missing entries (default), \"zero_out\", replaces missing entries zero, \"mean\", imputes missing entries covariate mean.","code":"data <- prepare_data(form = form, choice_data = choice_data)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"example-2","dir":"Articles","previous_headings":"The prepare_data() function","what":"Example","title":"Choice data","text":"Let’s prepare Train data set estimation previous specification form re: summary plot methods provide quick data overview:","code":"data <- prepare_data(form = form, choice_data = Train, re = re, id = \"id\", idc = \"choiceid\") summary(data) #> Summary of empirical choice data #> 235 decision makers  #> 5 to 19 choice occasions each  #> 2929 choices in total #>  #> Alternatives #>   frequency #> A      1474 #> B      1455 #>  #> Linear coefficients #>      name    re #> 1 comfort FALSE #> 2  change FALSE #> 3   price  TRUE #> 4    time  TRUE plot(data)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"simulate-choices","dir":"Articles","previous_headings":"","what":"Simulate choices","title":"Choice data","text":"simulate_choices function simulates discrete choice data prespecified probit model. Say want simulate choices N deciders T choice occasions7 among J alternatives model formulation form, call function simulate_choices() following optional arguments: re: character vector variable names form random effects. alternatives: character vector length J names choice alternatives. specified, alternatives labeled first J upper-case letters Roman alphabet. covariates: named list covariate values. element must vector length equal number choice occasions named according covariate, follow naming convention alternative specific covariates, .e. <covariate>_<alternative>. Covariates values specified drawn standard normal distribution. standardize: character vector variable names form get standardized. seed: Set seed simulation. can specify true parameters8 adding values alpha, fixed coefficient vector, C, number (greater equal 1) latent classes decision makers, s, vector class weights, b, matrix class means columns, Omega, matrix class covariance matrices columns, Sigma, differenced error term covariance matrix, Sigma_full, full error term covariance matrix, beta, matrix decision-maker specific coefficient vectors, z, class allocation vector. True parameters specified set random.","code":"data <- simulate_choices(form = form, N = N, T = T, J = J)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"example-3","dir":"Articles","previous_headings":"Simulate choices","what":"Example","title":"Choice data","text":"illustration, simulate choices N = 100 deciders T = 10 choice occasions alternatives B: {RprobitB} provides function overview_effects() can used get overview effects parameters can specified: Hence, coefficient vector alpha must length 3, elements 1 3 correspond var1, var3_A, var3_B, respectively. matrix b must dimension 2 x C, (default) C = 1 row 1 2 correspond var2_A ASC_A, respectively. can visualize covariates grouped chosen alternatives:  see consistent specification: Higher values var1_A example correspond frequently choice B (upper-right panel), coefficient var1 (first value alpha) negative.","code":"N <- 100 T <- 10 alternatives <- c(\"A\", \"B\") form <- choice ~ var1 | var2 | var3 re <- c(\"ASC\", \"var2\") overview_effects(form = form, re = re, alternatives = alternatives) #>     name    re #> 1   var1 FALSE #> 2 var3_A FALSE #> 3 var3_B FALSE #> 4 var2_A  TRUE #> 5  ASC_A  TRUE data <- simulate_choices(   form = form,    N = N,    T = T,    J = 2,    re = re,    alternatives = alternatives,    seed = 1,   alpha = c(-1,0,1),   b = matrix(c(2,-0.5), ncol = 1) ) summary(data) #> Summary of simulated choice data #> 100 decision makers  #> 10 choice occasions each  #> 1000 choices in total #>  #> Alternatives #>   frequency #> A       489 #> B       511 #>  #> Linear coefficients #>     name    re #> 1   var1 FALSE #> 2 var3_A FALSE #> 3 var3_B FALSE #> 4 var2_A  TRUE #> 5  ASC_A  TRUE plot(data, by_choice = TRUE)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"train-and-test-data-set","dir":"Articles","previous_headings":"","what":"Train and test data set","title":"Choice data","text":"function train_test() can used split output prepare_data() simulate_choices() train test subset. useful evaluating prediction performance fitted model. example, following code puts 70% deciders simulated data train subsample 30% deciders test subsample: Alternatively, following code puts 2 randomly chosen choice occasions per decider data test subsample, rest goes train subsample:","code":"train_test(data, test_proportion = 0.3, by = \"N\") #> $train #> Simulated data of 700 choices. #>  #> $test #> Simulated data of 300 choices. train_test(data, test_number = 2, by = \"T\", random = TRUE, seed = 1) #> $train #> Simulated data of 800 choices. #>  #> $test #> Simulated data of 200 choices."},{"path":"https://loelschlaeger.de/RprobitB/articles/v02_choice_data.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Choice data","text":"Croissant, Y. 2020. “Estimation Random Utility Models R: mlogit Package.” Journal Statistical Software 95 (11): 1–41. https://doi.org/10.18637/jss.v095.i11. Train, K. 2009. Discrete Choice Methods Simulation. 2. ed. Cambridge Univ. Press.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"bayes-estimation-of-the-probit-model","dir":"Articles","previous_headings":"","what":"Bayes estimation of the probit model","title":"Model fitting","text":"Bayes estimation probit model builds upon work McCulloch Rossi (1994), Nobile (1998), Allenby Rossi (1998), Imai Dyk (2005). key ingredient concept data augmentation, see Albert Chib (1993): idea treat latent utilities \\(U\\) model equation \\(U = X\\beta + \\epsilon\\) additional parameters. , conditional \\(U\\), probit model constitutes standard Bayesian linear regression set-. posterior distribution can approximated iteratively drawing updating model parameter conditional parameters (-called Gibbs sampling approach). priori, assume following (conjugate) parameter distributions: \\((s_1,\\dots,s_C)\\sim D_C(\\delta)\\), \\(D_C(\\delta)\\) denotes \\(C\\)-dimensional Dirichlet distribution concentration parameter vector \\(\\delta = (\\delta_1,\\dots,\\delta_C)\\), \\(\\alpha\\sim \\text{MVN}_{P_f}(\\psi,\\Psi)\\), \\(\\text{MVN}_{P_f}\\) denotes \\(P_f\\)-dimensional normal distribution mean \\(\\psi\\) covariance \\(\\Psi\\), \\(b_c \\sim \\text{MVN}_{P_r}(\\xi,\\Xi)\\), independent \\(c\\), \\(\\Omega_c \\sim W^{-1}_{P_r}(\\nu,\\Theta)\\), independent \\(c\\), \\(W^{-1}_{P_r}(\\nu,\\Theta)\\) denotes \\(P_r\\)-dimensional inverse Wishart distribution \\(\\nu\\) degrees freedom scale matrix \\(\\Theta\\), \\(\\Sigma \\sim W^{-1}_{J-1}(\\kappa,\\Lambda)\\). prior distributions imply following conditional posterior distributions: class weights drawn Dirichlet distribution \\[\\begin{equation} (s_1,\\dots,s_C)\\mid \\delta,z \\sim D_C(\\delta_1+m_1,\\dots,\\delta_C+m_C), \\end{equation}\\] \\(c=1,\\dots,C\\), \\(m_c=\\#\\{n:z_n=c\\}\\) denotes current absolute class size.2 Independently \\(n\\), update allocation variables \\((z_n)_n\\) conditional distribution \\[\\begin{equation} \\text{Prob}(z_n=c\\mid s,\\beta,b,\\Omega )=\\frac{s_c\\phi_{P_r}(\\beta_n\\mid b_c,\\Omega_c)}{\\sum_c s_c\\phi_{P_r}(\\beta_n\\mid b_c,\\Omega_c)}. \\end{equation}\\] class means \\((b_c)_c\\) updated independently \\(c\\) via \\[\\begin{equation} b_c\\mid \\Xi,\\Omega,\\xi,z,\\beta \\sim\\text{MVN}_{P_r}\\left( \\mu_{b_c}, \\Sigma_{b_c}  \\right), \\end{equation}\\] \\(\\mu_{b_c}=(\\Xi^{-1}+m_c\\Omega_c^{-1})^{-1}(\\Xi^{-1}\\xi +m_c\\Omega_c^{-1}\\bar{b}_c)\\), \\(\\Sigma_{b_c}=(\\Xi^{-1}+m_c\\Omega_c^{-1})^{-1}\\), \\(\\bar{b}_c=m_c^{-1}\\sum_{n:z_n=c} \\beta_n\\). class covariance matrices \\((\\Omega_c)_c\\) updated independently \\(c\\) via \\[\\begin{equation} \\Omega_c \\mid \\nu,\\Theta,z,\\beta,b \\sim W^{-1}_{P_r}(\\mu_{\\Omega_c},\\Sigma_{\\Omega_c}), \\end{equation}\\] \\(\\mu_{\\Omega_c}=\\nu+m_c\\) \\(\\Sigma_{\\Omega_c}=\\Theta^{-1} + \\sum_{n:z_n=c} (\\beta_n-b_c)(\\beta_n-b_c)'\\). Independently \\(n\\) \\(t\\) conditionally components, utility vectors \\((U_{nt:})\\) follow \\(J-1\\)-dimensional truncated multivariate normal distribution, truncation points determined choices \\(y_{nt}\\). sample truncated multivariate normal distribution, apply sub-Gibbs sampler, following approach Geweke (1998): \\[\\begin{equation} U_{ntj} \\mid U_{nt(-j)},y_{nt},\\Sigma,W,\\alpha,X,\\beta  \\sim \\mathcal{N}(\\mu_{U_{ntj}},\\Sigma_{U_{ntj}}) \\cdot \\begin{cases} 1(U_{ntj}>\\max(U_{nt(-j)},0) ) & \\text{}~ y_{nt}=j\\\\ 1(U_{ntj}<\\max(U_{nt(-j)},0) ) & \\text{}~ y_{nt}\\neq j \\end{cases}, \\end{equation}\\] \\(U_{nt(-j)}\\) denotes vector \\((U_{nt:})\\) without element \\(U_{ntj}\\), \\(\\mathcal{N}\\) denotes univariate normal distribution, \\(\\Sigma_{U_{ntj}} = 1/(\\Sigma^{-1})_{jj}\\) \\[\\begin{equation} \\mu_{U_{ntj}} = W_{ntj}'\\alpha + X_{ntj}'\\beta_n - \\Sigma_{U_{ntj}} (\\Sigma^{-1})_{j(-j)}   (U_{nt(-j)} - W_{nt(-j)}'\\alpha - X_{nt(-j)}' \\beta_n ), \\end{equation}\\] \\((\\Sigma^{-1})_{jj}\\) denotes \\((j,j)\\)th element \\(\\Sigma^{-1}\\), \\((\\Sigma^{-1})_{j(-j)}\\) \\(j\\)th row without \\(j\\)th entry, \\(W_{nt(-j)}\\) \\(X_{nt(-j)}\\) coefficient matrices \\(W_{nt}\\) \\(X_{nt}\\), respectively, without \\(j\\)th column. Updating fixed coefficient vector \\(\\alpha\\) achieved applying formula Bayesian linear regression regressors \\(W_{nt}\\) regressands \\((U_{nt:})-X_{nt}'\\beta_n\\), .e. \\[\\begin{equation} \\alpha \\mid \\Psi,\\psi,W,\\Sigma,U,X,\\beta \\sim \\text{MVN}_{P_f}(\\mu_\\alpha,\\Sigma_\\alpha), \\end{equation}\\] \\(\\mu_\\alpha = \\Sigma_\\alpha (\\Psi^{-1}\\psi + \\sum_{n=1,t=1}^{N,T} W_{nt} \\Sigma^{-1} ((U_{nt:})-X_{nt}'\\beta_n) )\\) \\(\\Sigma_\\alpha = (\\Psi^{-1} + \\sum_{n=1,t=1}^{N,T} W_{nt}\\Sigma^{-1} W_{nt}^{'} )^{-1}\\). Analogously \\(\\alpha\\), random coefficients \\((\\beta_n)_n\\) updated independently via \\[\\begin{equation} \\beta_n \\mid \\Omega,b,X,\\Sigma,U,W,\\alpha \\sim \\text{MVN}_{P_r}(\\mu_{\\beta_n},\\Sigma_{\\beta_n}), \\end{equation}\\] \\(\\mu_{\\beta_n} = \\Sigma_{\\beta_n} (\\Omega_{z_n}^{-1}b_{z_n} + \\sum_{t=1}^{T} X_{nt} \\Sigma^{-1} (U_{nt}-W_{nt}'\\alpha) )\\) \\(\\Sigma_{\\beta_n} = (\\Omega_{z_n}^{-1} + \\sum_{t=1}^{T} X_{nt}\\Sigma^{-1} X_{nt}^{'} )^{-1}\\) . error term covariance matrix \\(\\Sigma\\) updated means \\[\\begin{equation} \\Sigma \\mid \\kappa,\\Lambda,U,W,\\alpha,X,\\beta \\sim W^{-1}_{J-1}(\\kappa+NT,\\Lambda+S), \\\\ \\end{equation}\\] \\(S = \\sum_{n=1,t=1}^{N,T} \\varepsilon_{nt} \\varepsilon_{nt}'\\) \\(\\varepsilon_{nt} = (U_{nt:}) - W_{nt}'\\alpha - X_{nt}'\\beta_n\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"parameter-normalization","dir":"Articles","previous_headings":"Bayes estimation of the probit model","what":"Parameter normalization","title":"Model fitting","text":"Samples obtained updating scheme described lack identification (except \\(s\\) \\(z\\) draws), compare vignette model definition. Therefore, subsequent sampling, following normalizations required \\(\\)th updates iterations \\(\\): \\(\\alpha^{()} \\cdot \\omega^{()}\\), \\(b_c^{()} \\cdot \\omega^{()}\\), \\(c=1,\\dots,C\\), \\(U_{nt}^{()} \\cdot \\omega^{()}\\), \\(n = 1,\\dots,N\\), \\(t = 1,\\dots,T\\), \\(\\beta_n^{()} \\cdot \\omega^{()}\\), \\(n = 1,\\dots,N\\), \\(\\Omega_c^{()} \\cdot (\\omega^{()})^2\\), \\(c=1,\\dots,C\\), \\(\\Sigma^{()} \\cdot (\\omega^{()})^2\\), either \\(\\omega^{()} = \\sqrt{\\text{const} / (\\Sigma^{()})_{jj}}\\) \\((\\Sigma^{()})_{jj}\\) \\(j\\)th diagonal element \\(\\Sigma^{()}\\), \\(1\\leq j \\leq J-1\\), alternatively \\(\\omega^{()} = \\text{const} / \\alpha^{()}_p\\) coordinate \\(1\\leq p \\leq P_f\\) \\(\\)th draw coefficient vector \\(\\alpha\\). , \\(\\text{const}\\) positive constant (typically 1). preferences flipped \\(\\omega^{()} < 0\\), case \\(\\alpha^{()}_p < 0\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"burn-in-and-thinning","dir":"Articles","previous_headings":"Bayes estimation of the probit model","what":"Burn-in and thinning","title":"Model fitting","text":"theory behind Gibbs sampling constitutes sequence samples produced updating scheme Markov chain stationary distribution equal desired joint posterior distribution. takes certain number iterations stationary distribution approximated reasonably well. Therefore, common practice discard first \\(B\\) \\(R\\) samples (-called burn-period). Furthermore, correlation nearby samples expected. order obtain independent samples, consider every \\(Q\\)th sample computing Gibbs sample statistics like expectation standard deviation. independence samples can verified computing serial correlation convergence Gibbs sampler can checked considering trace plots, see .","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"the-mcmc-function","dir":"Articles","previous_headings":"","what":"The mcmc() function","title":"Model fitting","text":"Gibbs sampling scheme described can executed applying function data must RprobitB_data object (see vignette choice data). function following optional arguments: scale: named list three elements, determining parameter normalization: parameter: Either \"\" (element alpha) \"s\" (diagonal element Sigma).3 index: index parameter gets fixed. value: value fixed parameter (.e. value \\(\\text{const}\\) introduced ). Per default, first error-term variance fixed 1, .e. scale = list(\"parameter\" = \"s\", \"index\" = 1, \"value\" = 1). R: number iterations Gibbs sampler. default R = 10000. B: length burn-period, .e. non-negative number samples discarded. default B = R/2. Q: thinning factor Gibbs samples, .e. every Qth sample kept. default Q = 1. print_progress: boolean, determining whether print Gibbs sampler progress. prior: named list parameters prior distributions (default values documented check_prior() function): eta: mean vector length P_f normal prior alpha. Psi: covariance matrix dimension P_f x P_f normal prior alpha. delta: concentration parameter length 1 Dirichlet prior s. xi: mean vector length P_r normal prior b_c. D: covariance matrix dimension P_r x P_r normal prior b_c. nu: degrees freedom (natural number greater P_r) Inverse Wishart prior Omega_c. Theta: scale matrix dimension P_r x P_r Inverse Wishart prior Omega_c. kappa: degrees freedom (natural number greater J-1) Inverse Wishart prior Sigma. E: scale matrix dimension J-1 x J-1 Inverse Wishart prior Sigma. latent_classes: list parameters specifying number updating scheme latent classes, see vignette modeling heterogeneity fitting.","code":"mcmc(data = data)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Model fitting","text":"previous vignette choice data, introduced Train data set mlogit package (Croissant 2020) contains 2922 choices two fictional train route alternatives. First, transform travel time minutes hours travel price guilders euros: following lines fit probit model explains chosen trip alternatives (choice) price, time, number changes, level comfort (lower value higher comfort). normalization, first linear coefficient, price, fixed -1, allows interpret coefficients monetary values: estimated model saved {RprobitB} can accessed via estimated coefficients (using mean Gibbs samples point estimate) can printed via visualized via  results indicate deciders value one hour travel time 25€, additional change 5€, comfortable class 14€.4","code":"data(\"Train\", package = \"mlogit\") Train$price_A <- Train$price_A / 100 * 2.20371 Train$price_B <- Train$price_B / 100 * 2.20371 Train$time_A <- Train$time_A / 60 Train$time_B <- Train$time_B / 60 form <- choice ~ price + time + change + comfort | 0 data <- prepare_data(form = form, choice_data = Train) model_train <- mcmc(   data = data,   scale = list(\"parameter\" = \"a\", index = 1, value = -1) ) data(model_train, package = \"RprobitB\") coef(model_train) #>            Estimate   (sd) #> 1   price     -1.00 (0.00) #> 2    time    -25.39 (2.23) #> 3  change     -4.79 (0.86) #> 4 comfort    -14.40 (0.90) plot(coef(model_train), sd = 3)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"checking-the-gibbs-samples","dir":"Articles","previous_headings":"","what":"Checking the Gibbs samples","title":"Model fitting","text":"Gibbs samples saved list form RprobitB_fit object entry \"gibbs_samples\", .e. object contains 2 elements: gibbs_samples_raw list raw samples Gibbs sampler, gibbs_samples_nbt Gibbs samples used parameter estimates, .e. normalized thinned Gibbs samples burn-. Calling summary function estimated RprobitB_fit object yields additional information Gibbs samples gibbs_samples_nbt. can specify list FUN functions compute point estimate Gibbs samples5, example mean arithmetic mean, stats::sd standard deviation, R_hat Gelman-Rubin statistic (Gelman Rubin 1992),6 custom statistics like absolute difference median mean. Calling plot method additional argument type = \"trace\" plots trace Gibbs samples gibbs_samples_nbt:  Additionally, can visualize serial correlation Gibbs samples via argument type = \"acf\". boxes top-right corner state total sample size TSS (R - B = 10000 - 5000 = 5000), effective sample size ESS, factor TSS larger ESS.  , effective sample size value \\(\\text{TSS} / (1 + \\sum_{k\\geq 1} \\rho_k)\\), \\(\\rho_k\\) auto correlation chain offset \\(k\\) positions. auto correlations estimated via stats::acf() function.","code":"str(model_train$gibbs_samples, max.level = 2, give.attr = FALSE) #> List of 2 #>  $ gibbs_samples_nbt:List of 2 #>   ..$ alpha: num [1:500, 1:4] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ... #>   ..$ Sigma: num [1:500, 1] 607 581 627 615 850 ... #>  $ gibbs_samples_raw:List of 2 #>   ..$ alpha: num [1:10000, 1:4] -0.000739 -0.017192 -0.027834 -0.034068 -0.035992 ... #>   ..$ Sigma: num [1:10000, 1] 1.022 0.992 0.941 0.921 0.905 ... summary(model_train,          FUN = c(\"mean\"        = mean,                  \"sd\"          = stats::sd,                  \"R^\"          = R_hat,                 \"custom_stat\" = function(x) abs(mean(x) - median(x))                 )        ) #> Probit model #> choice ~ price + time + change + comfort | 0  #> R: 10000  #> B: 5000  #> Q: 10  #>  #> Normalization #> Level: Utility differences with respect to alternative 2. #> Scale: Coefficient of alpha_1 fixed to -1. #>  #> Gibbs sample statistics #>                mean           sd           R^  custom_stat #>  alpha #>                                                            #>      1        -1.00         0.00         1.00         0.00 #>      2       -25.39         2.23         1.01         0.00 #>      3        -4.79         0.86         1.00         0.02 #>      4       -14.40         0.90         1.00         0.01 #>  #>  Sigma #>                                                            #>    1,1       658.58        62.47         1.00         6.38 par(mfrow = c(2,1)) plot(model_train, type = \"trace\") par(mfrow = c(2,3)) plot(model_train, type = \"acf\")"},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"model-transformation-after-estimation","dir":"Articles","previous_headings":"","what":"Model transformation after estimation","title":"Model fitting","text":"transform method can used transform RprobitB_fit object three ways: change length B burn-period, example change thinning factor Q Gibbs samples, example change model normalization scale, example","code":"model_train <- transform(model_train, B = 1) model_train <- transform(model_train, Q = 100) model_train <- transform(model_train, scale = list(parameter = \"s\", index = 1, value = 1))"},{"path":"https://loelschlaeger.de/RprobitB/articles/v03_model_fitting.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Model fitting","text":"Albert, J. H., S. Chib. 1993. “Bayesian Analysis Binary Polychotomous Response Data.” Journal American Statistical Association 88. Allenby, G. M., P. Rossi. 1998. “Marketing Models Consumer Heterogeneity.” Journal Econometrics 89. Croissant, Y. 2020. “Estimation Random Utility Models R: mlogit Package.” Journal Statistical Software 95 (11): 1–41. https://doi.org/10.18637/jss.v095.i11. Gelman, ., D. B. Rubin. 1992. “Inference Iterative Simulation Using Multiple Sequences.” Statistical Science 7 (4): 457–72. https://doi.org/10.1214/ss/1177011136. Geweke, J. 1998. “Efficient Simulation Multivariate Normal Student-T Distributions Subject Linear Constraints Evaluation Constraint Probabilities.” Comput. Sci. Statist. Imai, K., D. . van Dyk. 2005. “Bayesian Analysis Multinomial Probit Model Using Marginal Data Augmentation.” Journal Econometrics 124. McCulloch, R., P. Rossi. 1994. “Exact Likelihood Analysis Multinomial Probit Model.” Journal Econometrics 64. Nobile, . 1998. “Hybrid Markov Chain Bayesian Analysis Multinomial Probit Model.” Statistics Computing 8.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v04_modeling_heterogeneity.html","id":"estimating-a-joint-normal-mixing-distribution","dir":"Articles","previous_headings":"","what":"Estimating a joint normal mixing distribution","title":"Modeling heterogeneity","text":"{mlogit} package (Croissant 2020) contains data set Electricity, residential electricity customers asked decide four hypothetical electricity suppliers. suppliers differed 6 characteristics: fixed price pf per kWh, contract length cf, boolean loc, indicating whether supplier local company, boolean wk, indicating whether supplier well known company, boolean tod, indicating whether supplier offers time--day electricity price (higher day lower night), boolean seas, indicating whether supplier’s price seasonal dependent. constitutes choice situation choice behavior heterogeneity expected: customers might prefer time--day electricity price (may home day), others can opposite preference. Ideally differences preferences modeled using characteristics deciders. many cases (data set) adequate information. Instead differences taste can captured means mixing distribution tod coefficient. corresponds assumption random coefficient underlying mixing distribution drawn decider. can use estimated mixing distribution determine example share deciders positive versus negative preference towards time--day electricity prices. Additionally, expect correlations random coefficients certain covariates, example positive correlation influence loc wk: deciders prefer local suppliers might also prefer well known companies due recommendations past experiences, although might expensive unknown suppliers. fitted multivariate normal distribution reveal correlations. following lines prepare Electricity data set estimation. use convenience function as_cov_names() relabels data columns alternative specific covariates required format “<covariate>_<alternative>”, compare vignette choice data. Via re = c(\"cl\",\"loc\",\"wk\",\"tod\",\"seas\") argument, specify want model random effects price coefficient, fix -1 interpret estimates monetary values. estimated model saved {RprobitB} can accessed via: Calling coef() method estimated model also returns estimated (marginal) variances mixing distribution besides average mean effects: sign estimates can example deduce, existence time--day electricity price tod contract negative effect. However, deciders heterogeneous , estimated variance coefficient large (12.37). holds contract length cl. particular, estimated share population prefers longer contract length equals: correlation covariates can accessed follows:2 , see confirmation initial assumption high correlation loc wk. pairwise mixing distributions can visualized via calling plot() method additional argument type = mixture:","code":"data(\"Electricity\", package = \"mlogit\") Electricity <- as_cov_names(Electricity, c(\"pf\",\"cl\",\"loc\",\"wk\",\"tod\",\"seas\"), 1:4) data <- prepare_data(   form = choice ~ pf + cl + loc + wk + tod + seas | 0,   choice_data = Electricity,   re = c(\"cl\",\"loc\",\"wk\",\"tod\",\"seas\")   ) model_elec <- mcmc(data, R = 1000, scale = list(parameter = \"a\", index = 1, value = -1)) data(model_elec, package = \"RprobitB\") coef(model_elec) #>         Estimate   (sd) Variance   (sd) #> 1   pf     -1.00 (0.00)       NA   (NA) #> 2   cl     -0.25 (0.03)     0.23 (0.04) #> 3  loc      2.75 (0.26)     6.79 (1.25) #> 4   wk      2.00 (0.19)     3.39 (0.67) #> 5  tod     -9.71 (0.25)    10.53 (2.18) #> 6 seas     -9.83 (0.22)     5.74 (1.14) cl_mu <- coef(model_elec)[\"cl\",\"mean\"] cl_sd <- sqrt(coef(model_elec)[\"cl\",\"var\"]) pnorm(cl_mu / cl_sd) #> [1] 0.3026362 cov_mix(model_elec, cor = TRUE) #>               cl         loc          wk         tod        seas #> cl    1.00000000  0.11510129  0.06972935 -0.06581389 -0.13741649 #> loc   0.11510129  1.00000000  0.81363601  0.09436351 -0.02207951 #> wk    0.06972935  0.81363601  1.00000000  0.09713654 -0.05208165 #> tod  -0.06581389  0.09436351  0.09713654  1.00000000  0.53757657 #> seas -0.13741649 -0.02207951 -0.05208165  0.53757657  1.00000000 plot(model_elec, type = \"mixture\")"},{"path":"https://loelschlaeger.de/RprobitB/articles/v04_modeling_heterogeneity.html","id":"estimating-latent-classes","dir":"Articles","previous_headings":"","what":"Estimating latent classes","title":"Modeling heterogeneity","text":"generally, {RprobitB} allows specify Gaussian mixture mixing distribution. particular, \\[ \\beta \\sim \\sum_{c=1}^C \\text{MVN} (b_c,\\Omega_c).\\] specification allows ) better approximation true underlying mixing distribution b) preference based classification deciders. estimate latent mixture, specify named list latent_classes following arguments submit estimation routine mcmc: C, fixed number (greater equal 1) latent classes, set 1 per default,3 weight_update, boolean, set TRUE weight-based update latent classes, see , dp_update, boolean, set TRUE Dirichlet process-based update latent classes, see , Cmax, maximum number latent classes, set 10 per default.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v04_modeling_heterogeneity.html","id":"weight-based-update-of-the-latent-classes","dir":"Articles","previous_headings":"Estimating latent classes","what":"Weight-based update of the latent classes","title":"Modeling heterogeneity","text":"following weight-based updating scheme analogue Bauer, Büscher, Batram (2019) executed within burn-period: remove class \\(c\\), \\(s_c<\\varepsilon_{\\text{min}}\\), .e. class weight \\(s_c\\) drops threshold \\(\\varepsilon_{\\text{min}}\\). case indicates class \\(c\\) negligible impact mixing distribution. split class \\(c\\) two classes \\(c_1\\) \\(c_2\\), \\(s_c>\\varepsilon_\\text{max}\\). case indicates class \\(c\\) high influence mixing distribution whose approximation can potentially improved increasing resolution directions high variance. Therefore, class means \\(b_{c_1}\\) \\(b_{c_2}\\) new classes \\(c_1\\) \\(c_2\\) shifted opposite directions class mean \\(b_c\\) old class \\(c\\) direction highest variance. join two classes \\(c_1\\) \\(c_2\\) one class \\(c\\), \\(\\lVert b_{c_1} - b_{c_2} \\rVert<\\varepsilon_{\\text{distmin}}\\), .e. euclidean distance class means \\(b_{c_1}\\) \\(b_{c_2}\\) drops threshold \\(\\varepsilon_{\\text{distmin}}\\). case indicates location redundancy repealed. parameters \\(c\\) assigned adding values \\(s\\) \\(c_1\\) \\(c_2\\) averaging values \\(b\\) \\(\\Omega\\). rules contain choices values \\(\\varepsilon_{\\text{min}}\\), \\(\\varepsilon_{\\text{max}}\\) \\(\\varepsilon_{\\text{distmin}}\\). adequate value \\(\\varepsilon_{\\text{distmin}}\\) depends scale parameters. Per default, {RprobitB} sets epsmin = 0.01, epsmax = 0.99, distmin = 0.1. values can adapted latent_class list.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v04_modeling_heterogeneity.html","id":"dirichlet-process-based-update-of-the-latent-classes","dir":"Articles","previous_headings":"Estimating latent classes","what":"Dirichlet process-based update of the latent classes","title":"Modeling heterogeneity","text":"alternative weight-based updating scheme determine correct number \\(C\\) latent classes, {RprobitB} implemented Dirichlet process.4 Dirichlet Process Bayesian nonparametric method, nonparametric means number model parameters can theoretically grow infinity. method allows add mixture components mixing distribution needed better approximation, see Neal (2000) documentation general case. literature offers many representations method, including Chinese Restaurant Process (Aldous 1985), stick-braking methapor (Sethuraman 1994), Polya Urn model (Blackwell MacQueen 1973). case, face situation find distribution \\(g\\) explains decider-specific coefficients \\((\\beta_n)_{n = 1,\\dots,N}\\), \\(g\\) supposed mixture unknown number \\(C\\) Gaussian densities, .e. \\(g = \\sum_{c = 1,\\dots,C} s_c \\text{MVN}(b_c, \\Omega_c)\\). Let \\(z_n \\\\{1,\\dots,C\\}\\) denote class membership \\(\\beta_n\\). priori, mixture weights \\((s_c)_c\\) given Dirichlet prior concentration parameter \\(\\delta/C\\), .e. \\((s_c)_c \\mid \\delta \\sim \\text{D}_C(\\delta/C,\\dots,\\delta/C)\\). Rasmussen (2000) shows \\[ \\Pr((z_n)_n\\mid \\delta) = \\frac{\\Gamma(\\delta)}{\\Gamma(N+\\delta)} \\prod_{c=1}^C \\frac{\\Gamma(m_c + \\delta/C)}{\\Gamma(\\delta/C)}, \\] \\(\\Gamma(\\cdot)\\) denotes gamma function \\(m_c = \\#\\{n:z_n = c\\}\\) number elements currently allocated class \\(c\\). Crucially, last equation independent class weights \\((s_c)_c\\), yet still depends finite number \\(C\\) latent classes. However, Li, Schofield, Gönen (2019) shows \\[ \\Pr(z_n = c \\mid z_{-n}, \\delta) = \\frac{m_{c,-n} + \\delta/C}{N-1+\\delta},\\] notation \\(-n\\) means excluding \\(n\\)th element. can let \\(C\\) approach infinity derive: \\[ \\Pr(z_n = c \\mid z_{-n}, \\delta) \\\\frac{m_{c,-n}}{N-1+\\delta}. \\] Note allocation probabilities sum 1, instead \\[ \\sum_{c = 1}^C \\frac{m_{c,-n}}{N-1+\\delta} = \\frac{N-1}{N-1+\\delta}. \\] difference 1 equals \\[ \\Pr(z_n \\neq z_m ~ \\forall ~ m \\neq n \\mid z_{-n}, \\delta) = \\frac{\\delta}{N-1+\\delta} \\] constitutes probability new cluster observation \\(n\\) created. Neal (2000) points probability proportional prior parameter \\(\\delta\\): greater value \\(\\delta\\) encourages creation new clusters, smaller value \\(\\delta\\) increases probability allocation already existing class. summary, Dirichlet process updates allocation \\(\\beta\\) coefficient vector one time, dependent allocations. number clusters can theoretically rise infinity, however, delete unoccupied clusters, \\(C\\) bounded \\(N\\). final step allocation update, update class means \\(b_c\\) covariance matrices \\(\\Omega_c\\) means posterior predictive distribution. mean covariance matrix new generated cluster drawn prior predictive distribution. corresponding formulas given Li, Schofield, Gönen (2019). Dirichlet process directly integrates existing Gibbs sampler. Given \\(\\beta\\) values, updated class means \\(b_c\\) class covariance matrices \\(\\Omega_c\\). Dirichlet process updating scheme implemented function update_classes_dp(). following, give small example bivariate case P_r = 2. sample true class means b_true class covariance matrices Omega_true C_true = 3 true latent classes. true (unbalanced) class sizes given vector N, z_true denotes true allocations. specify following prior parameters (definition see vignette model fitting): Initially, start C = 1 latent classes. class mean b set zero, covariance matrix Omega identity matrix: following call update_classes_dp() updates latent classes 100 iterations. Note specify arguments Cmax s_desc. former denotes maximum number latent classes. specification requirement Dirichlet process per se, rather implementation. Knowing maximum possible class number, can allocate required memory space, leads speed improvement. later can verify won’t exceed number Cmax = 10 latent classes point Dirichlet process. Setting s_desc = TRUE ensures classes ordered weights descending order ensure identifiability. Dirichlet process able infer true number C_true = 3 latent classes:","code":"set.seed(1) P_r <- 2 C_true <- 3 N <- c(100,70,30) (b_true <- matrix(replicate(C_true, rnorm(P_r)), nrow = P_r, ncol = C_true)) #>            [,1]       [,2]       [,3] #> [1,] -0.6264538 -0.8356286  0.3295078 #> [2,]  0.1836433  1.5952808 -0.8204684 (Omega_true <- matrix(replicate(C_true, rwishart(P_r + 1, 0.1*diag(P_r))$W, simplify = TRUE),                        nrow = P_r*P_r, ncol = C_true)) #>           [,1]        [,2]       [,3] #> [1,] 0.3093652  0.14358543  0.2734617 #> [2,] 0.1012729 -0.07444148 -0.1474941 #> [3,] 0.1012729 -0.07444148 -0.1474941 #> [4,] 0.2648235  0.05751780  0.2184029 beta <- c() for(c in 1:C_true) for(n in 1:N[c])   beta <- cbind(beta, rmvnorm(mu = b_true[,c,drop=F], Sigma = matrix(Omega_true[,c,drop=F], ncol = P_r))) z_true <- rep(1:3, times = N) delta <- 0.1 xi <- numeric(P_r) D <- diag(P_r) nu <- P_r + 2 Theta <- diag(P_r) z <- rep(1, ncol(beta)) C <- 1 b <- matrix(0, nrow = P_r, ncol = C) Omega <- matrix(rep(diag(P_r), C), nrow = P_r*P_r, ncol = C) for(r in 1:100){   dp <- RprobitB:::update_classes_dp(     Cmax = 10, beta, z, b, Omega, delta, xi, D, nu, Theta, s_desc = TRUE     )   z <- dp$z   b <- dp$b   Omega <- dp$Omega } par(mfrow = c(1,2)) plot(t(beta), xlab = bquote(beta[1]), ylab = bquote(beta[2]), pch = 19) RprobitB:::plot_class_allocation(beta, z, b, Omega, r = 100, perc = 0.95)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v04_modeling_heterogeneity.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Modeling heterogeneity","text":"Aldous, D. J. 1985. “Exchangeability Related Topics,” Lecture notes math., 1117: 1–198. Bauer, D., S. Büscher, M. Batram. 2019. “Non-Parameteric Estiation Mixed Discrete Choice Models.” Second International Choice Modelling Conference Kobe. Blackwell, D., J. MacQueen. 1973. “Ferguson Distributions via Polya Urn Schemes.” Annals Statistics 1: 353–55. Burda, M., M. Harding, J. Hausman. 2008. “Bayesian Mixed Logit–Probit Model Multinomial Choice.” Journal Econometrics 147 (2): 232–46. https://doi.org/10.1016/j.jeconom.2008.09.029. Croissant, Y. 2020. “Estimation Random Utility Models R: mlogit Package.” Journal Statistical Software 95 (11): 1–41. https://doi.org/10.18637/jss.v095.i11. Li, Y., E. Schofield, M. Gönen. 2019. “Tutorial Dirichlet Process Mixture Modeling.” Journal Mathematical Psychology 91: 128–44. https://doi.org/10.1016/j.jmp.2019.04.004. Neal, R. M. 2000. “Markov Chain Sampling Methods Dirichlet Process Mixture Models.” Journal Computational Graphical Statistics 9 (2): 249–65. http://www.jstor.org/stable/1390653. Rasmussen, C. E. 2000. Infinite Gaussian Mixture Model. Edited S. Solla, T. Leen, K. Müller. Vol. 12. MIT Press. Sethuraman, J. 1994. “Constructive Definition Dirichlet Priors.” Statistica Sinica 4 (2): 639–50. http://www.jstor.org/stable/24305538.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v05_choice_prediction.html","id":"reproducing-the-observed-choices","dir":"Articles","previous_headings":"","what":"Reproducing the observed choices","title":"Choice prediction","text":"{RprobitB} provides predict() method RprobitB_fit objects. Per default, method returns confusion matrix, gives overview -sample prediction performance: setting argument overview = FALSE, method instead returns predictions level individual choice occasions: Among three incorrect predictions shown , one decider id = 1 choice occasion idc = 8 outstanding. Alternative B chosen although model predicts probability 75% alternative . can use convenience function get_cov() extract characteristics particular choice situation: trip option 20€ cheaper 30 minutes faster, model outweighs better comfort class alternative B, recall estimated effects: misclassification can explained preferences differ average decider (choice behavior heterogeneity), unobserved factors influenced choice. Indeed, variance error term estimated high: Apart prediction accuracy, model performance can evaluated nuanced terms sensitivity specificity. following snippet exemplary shows visualize measures means receiver operating characteristic (ROC) curve (Fawcett 2006), using {plotROC} package (Sachs 2017). curve constructed plotting true positive fraction false positive fraction various cutoffs (n.cuts = 20). closer curve top-left corner, better binary classification.","code":"predict(model_train) #>     predicted #> true    A    B #>    A 1022  452 #>    B  440 1015 pred <- predict(model_train, overview = FALSE)  head(pred, n = 10) #>    id choiceid         A          B true predicted correct #> 1   1        1 0.9152710 0.08472896    A         A    TRUE #> 2   1        2 0.6395050 0.36049499    A         A    TRUE #> 3   1        3 0.7918220 0.20817801    A         A    TRUE #> 4   1        4 0.1792245 0.82077546    B         B    TRUE #> 5   1        5 0.5500492 0.44995085    B         A   FALSE #> 6   1        6 0.1299623 0.87003770    B         B    TRUE #> 7   1        7 0.5436992 0.45630079    B         A   FALSE #> 8   1        8 0.7589727 0.24102733    B         A   FALSE #> 9   1        9 0.5483893 0.45161074    A         A    TRUE #> 10  1       10 0.5931064 0.40689358    A         A    TRUE get_cov(model_train, id = 1, idc = 8) #>   id choiceid choice  price_A   time_A change_A comfort_A  price_B time_B #> 8  1        8      B 52.88904 1.916667        0         1 70.51872    2.5 #>   change_B comfort_B #> 8        0         0 coef(model_train) #>            Estimate   (sd) #> 1   price     -1.00 (0.00) #> 2    time    -25.39 (2.23) #> 3  change     -4.79 (0.86) #> 4 comfort    -14.40 (0.90) point_estimates(model_train)$Sigma #>          [,1] #> [1,] 658.5798 #> attr(,\"names\") #> [1] \"1,1\" library(plotROC) ggplot(data = pred, aes(m = A, d = ifelse(true == \"A\", 1, 0))) +    geom_roc(n.cuts = 20, labels = FALSE) +    style_roc(theme = theme_grey)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v05_choice_prediction.html","id":"forecasting-choice-behavior","dir":"Articles","previous_headings":"","what":"Forecasting choice behavior","title":"Choice prediction","text":"predict() method additional data argument. Per default, data = NULL, results -sample case outlined . Alternatively, data can either RprobitB_data object (example test subsample derived train_test() function, see vignette choice data), data frame custom choice characteristics. demonstrate second case following. Assume train company wants anticipate effect price increase market share. model, increasing ticket price 100€ 110€ (ceteris paribus) draws 15% customers competitor increase prices. However, offering better comfort class compensates higher price even results gain 7% market share:","code":"predict(   model_train,    data = data.frame(\"price_A\" = c(100,110),                      \"price_B\" = c(100,100)),   overview = FALSE) #>   id choiceid         A         B prediction #> 1  1        1 0.5000000 0.5000000          A #> 2  2        1 0.3483907 0.6516093          B predict(   model_train,    data = data.frame(\"price_A\"   = c(100,110),                      \"comfort_A\" = c(1,0),                     \"price_B\"   = c(100,100),                     \"comfort_B\" = c(1,1)),   overview = FALSE) #>   id choiceid         A         B prediction #> 1  1        1 0.5000000 0.5000000          A #> 2  2        1 0.5680923 0.4319077          A"},{"path":"https://loelschlaeger.de/RprobitB/articles/v05_choice_prediction.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Choice prediction","text":"Fawcett, T. 2006. “Introduction Roc Analysis.” Pattern Recognition Letters 27 (8): 861–74. https://doi.org/10.1016/j.patrec.2005.10.010. Sachs, M. C. 2017. “plotROC: Tool Plotting Roc Curves.” Journal Statistical Software, Code Snippets 79 (2): 1–19. https://doi.org/10.18637/jss.v079.c02.","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"the-model_selection-function","dir":"Articles","previous_headings":"","what":"The model_selection() function","title":"Model selection","text":"{RprobitB} provides convenience function model_selection(), takes arbitrary number RprobitB_model objects returns matrix model selection criteria: Specifying criteria optional. Per default, criteria = c(\"npar\", \"LL\", \"AIC\", \"BIC\").3 available model selection criteria explained following.","code":"model_selection(model_train, model_train_sparse,                  criteria = c(\"npar\", \"LL\", \"AIC\", \"BIC\", \"WAIC\", \"MMLL\", \"BF\", \"pred_acc\")) #>                          model_train model_train_sparse #> npar                               4                  1 #> LL                          -1727.74           -1865.86 #> AIC                          3463.48            3733.72 #> BIC                          3487.41            3739.70 #> WAIC                         3463.76            3733.91 #> se(WAIC)                        0.18               0.07 #> pWAIC                           4.32               1.15 #> MMLL                        -1732.14           -1867.48 #> BF(*,model_train)                  1             < 0.01 #> BF(*,model_train_sparse)       > 100                  1 #> pred_acc                      69.55%             63.37%"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"npar","dir":"Articles","previous_headings":"The model_selection() function","what":"npar","title":"Model selection","text":"\"npar\" yields number model parameters, computed npar() method: , model_train 4 parameters (coefficient price, time, change, comfort, respectively), model_train_sparse single price coefficient.","code":"npar(model_train, model_train_sparse) #> [1] 4 1"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"ll","dir":"Articles","previous_headings":"The model_selection() function","what":"LL","title":"Model selection","text":"\"LL\" included criteria, model_selection() returns model’s log-likelihood values. can also directly accessed via logLik() method:4","code":"logLik(model_train) #> [1] -1727.742 logLik(model_train_sparse) #> [1] -1865.861"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"aic","dir":"Articles","previous_headings":"The model_selection() function","what":"AIC","title":"Model selection","text":"Including \"AIC\" yields Akaike’s Information Criterion (Akaike 1974), computed \\[-2 \\cdot \\text{LL} + k \\cdot \\text{npar},\\] \\(\\text{LL}\\) model’s log-likelihood value, \\(k\\) penalty per parameter \\(k = 2\\) per default classical AIC, \\(\\text{npar}\\) number parameters fitted model. Alternatively, AIC() method also returns AIC values: AIC quantifies trade-- -fitting, smaller values preferred. , increase goodness fit justifies additional 3 parameters model_train.","code":"AIC(model_train, model_train_sparse, k = 2) #> [1] 3463.485 3733.723"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"bic","dir":"Articles","previous_headings":"The model_selection() function","what":"BIC","title":"Model selection","text":"Similar AIC, \"BIC\" yields Bayesian Information Criterion (Schwarz 1978), defined \\[-2 \\cdot \\text{LL} + \\log{(\\text{nobs})} \\cdot \\text{npar},\\] \\(\\text{LL}\\) model’s log-likelihood value, \\(\\text{nobs}\\) number data points (can accessed via nobs() method), \\(\\text{npar}\\) number parameters fitted model. interpretation analogue AIC. {RprobitB} also provided method BIC value:","code":"BIC(model_train, model_train_sparse) #> [1] 3487.414 3739.705"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"waic-with-sewaic-and-pwaic","dir":"Articles","previous_headings":"The model_selection() function","what":"WAIC (with se(WAIC) and pWAIC)","title":"Model selection","text":"WAIC short Widely Applicable (Watanabe-Akaike) Information Criterion (Watanabe Opper 2010). AIC BIC, smaller WAIC value better model. Including \"WAIC\" criteria yields WAIC value, standard error se(WAIC), effective number parameters pWAIC, see . WAIC defined \\[-2  \\cdot \\text{lppd} + 2\\cdot p_\\text{WAIC},\\] \\(\\text{lppd}\\) stands log pointwise predictive density \\(p_\\text{WAIC}\\) penalty term proportional variance posterior distribution sometimes called effective number parameters, see McElreath (2020) p. 220 reference. \\(\\text{lppd}\\) approximated follows. Let \\[p_{si} = \\Pr(y_i\\mid \\theta_s)\\] probability observation \\(y_i\\) (single choices) given \\(s\\)-th set \\(\\theta_s\\) parameter samples posterior. \\[\\text{lppd} = \\sum_i \\log \\left( S^{-1} \\sum_s p_{si} \\right).\\] penalty term computed sum variances log-probability observation: \\[p_\\text{WAIC} = \\sum_i \\mathbb{V}_{\\theta}  \\log (p_{si}) . \\] \\(\\text{WAIC}\\) standard error \\[\\sqrt{n \\cdot \\mathbb{V}_i \\left[-2 \\left(\\text{lppd} - \\mathbb{V}_{\\theta}  \\log (p_{si})  \\right)\\right]},\\] \\(n\\) number choices. computing WAIC  object, probabilities \\(p_{si}\\) must computed via compute_p_si() function:5 Afterwards, WAIC can accessed follows, number brackets standard error: can visualize convergence WAIC follows:   , approximations look satisfactory. WAIC value seem converged, use Gibbs samples increasing R mcmc() decreasing B Q via transform(), see vignette model fitting.","code":"model_train <- compute_p_si(model_train) WAIC(model_train) #> 3463.76 (0.18) WAIC(model_train_sparse) #> 3733.91 (0.07) plot(WAIC(model_train)) plot(WAIC(model_train_sparse))"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"mmll","dir":"Articles","previous_headings":"The model_selection() function","what":"MMLL","title":"Model selection","text":"\"MMLL\" criteria stands marginal model log-likelihood. model’s marginal likelihood \\(\\Pr(y\\mid M)\\) model \\(M\\) data \\(y\\) required computation Bayes factors, see . general, term closed form must approximated numerically. {RprobitB} uses posterior Gibbs samples derived mcmc() function approximate model’s marginal likelihood via posterior harmonic mean estimator (Newton Raftery 1994): Let \\(S\\) denote number available posterior samples \\(\\theta_1,\\dots,\\theta_S\\). , \\[\\Pr(y\\mid M) = \\left(\\mathbb{E}_\\text{posterior} 1/\\Pr(y\\mid \\theta,M) \\right)^{-1} \\approx \\left( \\frac{1}{S} \\sum_s 1/\\Pr(y\\mid \\theta_s,M) \\right) ^{-1} = \\tilde{\\Pr}(y\\mid M).\\] law large numbers, \\(\\tilde{\\Pr}(y\\mid M) \\\\Pr(y\\mid M)\\) almost surely \\(S \\\\infty\\). WAIC, computing MMLL relies probabilities \\(p_{si} = \\Pr(y_i\\mid \\theta_s)\\), must first computed via compute_p_si() function. Afterwards, mml() function can called RprobitB_fit object input. function returns RprobitB_fit object, marginal likelihood value saved entry \"mml\" marginal log-likelihood value attribute \"mmll\"::6 Analogue WAIC value, computation MMLL approximation improves rising (posterior) sample size. convergence can verified visually via plot() method:7  two options improving approximation: WAIC, can use posterior samples. Alternatively, can combine posterior harmonic mean estimate prior arithmetic mean estimator (Hammersley Handscomb 1964): approach, \\(S\\) samples \\(\\theta_1,\\dots,\\theta_S\\) drawn model’s prior distribution. , \\[\\Pr(y\\mid M) = \\mathbb{E}_\\text{prior} \\Pr(y\\mid \\theta,M) \\approx \\frac{1}{S} \\sum_s \\Pr(y\\mid \\theta_s,M) = \\tilde{\\Pr}(y\\mid M).\\] , hols law large numbers, \\(\\tilde{\\Pr}(y\\mid M) \\\\Pr(y\\mid M)\\) almost surely \\(S \\\\infty\\). final approximation model’s marginal likelihood weighted sum posterior harmonic mean estimate prior arithmetic mean estimate, weights determined sample sizes. use prior arithmetic mean estimator, call mml() function specification number prior draws S set recompute = TRUE:8 Note prior arithmetic mean estimator works well prior posterior distribution similar shape strong overlap, Gronau et al. (2017) points . Otherwise, sampled prior values result likelihood value close zero, thereby contributing marginally approximation. case, large number S prior samples required.","code":"model_train <- mml(model_train) model_train$mml #> 3.54e-117 * exp(-1464) attr(model_train$mml, \"mmll\") #> [1] -1732.138 plot(model_train$mml, log = TRUE) model_train <- mml(model_train, S = 1000, recompute = TRUE)"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"bf","dir":"Articles","previous_headings":"The model_selection() function","what":"BF","title":"Model selection","text":"Bayes factor index relative posterior model plausibility one model another (Marin Robert 2014). Given data \\(\\texttt{y}\\) two models \\(\\texttt{mod0}\\) \\(\\texttt{mod1}\\), defined \\[ BF(\\texttt{mod0},\\texttt{mod1}) = \\frac{\\Pr(\\texttt{mod0} \\mid \\texttt{y})}{\\Pr(\\texttt{mod1} \\mid \\texttt{y})} = \\frac{\\Pr(\\texttt{y} \\mid \\texttt{mod0} )}{\\Pr(\\texttt{y} \\mid \\texttt{mod1})} / \\frac{\\Pr(\\texttt{mod0})}{\\Pr(\\texttt{mod1})}. \\] ratio \\(\\Pr(\\texttt{mod0}) / \\Pr(\\texttt{mod1})\\) expresses factor \\(\\texttt{mod0}\\) priori assumed correct model. Per default, {RprobitB} sets \\(\\Pr(\\texttt{mod0}) = \\Pr(\\texttt{mod1}) = 0.5\\). front part \\(\\Pr(\\texttt{y} \\mid \\texttt{mod0} ) / \\Pr(\\texttt{y} \\mid \\texttt{mod1})\\) ratio marginal model likelihoods. value \\(BF(\\texttt{mod0},\\texttt{mod1}) > 1\\) means model \\(\\texttt{mod0}\\) strongly supported data consideration \\(\\texttt{mod1}\\). Adding \"BF\" criteria argument model_selection yields Bayes factors. see decisive evidence (Jeffreys 1998) favor model_train.","code":"model_selection(model_train, model_train_sparse, criteria = c(\"BF\")) #>                          model_train model_train_sparse #> BF(*,model_train)                  1             < 0.01 #> BF(*,model_train_sparse)       > 100                  1"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"pred_acc","dir":"Articles","previous_headings":"The model_selection() function","what":"pred_acc","title":"Model selection","text":"Finally, adding \"pred_acc\" criteria argument model_selection() function returns share correctly predicted choices. output model_selection() (alternatively one following) deduce model_train correctly predicts 6% choices model_train_sparse:9","code":"pred_acc(model_train, model_train_sparse) #> [1] 0.6954592 0.6336634"},{"path":"https://loelschlaeger.de/RprobitB/articles/v06_model_selection.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Model selection","text":"Akaike, H. 1974. “New Look Statistical Model Identification” 19. Gronau, Q. F., . Sarafoglou, D. Matzke, . Ly, U. Boehm, M. Marsman, D. S. Leslie, J. J. Forster, E. Wagenmakers, H. Steingroever. 2017. “Tutorial Bridge Sampling.” Journal Mathematical Psychology 81: 80–97. Hammersley, J. M., D. C. Handscomb. 1964. “General Principles Monte Carlo Method,” 50–75. Jeffreys, Harold. 1998. Theory Probability. OUP Oxford. Marin, J., C. Robert. 2014. Bayesian essentials R. Springer Textbooks Statistics. Springer Verlag, New York. McElreath, R. 2020. Statistical Rethinking: Bayesian Course Examples R Stan. 2. ed. Chapman; Hall/CRC. Newton, M. ., . E. Raftery. 1994. “Approximate Bayesian Inference Weighted Likelihood Bootstrap.” Journal Royal Statistical Society: Series B (Methodological) 56 (1): 3–26. Schwarz, G. 1978. “Estimating Dimension Model.” Annals Statistics 6. Watanabe, S., M. Opper. 2010. “Asymptotic Equivalence Bayes Cross Validation Widely Applicable Information Criterion Singular Learning Theory.” Journal Machine Learning Research 11 (12).","code":""},{"path":"https://loelschlaeger.de/RprobitB/articles/v07_applications.html","id":"chess-opening-choice","dir":"Articles","previous_headings":"","what":"Chess opening choice","title":"Applications","text":"… moves already … possible positions. moves opening stage limited. package contains data set. example, model responds black player white’s move 1. e4. (options exist)","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lennart Oelschläger. Author, maintainer. Dietmar Bauer. Author. Sebastian Büscher. Contributor. Manuel Batram. Contributor.","code":""},{"path":"https://loelschlaeger.de/RprobitB/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Oelschläger L, Bauer D (2022). RprobitB: Bayes Estimation Mixed Multinomial Probit Models. R package version 1.0.0.9000, https://loelschlaeger.de/RprobitB/.","code":"@Manual{,   title = {RprobitB: Bayes Estimation of Mixed Multinomial Probit Models},   author = {Lennart Oelschläger and Dietmar Bauer},   year = {2022},   note = {R package version 1.0.0.9000},   url = {https://loelschlaeger.de/RprobitB/}, }"},{"path":"https://loelschlaeger.de/RprobitB/index.html","id":"rprobitb-bayesian-probit-choice-modeling-","dir":"","previous_headings":"","what":"Probit models for discrete choice data","title":"Probit models for discrete choice data","text":"goal {RprobitB} explain choices made deciders among discrete set alternatives. Bayesian way. example, think tourists want book train trip holiday destination: knowledge prefer certain alternative another great value train companies, especially customer’s willingness pay say faster comfortable trip.","code":""},{"path":"https://loelschlaeger.de/RprobitB/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Probit models for discrete choice data","text":"can install released version {RprobitB} CRAN : development version GitHub :","code":"install.packages(\"RprobitB\") # install.packages(\"devtools\") devtools::install_github(\"loelschlaeger/RprobitB\")"},{"path":"https://loelschlaeger.de/RprobitB/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Probit models for discrete choice data","text":"package documented several vignettes, see .","code":""},{"path":"https://loelschlaeger.de/RprobitB/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Probit models for discrete choice data","text":"analyze data set 2929 stated choices 235 Dutch individuals deciding two virtual train trip options based price, travel time, level comfort, number changes. data saved {mlogit} package. transform travel time minutes hours travel price guilders euros: following lines fit probit model explains chosen trip alternatives (choice) price, time, number changes, level comfort (lower value higher comfort). normalization, first linear coefficient, price, fixed -1, allows interpret coefficients monetary values: estimated effects can visualized via:  results indicate deciders value one hour travel time 25€, additional change 5€, comfortable class 15€. Now assume train company wants anticipate effect price increase market share. model, increasing ticket price 100€ 110€ (ceteris paribus) draws 15% customers competitor increase prices: However, offering better comfort class (0 better 1) compensates higher price even results gain 7% market share: just tip iceberg: {RprobitB} offers tools modeling choice behavior heterogeneity, preference-based classification deciders, model comparison .","code":"data(\"Train\", package = \"mlogit\") Train$price_A <- Train$price_A / 100 * 2.20371 Train$price_B <- Train$price_B / 100 * 2.20371 Train$time_A <- Train$time_A / 60 Train$time_B <- Train$time_B / 60 str(Train) #> 'data.frame':    2929 obs. of  11 variables: #>  $ id       : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ choiceid : int  1 2 3 4 5 6 7 8 9 10 ... #>  $ choice   : Factor w/ 2 levels \"A\",\"B\": 1 1 1 2 2 2 2 2 1 1 ... #>  $ price_A  : num  52.9 52.9 52.9 88.1 52.9 ... #>  $ time_A   : num  2.5 2.5 1.92 2.17 2.5 ... #>  $ change_A : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ comfort_A: num  1 1 1 1 1 0 1 1 0 1 ... #>  $ price_B  : num  88.1 70.5 88.1 70.5 70.5 ... #>  $ time_B   : num  2.5 2.17 1.92 2.5 2.5 ... #>  $ change_B : num  0 0 0 0 0 0 0 0 0 0 ... #>  $ comfort_B: num  1 1 0 0 0 0 1 0 1 0 ... form <- choice ~ price + time + change + comfort | 0 data <- prepare_data(form, Train) model <- mcmc(data, scale = list(\"parameter\" = \"a\", index = 1, value = -1)) plot(coef(model)) predict(   model,    data = data.frame(\"price_A\" = c(100,110),                      \"price_B\" = c(100,100)),   overview = FALSE) #>   id choiceid         A         B prediction #> 1  1        1 0.5000000 0.5000000          A #> 2  2        1 0.3483907 0.6516093          B predict(   model,    data = data.frame(\"price_A\"   = c(100,110),                      \"comfort_A\" = c(1,0),                     \"price_B\"   = c(100,100),                     \"comfort_B\" = c(1,1)),   overview = FALSE) #>   id choiceid         A         B prediction #> 1  1        1 0.5000000 0.5000000          A #> 2  2        1 0.5680923 0.4319077          A"},{"path":"https://loelschlaeger.de/RprobitB/reference/AIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Akaike's Information Criterion — AIC","title":"Akaike's Information Criterion — AIC","text":"function calculates Akaike's Information Criterion (AIC) RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/AIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Akaike's Information Criterion — AIC","text":"","code":"AIC(object, ..., k)  # S3 method for RprobitB_fit AIC(object, ..., k = 2)"},{"path":"https://loelschlaeger.de/RprobitB/reference/AIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Akaike's Information Criterion — AIC","text":"object object class RprobitB_fit. ... Optionally objects class RprobitB_fit. k numeric, penalty per parameter. default k = 2 classical AIC.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/AIC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Akaike's Information Criterion — AIC","text":"Either numeric value (just one object provided) numeric vector.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/AIC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Akaike's Information Criterion — AIC","text":"AIC computed $$-2 \\cdot \\text{LL} + k \\cdot \\text{npar},$$ \\(\\text{LL}\\) model's log-likelihood value estimated parameters, \\(k\\) penalty per parameter (\\(k = 2\\) classical AIC), \\(npar\\) number parameters fitted model. AIC quantifies trade-- -fitting, smaller values preferred.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/AIC.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Akaike's Information Criterion — AIC","text":"","code":"data(\"model_train\", package = \"RprobitB\") AIC(model_train) #> [1] 3463.485"},{"path":"https://loelschlaeger.de/RprobitB/reference/BIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Information Criterion — BIC","title":"Bayesian Information Criterion — BIC","text":"function calculates Bayesian Information Criterion (BIC) Schwarz Information Criterion RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/BIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Information Criterion — BIC","text":"","code":"BIC(object, ...)  # S3 method for RprobitB_fit BIC(object, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/BIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Information Criterion — BIC","text":"object object class RprobitB_fit. ... Optionally objects class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/BIC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Information Criterion — BIC","text":"Either numeric value (just one object provided) numeric vector.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/BIC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Information Criterion — BIC","text":"BIC computed $$-2 \\cdot \\text{LL} + \\text{npar} \\cdot \\ln{\\text{nobs}},$$ \\(\\text{LL}\\) model's log-likelihood value estimated parameters, \\(npar\\) number parameters fitted model, \\(\\text{nobs}\\) number data points. BIC quantifies trade-- -fitting, smaller values preferred.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/BIC.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Information Criterion — BIC","text":"","code":"data(\"model_train\", package = \"RprobitB\") BIC(model_train) #> [1] 3487.414"},{"path":"https://loelschlaeger.de/RprobitB/reference/R_hat.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Gelman-Rubin statistic — R_hat","title":"Compute Gelman-Rubin statistic — R_hat","text":"function computes Gelman-Rubin statistic R_hat.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/R_hat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Gelman-Rubin statistic — R_hat","text":"","code":"R_hat(samples, parts = 2)"},{"path":"https://loelschlaeger.de/RprobitB/reference/R_hat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Gelman-Rubin statistic — R_hat","text":"samples vector matrix samples Markov chain, e.g. Gibbs samples. samples matrix, column gives samples separate run. parts number parts divide chain sub-chains.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/R_hat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Gelman-Rubin statistic — R_hat","text":"numeric value, Gelman-Rubin statistic.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/R_hat.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Gelman-Rubin statistic — R_hat","text":"https://bookdown.org/rdpeng/advstatcomp/monitoring-convergence.html","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/R_hat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Gelman-Rubin statistic — R_hat","text":"","code":"no_chains <- 2 length_chains <- 1e3 samples <- matrix(NA, length_chains, no_chains) samples[1, ] <- 1 Gamma <- matrix(c(0.8, 0.1, 0.2, 0.9), 2, 2) for (c in 1:no_chains) {   for (t in 2:length_chains) {     samples[t, c] <- sample(1:2, 1, prob = Gamma[samples[t - 1, c], ])   } } R_hat(samples) #> [1] 1.025849"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB.html","id":null,"dir":"Reference","previous_headings":"","what":"RprobitB: A package for Bayes estimation of probit models — RprobitB","title":"RprobitB: A package for Bayes estimation of probit models — RprobitB","text":"package provides tools Bayes estimation probit models.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Create object of class RprobitB_data — RprobitB_data","title":"Create object of class RprobitB_data — RprobitB_data","text":"function constructs object class RprobitB_data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create object of class RprobitB_data — RprobitB_data","text":"","code":"RprobitB_data(   data,   choice_data,   N,   T,   J,   P_f,   P_r,   alternatives,   form,   re,   ASC,   linear_coefs,   standardize,   simulated,   choice_available,   true_parameter,   res_var_names )"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create object of class RprobitB_data — RprobitB_data","text":"data list choice data. list N elements. element list two elements, X y, covariates decisions decision maker. precisely, X list T elements, element matrix dimension Jx(P_f+P_r) contains characteristics one choice occasion. y vector length T contains labels chosen alternatives. choice_data data frame choice data wide format, .e. row represents one choice occasion. N number (greater equal 1) decision makers. T number (greater equal 1) choice occasions vector choice occasions length N (.e. decision maker specific number). J number (greater equal 2) choice alternatives. P_f number covariates connected fixed coefficient (can 0). P_r number covariates connected random coefficient (can 0). alternatives character vector names choice alternatives. specified, choice set defined observed choices. form formula object used specify probit model. structure choice ~ | B | C, names alternative choice situation specific covariates generic coefficient, B names choice situation specific covariates alternative specific coefficients, C names alternative choice situation specific covariates alternative specific coefficients. Separate multiple covariates one type + sign. default, alternative specific constants (ASCs) added model (except last alternative due identifiability). can removed adding +0 second spot. See vignette choice data details. re character (vector) covariates form random effects. re = NULL (default), random effects. random effects alternative specific constants, include \"ASC\" re. ASC boolean, determining whether model ASCs. linear_coefs data frame coefficient names booleans indicating whether connected random effects. standardize character vector names covariates get standardized. Covariates type 1 3 addressed <covariate>_<alternative>. standardize = \"\", covariates get standardized. simulated boolean, TRUE data simulated, otherwise data empirical. choice_available boolean, TRUE data contains observed choices. true_parameter object class RprobitB_parameters. res_var_names names list reserved variable names choice_data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create object of class RprobitB_data — RprobitB_data","text":"object class RprobitB_data arguments function elements.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Create object of class RprobitB_fit. — RprobitB_fit","title":"Create object of class RprobitB_fit. — RprobitB_fit","text":"function creates object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create object of class RprobitB_fit. — RprobitB_fit","text":"","code":"RprobitB_fit(   data,   normalization,   R,   B,   Q,   latent_classes,   prior,   gibbs_samples,   class_sequence )"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create object of class RprobitB_fit. — RprobitB_fit","text":"data object class RprobitB_data. normalization object class RprobitB_normalization. R number iterations Gibbs sampler. B length burn-period, .e. non-negative number samples discarded. Q thinning factor Gibbs samples, .e. every Qth sample kept. latent_classes Either NULL (latent classes) list parameters specifying number latent classes updating scheme: C: fixed number (greater equal 1) latent classes, set 1 per default. either weight_update = TRUE dp_update = TRUE (.e. classes updated), C equals initial number latent classes. weight_update: boolean, set TRUE weight-based update latent classes. See ... details. dp_update: boolean, set TRUE update latent classes based Dirichlet process. See ... details. Cmax: maximum number latent classes. buffer: number iterations wait next weight-based update latent classes. epsmin: threshold weight (0 1) removing latent class weight-based updating scheme. epsmax: threshold weight (0 1) splitting latent class weight-based updating scheme. distmin: (non-negative) threshold difference class means joining two latent classes weight-based updating scheme. prior named list parameters prior distributions. See documentation check_prior details parameters can specified. gibbs_samples object class RprobitB_gibbs_samples. class_sequence sequence class numbers Gibbs sampling length R.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create object of class RprobitB_fit. — RprobitB_fit","text":"object class RprobitB_fit, .e. list arguments function elements.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_gibbs_samples_statistics.html","id":null,"dir":"Reference","previous_headings":"","what":"Create object of class RprobitB_gibbs_samples_statistics — RprobitB_gibbs_samples_statistics","title":"Create object of class RprobitB_gibbs_samples_statistics — RprobitB_gibbs_samples_statistics","text":"function creates object class RprobitB_gibbs_samples_statistics.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_gibbs_samples_statistics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create object of class RprobitB_gibbs_samples_statistics — RprobitB_gibbs_samples_statistics","text":"","code":"RprobitB_gibbs_samples_statistics(gibbs_samples, FUN)"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_gibbs_samples_statistics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create object of class RprobitB_gibbs_samples_statistics — RprobitB_gibbs_samples_statistics","text":"gibbs_samples object class RprobitB_gibbs_samples. FUN (preferably named) list functions compute parameter statistics Gibbs samples, .e. mean mean, sd standard deviation, min minimum, max maximum, median median, function(x) quantile(x, p) pth quantile, R_hat Gelman-Rubin statistic.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_gibbs_samples_statistics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create object of class RprobitB_gibbs_samples_statistics — RprobitB_gibbs_samples_statistics","text":"object class RprobitB_gibbs_samples_statistics, list statistics gibbs_samples obtained applying elements FUN.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":null,"dir":"Reference","previous_headings":"","what":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"function creates object class RprobitB_latent_classes defines number latent classes updating scheme. RprobitB_latent_classes-object generated function relevance model possesses least one random coefficient, .e. P_r>0.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"","code":"RprobitB_latent_classes(latent_classes = NULL)"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"latent_classes Either NULL (latent classes) list parameters specifying number latent classes updating scheme: C: fixed number (greater equal 1) latent classes, set 1 per default. either weight_update = TRUE dp_update = TRUE (.e. classes updated), C equals initial number latent classes. weight_update: boolean, set TRUE weight-based update latent classes. See ... details. dp_update: boolean, set TRUE update latent classes based Dirichlet process. See ... details. Cmax: maximum number latent classes. buffer: number iterations wait next weight-based update latent classes. epsmin: threshold weight (0 1) removing latent class weight-based updating scheme. epsmax: threshold weight (0 1) splitting latent class weight-based updating scheme. distmin: (non-negative) threshold difference class means joining two latent classes weight-based updating scheme.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"object class RprobitB_latent_classes.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"why-update-latent-classes-","dir":"Reference","previous_headings":"","what":"Why update latent classes?","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"order specify number latent classes estimation.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"what-options-to-update-latent-classes-exist-","dir":"Reference","previous_headings":"","what":"What options to update latent classes exist?","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"Currently two updating schemes implemented, weight-based via Dirichlet process, see vignette modeling heterogeneity.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"what-is-the-default-behavior-","dir":"Reference","previous_headings":"","what":"What is the default behavior?","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"One latent class without updates specified per default. Print RprobitB_latent_classes-object see summary relevant (default) parameter settings.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"why-is-cmax-required-","dir":"Reference","previous_headings":"","what":"Why is Cmax required?","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"implementation requires upper bound number latent classes saving Gibbs samples. However, restriction since number latent classes bounded number deciders case. plot method visualizing sequence class numbers estimation can used check Cmax reached, see plot.RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_latent_classes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create object of class RprobitB_latent_classes — RprobitB_latent_classes","text":"","code":"### default setting RprobitB:::RprobitB_latent_classes() #> Latent classes #> C = 1   ### setting for a fixed number of two latent classes RprobitB:::RprobitB_latent_classes(list(C = 2)) #> Latent classes #> C = 2   ### setting for weight-based on Dirichlet process-based updates RprobitB:::RprobitB_latent_classes(   list(\"weight_update\" = TRUE, \"dp_update\" = TRUE) ) #> Latent classes #> DP-based update: TRUE  #> Weight-based update: TRUE  #> Initial classes: 1  #> Maximum classes: 10  #> Updating buffer: 100  #> Minimum class weight: 0.01  #> Maximum class weight: 0.99  #> Mimumum class distance: 0.1"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_normalization.html","id":null,"dir":"Reference","previous_headings":"","what":"Create object of class RprobitB_normalization — RprobitB_normalization","title":"Create object of class RprobitB_normalization — RprobitB_normalization","text":"function creates object class RprobitB_normalization.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_normalization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create object of class RprobitB_normalization — RprobitB_normalization","text":"","code":"RprobitB_normalization(   J,   P_f,   level = J,   scale = list(parameter = \"s\", index = 1, value = 1) )"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_normalization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create object of class RprobitB_normalization — RprobitB_normalization","text":"J number (greater equal 2) choice alternatives. P_f number covariates connected fixed coefficient (can 0). level number alternative respect utility differences computed. Currently, level = J (.e. utility differences respect last alternative) implemented. scale named list three elements, determining parameter normalization respect utility scale: parameter: Either \"\" (linear coefficient \"alpha\") \"s\" (variance error-term covariance matrix \"Sigma\"). index: index parameter gets fixed. value: value fixed parameter.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_normalization.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create object of class RprobitB_normalization — RprobitB_normalization","text":"object class RprobitB_normalization, list elements level scale.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_normalization.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create object of class RprobitB_normalization — RprobitB_normalization","text":"choice model normalized respect level scale. level normalization, RprobitB takes utility differences respect one alternative. scale normalization, RprobitB fixes model parameter. Per default, first error-term variance fixed 1, .e. scale = list(\"parameter\" = \"s\", \"index\" = 1, \"value\" = 1). Alternatively, error-term variance non-random coefficient can fixed.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_normalization.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create object of class RprobitB_normalization — RprobitB_normalization","text":"","code":"RprobitB:::RprobitB_normalization(   J = 2, P_f = 1, level = 2,   scale = list(\"parameter\" = \"s\", \"index\" = 1, \"value\" = 1) ) #> Normalization #> Level: Utility differences with respect to alternative 2. #> Scale: Coefficient of the 1. error term variance in Sigma fixed to 1."},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_parameter.html","id":null,"dir":"Reference","previous_headings":"","what":"Create object of class RprobitB_parameter — RprobitB_parameter","title":"Create object of class RprobitB_parameter — RprobitB_parameter","text":"function creates object class RprobitB_parameter. sample = TRUE, missing parameters sampled. parameters checked values P_f, P_r, J, N.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_parameter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create object of class RprobitB_parameter — RprobitB_parameter","text":"","code":"RprobitB_parameter(   P_f,   P_r,   J,   N,   alpha = NULL,   C = NULL,   s = NULL,   b = NULL,   Omega = NULL,   Sigma = NULL,   Sigma_full = NULL,   beta = NULL,   z = NULL,   seed = NULL,   sample = TRUE )"},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_parameter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create object of class RprobitB_parameter — RprobitB_parameter","text":"P_f number covariates connected fixed coefficient (can 0). P_r number covariates connected random coefficient (can 0). J number (greater equal 2) choice alternatives. N number (greater equal 1) decision makers. alpha fixed coefficient vector length P_f. Set NA P_f = 0. C number (greater equal 1) latent classes decision makers. Set NA P_r = 0. Otherwise, C = 1 per default. s vector class weights length C. Set NA P_r = 0. identifiability, vector must non-ascending. b matrix class means columns dimension P_r x C. Set NA P_r = 0. Omega matrix class covariance matrices columns dimension P_r*P_r x C. Set NA P_r = 0. Sigma differenced error term covariance matrix dimension J-1 x J-1 respect alternative J. Sigma_full error term covariance matrix dimension J x J. Internally, Sigma_full gets differenced respect alternative J, becomes identified covariance matrix dimension J-1 x J-1. Sigma specified, Sigma_full ignored. beta matrix decision-maker specific coefficient vectors dimension P_r x N. Set NA P_r = 0. z vector allocation variables length N. Set NA P_r = 0. seed Set seed sampling missing parameters. sample boolean, TRUE missing parameters get sampled.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_parameter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create object of class RprobitB_parameter — RprobitB_parameter","text":"object class RprobitB_parameter, .e. named list model parameters alpha, C, s, b, Omega, Sigma, Sigma_full, beta, z.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/RprobitB_parameter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create object of class RprobitB_parameter — RprobitB_parameter","text":"","code":"RprobitB_parameter(P_f = 1, P_r = 2, J = 3, N = 10) #> RprobitB model parameter #>  #> alpha : 2.4 #>  #> C : 1 #>  #> s : 1 #>  #> b : 2 x 1 matrix of doubles  #>  #>      [,1] #> [1,] -1.4 #> [2,]  1.2 #>  #>  #> Omega : 4 x 1 matrix of doubles  #>  #>          [,1] #> [1,] 2.734520 #> [2,] 1.949271 #> [3,] 1.949271 #> [4,] 1.722950 #>  #>  #> Sigma : 2 x 2 matrix of doubles  #>  #>           [,1]     [,2] #> [1,] 13.643705 5.379367 #> [2,]  5.379367 7.679335 #>  #>  #> Sigma_full : 3 x 3 matrix of doubles  #>  #>           [,1]       [,2]       [,3] #> [1,]  4.948325 -2.0015456 -2.0680518 #> [2,] -2.001546  1.6128888 -0.7535849 #> [3,] -2.068052 -0.7535849  4.5592764 #>  #>  #> beta : 2 x 10 matrix of doubles  #>  #>         [,1]   [,2]    [,3] ...   [,10] #> [1,] -1.8313 0.2388 -1.8140 ... -1.1909 #> [2,]  0.8301 2.9557  0.9114 ...  2.4032 #>  #>  #> z : integer vector of length 10  #>  #> 1 1 1 ... 1 #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/WAIC.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute WAIC value — WAIC","title":"Compute WAIC value — WAIC","text":"function computes WAIC value RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/WAIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute WAIC value — WAIC","text":"","code":"WAIC(x)"},{"path":"https://loelschlaeger.de/RprobitB/reference/WAIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute WAIC value — WAIC","text":"x object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/WAIC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute WAIC value — WAIC","text":"numeric, WAIC value, following attributes: se_waic, standard error WAIC value, lppd, log pointwise predictive density, p_waic, effective number parameters, p_waic_vec, vector summands p_waic, p_si, output compute_p_si.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/WAIC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute WAIC value — WAIC","text":"WAIC short Widely Applicable (Watanabe-Akaike) Information Criterion. AIC BIC, smaller WAIC value better model. definition $$\\text{WAIC} = -2 \\cdot \\text{lppd} + 2 \\cdot  p_\\text{WAIC},$$ \\(\\text{lppd}\\) stands log pointwise predictive density \\(p_\\text{WAIC}\\) penalty term proportional variance posterior distribution sometimes called effective number parameters. \\(\\text{lppd}\\) approximated follows. Let $$p_{} = \\Pr(y_i\\mid \\theta_s)$$ probability observation \\(y_i\\) given \\(s\\)th set \\(\\theta_s\\) parameter samples posterior. $$\\text{lppd} = \\sum_i \\log S^{-1} \\sum_s p_{si}.$$ penalty term computed sum variances log-probability observation: $$p_\\text{WAIC} = \\sum_i \\mathbb{V}_{\\theta} \\left[ \\log p_{si} \\right].$$ \\(\\text{WAIC}\\) standard error \\(\\text{SE}\\) $$\\text{SE} = \\sqrt{n \\cdot \\mathbb{V}_i \\left[-2 \\left(\\text{lppd} - \\mathbb{V}_{\\theta} \\left[ \\log p_{si} \\right] \\right)\\right]},$$ \\(n\\) number choices.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/WAIC.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute WAIC value — WAIC","text":"","code":"data(\"model_train\", package = \"RprobitB\") x <- WAIC(model_train) print(x) #> 3463.76 (0.18)  if (FALSE) { ### plot convergence plot(x) }"},{"path":"https://loelschlaeger.de/RprobitB/reference/as_cov_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Relabel the alternative specific covariates to the required format — as_cov_names","title":"Relabel the alternative specific covariates to the required format — as_cov_names","text":"RprobitB, alternative specific covariates must named format \"<covariate>_<alternative>\". convenience function generates format given choice_data set.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/as_cov_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Relabel the alternative specific covariates to the required format — as_cov_names","text":"","code":"as_cov_names(choice_data, cov, alternatives)"},{"path":"https://loelschlaeger.de/RprobitB/reference/as_cov_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Relabel the alternative specific covariates to the required format — as_cov_names","text":"choice_data data frame choice data wide format, .e. row represents one choice occasion. cov character vector names alternative specific covariates choice_data. alternatives (character numeric) vector alternative names.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/as_cov_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Relabel the alternative specific covariates to the required format — as_cov_names","text":"choice_data input updated column names.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/as_cov_names.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Relabel the alternative specific covariates to the required format — as_cov_names","text":"","code":"data(\"Electricity\", package = \"mlogit\") cov <- c(\"pf\",\"cl\",\"loc\",\"wk\",\"tod\",\"seas\") alternatives <- 1:4 colnames(Electricity) #>  [1] \"choice\" \"id\"     \"pf1\"    \"pf2\"    \"pf3\"    \"pf4\"    \"cl1\"    \"cl2\"    #>  [9] \"cl3\"    \"cl4\"    \"loc1\"   \"loc2\"   \"loc3\"   \"loc4\"   \"wk1\"    \"wk2\"    #> [17] \"wk3\"    \"wk4\"    \"tod1\"   \"tod2\"   \"tod3\"   \"tod4\"   \"seas1\"  \"seas2\"  #> [25] \"seas3\"  \"seas4\"  Electricity <- as_cov_names(Electricity, cov, alternatives) colnames(Electricity) #>  [1] \"choice\" \"id\"     \"pf_1\"   \"pf_2\"   \"pf_3\"   \"pf_4\"   \"cl_1\"   \"cl_2\"   #>  [9] \"cl_3\"   \"cl_4\"   \"loc_1\"  \"loc_2\"  \"loc_3\"  \"loc_4\"  \"wk_1\"   \"wk_2\"   #> [17] \"wk_3\"   \"wk_4\"   \"tod_1\"  \"tod_2\"  \"tod_3\"  \"tod_4\"  \"seas_1\" \"seas_2\" #> [25] \"seas_3\" \"seas_4\""},{"path":"https://loelschlaeger.de/RprobitB/reference/check_form.html","id":null,"dir":"Reference","previous_headings":"","what":"Check the model formula — check_form","title":"Check the model formula — check_form","text":"function checks input form.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/check_form.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check the model formula — check_form","text":"","code":"check_form(form, re = NULL)"},{"path":"https://loelschlaeger.de/RprobitB/reference/check_form.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check the model formula — check_form","text":"form formula object used specify probit model. structure choice ~ | B | C, names alternative choice situation specific covariates generic coefficient, B names choice situation specific covariates alternative specific coefficients, C names alternative choice situation specific covariates alternative specific coefficients. Separate multiple covariates one type + sign. default, alternative specific constants (ASCs) added model (except last alternative due identifiability). can removed adding +0 second spot. See vignette choice data details. re character (vector) covariates form random effects. re = NULL (default), random effects. random effects alternative specific constants, include \"ASC\" re.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/check_form.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check the model formula — check_form","text":"object class RprobitB_formula, list contains following elements: form: input form. choice: dependent variable form. re: input re, covariates part form removed. vars: list three character vectors covariate names three covariate types. ASC: boolean, determining whether model ASCs.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/reference/check_form.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check the model formula — check_form","text":"","code":"form <- choice ~ price + time + comfort + change re <- c(\"price\", \"time\") check_form(form = form, re = re) #> choice ~ price + time + comfort + change #> <environment: 0x7fadb1d68220> #> - dependent variable: choice  #> - type 1 covariate(s): price, time, comfort, change  #> - type 2 covariate(s):   #> - type 3 covariate(s):   #> - random effects: price, time  #> - ASC: TRUE"},{"path":"https://loelschlaeger.de/RprobitB/reference/check_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Check prior parameters — check_prior","title":"Check prior parameters — check_prior","text":"function checks compatibility submitted parameters prior distributions sets missing values default values.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/check_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check prior parameters — check_prior","text":"","code":"check_prior(   P_f,   P_r,   J,   eta = numeric(P_f),   Psi = diag(P_f),   delta = 1,   xi = numeric(P_r),   D = diag(P_r),   nu = P_r + 2,   Theta = diag(P_r),   kappa = J + 1,   E = diag(J - 1) )"},{"path":"https://loelschlaeger.de/RprobitB/reference/check_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check prior parameters — check_prior","text":"P_f number covariates connected fixed coefficient (can 0). P_r number covariates connected random coefficient (can 0). J number (greater equal 2) choice alternatives. eta mean vector length P_f normal prior alpha. Psi covariance matrix dimension P_f x P_f normal prior alpha. delta numeric concentration parameter vector rep(delta,C) Dirichlet prior s. xi mean vector length P_r normal prior b_c. D covariance matrix dimension P_r x P_r normal prior b_c. nu degrees freedom (natural number greater P_r) Inverse Wishart prior Omega_c. Theta scale matrix dimension P_r x P_r Inverse Wishart prior Omega_c. kappa degrees freedom (natural number greater J-1) Inverse Wishart prior Sigma. E scale matrix dimension J-1 x J-1 Inverse Wishart prior Sigma.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/check_prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check prior parameters — check_prior","text":"object class RprobitB_prior, list containing prior parameters. Parameters relevant model configuration set NA.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/check_prior.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check prior parameters — check_prior","text":"priori, assume model parameters follow distributions: \\(\\alpha \\sim N(\\eta, \\Psi)\\) \\(s \\sim Dir(\\delta)\\) \\(b_c \\sim N(\\xi, D)\\) classes \\(c\\) \\(\\Omega_c \\sim IW(\\nu,\\Theta)\\) classes \\(c\\) \\(\\Sigma \\sim IW(\\kappa,E)\\) \\(N\\) denotes normal, \\(Dir\\) Dirichlet, \\(IW\\) Inverted Wishart distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/check_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check prior parameters — check_prior","text":"","code":"check_prior(P_f = 1, P_r = 2, J = 3) #> $eta #> [1] 0 #>  #> $Psi #>      [,1] #> [1,]    1 #>  #> $delta #> [1] 1 #>  #> $xi #> [1] 0 0 #>  #> $D #>      [,1] [,2] #> [1,]    1    0 #> [2,]    0    1 #>  #> $nu #> [1] 4 #>  #> $Theta #>      [,1] [,2] #> [1,]    1    0 #> [2,]    0    1 #>  #> $kappa #> [1] 4 #>  #> $E #>      [,1] [,2] #> [1,]    1    0 #> [2,]    0    1 #>  #> attr(,\"class\") #> [1] \"RprobitB_prior\""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_berserk.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of berserking — choice_berserk","title":"Choice of berserking — choice_berserk","text":"dataset includes binary 'berserking' choice participants yearly bullet arena 2022 online chess platform https://lichess.org. Berserking choice player beginning game: player clicks 'Berserk button', lose half clock time, win worth one extra tournament point.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_berserk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of berserking — choice_berserk","text":"","code":"data(choice_berserk)"},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_berserk.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Choice of berserking — choice_berserk","text":"data frame containing berserking choices 6174 chess players 126902 online bullet (1+0) games. consists following columns: player_id, unique lichess username chess player game_id, unique lichess identification game berserk, 1 player berserked 0 , white, 1 player white pieces 0 rating, player's lichess bullet rating start game rating_diff, rating difference opponent lost, 1 player lost game (hence lost streak) 0 min_rem, number minutes left tournament streak, 1 player streak (see details) 0 ","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_berserk.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Choice of berserking — choice_berserk","text":"data obtained via lichess API https://lichess.org/api tournament id 'RibHfoX6' 2022-03-29.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_berserk.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice of berserking — choice_berserk","text":"'berserk' feature online chess platform https://lichess.org. game starts, player can click button, lose half clock time, win worth one extra tournament point. considered tournament following characteristics: tournament startet 2022-01-10 17:00:25 lasted 240 minutes. time control 1 minute per player per game (bullet format). players automatically immediately paired game finished, -called 'arena tournament' modus. players can pause participation time. win base score 2 points, draw 1 point, loss worth points. player wins two games consecutively, start double point streak, means following games continue worth double points fail win game.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_berserk.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Choice of berserking — choice_berserk","text":"See https://lichess.org/tournament/help?system=arena information tournament format.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_chess_opening.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice of a chess opening — choice_chess_opening","title":"Choice of a chess opening — choice_chess_opening","text":"dataset includes opening choices 37229 chess games.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_chess_opening.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice of a chess opening — choice_chess_opening","text":"","code":"data(choice_chess_opening)"},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_chess_opening.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Choice of a chess opening — choice_chess_opening","text":"data frame following columns: fideid_w, FIDE identifier White player fideid_b, FIDE identifier Black player w1, first move White player b1, first move Black player sex_w, gender White player (0 male 1 female) sex_b, gender Black player (0 male 1 female) byear_w, birth year White player byear_b, birth year Black player rating_w, FIDE rating White player rating_b, FIDE rating Black player date, date chess game result, result chess game game, full notation chess game","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_chess_opening.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Choice of a chess opening — choice_chess_opening","text":"opening moves scraped 'Week Chess Archive' https://theweekinchess.com/twic issues 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428. player's metadata (fideid, sex, byear, rating) added matching pairings names official FIDE rating list http://ratings.fide.com 2022-03-28. Ambiguous matches dropped. Chess games blitz rapid time control ignored.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_probabilities.html","id":null,"dir":"Reference","previous_headings":"","what":"Return choice probabilities of an RprobitB_fit. — choice_probabilities","title":"Return choice probabilities of an RprobitB_fit. — choice_probabilities","text":"function returns choice probabilities RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_probabilities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return choice probabilities of an RprobitB_fit. — choice_probabilities","text":"","code":"choice_probabilities(x, data = NULL, par_set = mean)"},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_probabilities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return choice probabilities of an RprobitB_fit. — choice_probabilities","text":"x object class RprobitB_fit. data Either NULL object class RprobitB_data. former case, choice probabilities computed data used model fitting. Alternatively, new data set can provided. par_set Specifying parameter set calculation either function computes posterior point estimate (default mean()), \"true\" select true parameter set, object class RprobitB_parameter.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_probabilities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return choice probabilities of an RprobitB_fit. — choice_probabilities","text":"data frame choice probabilities choice situations rows alternatives columns. first two columns decider identifier \"id\" choice situation identifier \"idc\".","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/choice_probabilities.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return choice probabilities of an RprobitB_fit. — choice_probabilities","text":"","code":"data <- simulate_choices(form = choice ~ covariate, N = 10, T = 10, J = 2) x <- mcmc(data) #> Computing sufficient statistics 0 of 4  #> Computing sufficient statistics 1 of 4  #> Computing sufficient statistics 2 of 4  #> Computing sufficient statistics 3 of 4  #> Computing sufficient statistics 4 of 4  #> Gibbs sampler iteration 1 of 10000  #> Gibbs sampler iteration 10 of 10000  #> Gibbs sampler iteration 20 of 10000  #> Gibbs sampler iteration 30 of 10000  #> Gibbs sampler iteration 40 of 10000  #> Gibbs sampler iteration 50 of 10000  #> Gibbs sampler iteration 60 of 10000  #> Gibbs sampler iteration 70 of 10000  #> Gibbs sampler iteration 80 of 10000  #> Gibbs sampler iteration 90 of 10000  #> Gibbs sampler iteration 100 of 10000  #> Gibbs sampler iteration 110 of 10000  #> Gibbs sampler iteration 120 of 10000  #> Gibbs sampler iteration 130 of 10000  #> Gibbs sampler iteration 140 of 10000  #> Gibbs sampler iteration 150 of 10000  #> Gibbs sampler iteration 160 of 10000  #> Gibbs sampler iteration 170 of 10000  #> Gibbs sampler iteration 180 of 10000  #> Gibbs sampler iteration 190 of 10000  #> Gibbs sampler iteration 200 of 10000  #> Gibbs sampler iteration 210 of 10000  #> Gibbs sampler iteration 220 of 10000  #> Gibbs sampler iteration 230 of 10000  #> Gibbs sampler iteration 240 of 10000  #> Gibbs sampler iteration 250 of 10000  #> Gibbs sampler iteration 260 of 10000  #> Gibbs sampler iteration 270 of 10000  #> Gibbs sampler iteration 280 of 10000  #> Gibbs sampler iteration 290 of 10000  #> Gibbs sampler iteration 300 of 10000  #> Gibbs sampler iteration 310 of 10000  #> Gibbs sampler iteration 320 of 10000  #> Gibbs sampler iteration 330 of 10000  #> Gibbs sampler iteration 340 of 10000  #> Gibbs sampler iteration 350 of 10000  #> Gibbs sampler iteration 360 of 10000  #> Gibbs sampler iteration 370 of 10000  #> Gibbs sampler iteration 380 of 10000  #> Gibbs sampler iteration 390 of 10000  #> Gibbs sampler iteration 400 of 10000  #> Gibbs sampler iteration 410 of 10000  #> Gibbs sampler iteration 420 of 10000  #> Gibbs sampler iteration 430 of 10000  #> Gibbs sampler iteration 440 of 10000  #> Gibbs sampler iteration 450 of 10000  #> Gibbs sampler iteration 460 of 10000  #> Gibbs sampler iteration 470 of 10000  #> Gibbs sampler iteration 480 of 10000  #> Gibbs sampler iteration 490 of 10000  #> Gibbs sampler iteration 500 of 10000  #> Gibbs sampler iteration 510 of 10000  #> Gibbs sampler iteration 520 of 10000  #> Gibbs sampler iteration 530 of 10000  #> Gibbs sampler iteration 540 of 10000  #> Gibbs sampler iteration 550 of 10000  #> Gibbs sampler iteration 560 of 10000  #> Gibbs sampler iteration 570 of 10000  #> Gibbs sampler iteration 580 of 10000  #> Gibbs sampler iteration 590 of 10000  #> Gibbs sampler iteration 600 of 10000  #> Gibbs sampler iteration 610 of 10000  #> Gibbs sampler iteration 620 of 10000  #> Gibbs sampler iteration 630 of 10000  #> Gibbs sampler iteration 640 of 10000  #> Gibbs sampler iteration 650 of 10000  #> Gibbs sampler iteration 660 of 10000  #> Gibbs sampler iteration 670 of 10000  #> Gibbs sampler iteration 680 of 10000  #> Gibbs sampler iteration 690 of 10000  #> Gibbs sampler iteration 700 of 10000  #> Gibbs sampler iteration 710 of 10000  #> Gibbs sampler iteration 720 of 10000  #> Gibbs sampler iteration 730 of 10000  #> Gibbs sampler iteration 740 of 10000  #> Gibbs sampler iteration 750 of 10000  #> Gibbs sampler iteration 760 of 10000  #> Gibbs sampler iteration 770 of 10000  #> Gibbs sampler iteration 780 of 10000  #> Gibbs sampler iteration 790 of 10000  #> Gibbs sampler iteration 800 of 10000  #> Gibbs sampler iteration 810 of 10000  #> Gibbs sampler iteration 820 of 10000  #> Gibbs sampler iteration 830 of 10000  #> Gibbs sampler iteration 840 of 10000  #> Gibbs sampler iteration 850 of 10000  #> Gibbs sampler iteration 860 of 10000  #> Gibbs sampler iteration 870 of 10000  #> Gibbs sampler iteration 880 of 10000  #> Gibbs sampler iteration 890 of 10000  #> Gibbs sampler iteration 900 of 10000  #> Gibbs sampler iteration 910 of 10000  #> Gibbs sampler iteration 920 of 10000  #> Gibbs sampler iteration 930 of 10000  #> Gibbs sampler iteration 940 of 10000  #> Gibbs sampler iteration 950 of 10000  #> Gibbs sampler iteration 960 of 10000  #> Gibbs sampler iteration 970 of 10000  #> Gibbs sampler iteration 980 of 10000  #> Gibbs sampler iteration 990 of 10000  #> Gibbs sampler iteration 1000 of 10000  #> Gibbs sampler iteration 1010 of 10000  #> Gibbs sampler iteration 1020 of 10000  #> Gibbs sampler iteration 1030 of 10000  #> Gibbs sampler iteration 1040 of 10000  #> Gibbs sampler iteration 1050 of 10000  #> Gibbs sampler iteration 1060 of 10000  #> Gibbs sampler iteration 1070 of 10000  #> Gibbs sampler iteration 1080 of 10000  #> Gibbs sampler iteration 1090 of 10000  #> Gibbs sampler iteration 1100 of 10000  #> Gibbs sampler iteration 1110 of 10000  #> Gibbs sampler iteration 1120 of 10000  #> Gibbs sampler iteration 1130 of 10000  #> Gibbs sampler iteration 1140 of 10000  #> Gibbs sampler iteration 1150 of 10000  #> Gibbs sampler iteration 1160 of 10000  #> Gibbs sampler iteration 1170 of 10000  #> Gibbs sampler iteration 1180 of 10000  #> Gibbs sampler iteration 1190 of 10000  #> Gibbs sampler iteration 1200 of 10000  #> Gibbs sampler iteration 1210 of 10000  #> Gibbs sampler iteration 1220 of 10000  #> Gibbs sampler iteration 1230 of 10000  #> Gibbs sampler iteration 1240 of 10000  #> Gibbs sampler iteration 1250 of 10000  #> Gibbs sampler iteration 1260 of 10000  #> Gibbs sampler iteration 1270 of 10000  #> Gibbs sampler iteration 1280 of 10000  #> Gibbs sampler iteration 1290 of 10000  #> Gibbs sampler iteration 1300 of 10000  #> Gibbs sampler iteration 1310 of 10000  #> Gibbs sampler iteration 1320 of 10000  #> Gibbs sampler iteration 1330 of 10000  #> Gibbs sampler iteration 1340 of 10000  #> Gibbs sampler iteration 1350 of 10000  #> Gibbs sampler iteration 1360 of 10000  #> Gibbs sampler iteration 1370 of 10000  #> Gibbs sampler iteration 1380 of 10000  #> Gibbs sampler iteration 1390 of 10000  #> Gibbs sampler iteration 1400 of 10000  #> Gibbs sampler iteration 1410 of 10000  #> Gibbs sampler iteration 1420 of 10000  #> Gibbs sampler iteration 1430 of 10000  #> Gibbs sampler iteration 1440 of 10000  #> Gibbs sampler iteration 1450 of 10000  #> Gibbs sampler iteration 1460 of 10000  #> Gibbs sampler iteration 1470 of 10000  #> Gibbs sampler iteration 1480 of 10000  #> Gibbs sampler iteration 1490 of 10000  #> Gibbs sampler iteration 1500 of 10000  #> Gibbs sampler iteration 1510 of 10000  #> Gibbs sampler iteration 1520 of 10000  #> Gibbs sampler iteration 1530 of 10000  #> Gibbs sampler iteration 1540 of 10000  #> Gibbs sampler iteration 1550 of 10000  #> Gibbs sampler iteration 1560 of 10000  #> Gibbs sampler iteration 1570 of 10000  #> Gibbs sampler iteration 1580 of 10000  #> Gibbs sampler iteration 1590 of 10000  #> Gibbs sampler iteration 1600 of 10000  #> Gibbs sampler iteration 1610 of 10000  #> Gibbs sampler iteration 1620 of 10000  #> Gibbs sampler iteration 1630 of 10000  #> Gibbs sampler iteration 1640 of 10000  #> Gibbs sampler iteration 1650 of 10000  #> Gibbs sampler iteration 1660 of 10000  #> Gibbs sampler iteration 1670 of 10000  #> Gibbs sampler iteration 1680 of 10000  #> Gibbs sampler iteration 1690 of 10000  #> Gibbs sampler iteration 1700 of 10000  #> Gibbs sampler iteration 1710 of 10000  #> Gibbs sampler iteration 1720 of 10000  #> Gibbs sampler iteration 1730 of 10000  #> Gibbs sampler iteration 1740 of 10000  #> Gibbs sampler iteration 1750 of 10000  #> Gibbs sampler iteration 1760 of 10000  #> Gibbs sampler iteration 1770 of 10000  #> Gibbs sampler iteration 1780 of 10000  #> Gibbs sampler iteration 1790 of 10000  #> Gibbs sampler iteration 1800 of 10000  #> Gibbs sampler iteration 1810 of 10000  #> Gibbs sampler iteration 1820 of 10000  #> Gibbs sampler iteration 1830 of 10000  #> Gibbs sampler iteration 1840 of 10000  #> Gibbs sampler iteration 1850 of 10000  #> Gibbs sampler iteration 1860 of 10000  #> Gibbs sampler iteration 1870 of 10000  #> Gibbs sampler iteration 1880 of 10000  #> Gibbs sampler iteration 1890 of 10000  #> Gibbs sampler iteration 1900 of 10000  #> Gibbs sampler iteration 1910 of 10000  #> Gibbs sampler iteration 1920 of 10000  #> Gibbs sampler iteration 1930 of 10000  #> Gibbs sampler iteration 1940 of 10000  #> Gibbs sampler iteration 1950 of 10000  #> Gibbs sampler iteration 1960 of 10000  #> Gibbs sampler iteration 1970 of 10000  #> Gibbs sampler iteration 1980 of 10000  #> Gibbs sampler iteration 1990 of 10000  #> Gibbs sampler iteration 2000 of 10000  #> Gibbs sampler iteration 2010 of 10000  #> Gibbs sampler iteration 2020 of 10000  #> Gibbs sampler iteration 2030 of 10000  #> Gibbs sampler iteration 2040 of 10000  #> Gibbs sampler iteration 2050 of 10000  #> Gibbs sampler iteration 2060 of 10000  #> Gibbs sampler iteration 2070 of 10000  #> Gibbs sampler iteration 2080 of 10000  #> Gibbs sampler iteration 2090 of 10000  #> Gibbs sampler iteration 2100 of 10000  #> Gibbs sampler iteration 2110 of 10000  #> Gibbs sampler iteration 2120 of 10000  #> Gibbs sampler iteration 2130 of 10000  #> Gibbs sampler iteration 2140 of 10000  #> Gibbs sampler iteration 2150 of 10000  #> Gibbs sampler iteration 2160 of 10000  #> Gibbs sampler iteration 2170 of 10000  #> Gibbs sampler iteration 2180 of 10000  #> Gibbs sampler iteration 2190 of 10000  #> Gibbs sampler iteration 2200 of 10000  #> Gibbs sampler iteration 2210 of 10000  #> Gibbs sampler iteration 2220 of 10000  #> Gibbs sampler iteration 2230 of 10000  #> Gibbs sampler iteration 2240 of 10000  #> Gibbs sampler iteration 2250 of 10000  #> Gibbs sampler iteration 2260 of 10000  #> Gibbs sampler iteration 2270 of 10000  #> Gibbs sampler iteration 2280 of 10000  #> Gibbs sampler iteration 2290 of 10000  #> Gibbs sampler iteration 2300 of 10000  #> Gibbs sampler iteration 2310 of 10000  #> Gibbs sampler iteration 2320 of 10000  #> Gibbs sampler iteration 2330 of 10000  #> Gibbs sampler iteration 2340 of 10000  #> Gibbs sampler iteration 2350 of 10000  #> Gibbs sampler iteration 2360 of 10000  #> Gibbs sampler iteration 2370 of 10000  #> Gibbs sampler iteration 2380 of 10000  #> Gibbs sampler iteration 2390 of 10000  #> Gibbs sampler iteration 2400 of 10000  #> Gibbs sampler iteration 2410 of 10000  #> Gibbs sampler iteration 2420 of 10000  #> Gibbs sampler iteration 2430 of 10000  #> Gibbs sampler iteration 2440 of 10000  #> Gibbs sampler iteration 2450 of 10000  #> Gibbs sampler iteration 2460 of 10000  #> Gibbs sampler iteration 2470 of 10000  #> Gibbs sampler iteration 2480 of 10000  #> Gibbs sampler iteration 2490 of 10000  #> Gibbs sampler iteration 2500 of 10000  #> Gibbs sampler iteration 2510 of 10000  #> Gibbs sampler iteration 2520 of 10000  #> Gibbs sampler iteration 2530 of 10000  #> Gibbs sampler iteration 2540 of 10000  #> Gibbs sampler iteration 2550 of 10000  #> Gibbs sampler iteration 2560 of 10000  #> Gibbs sampler iteration 2570 of 10000  #> Gibbs sampler iteration 2580 of 10000  #> Gibbs sampler iteration 2590 of 10000  #> Gibbs sampler iteration 2600 of 10000  #> Gibbs sampler iteration 2610 of 10000  #> Gibbs sampler iteration 2620 of 10000  #> Gibbs sampler iteration 2630 of 10000  #> Gibbs sampler iteration 2640 of 10000  #> Gibbs sampler iteration 2650 of 10000  #> Gibbs sampler iteration 2660 of 10000  #> Gibbs sampler iteration 2670 of 10000  #> Gibbs sampler iteration 2680 of 10000  #> Gibbs sampler iteration 2690 of 10000  #> Gibbs sampler iteration 2700 of 10000  #> Gibbs sampler iteration 2710 of 10000  #> Gibbs sampler iteration 2720 of 10000  #> Gibbs sampler iteration 2730 of 10000  #> Gibbs sampler iteration 2740 of 10000  #> Gibbs sampler iteration 2750 of 10000  #> Gibbs sampler iteration 2760 of 10000  #> Gibbs sampler iteration 2770 of 10000  #> Gibbs sampler iteration 2780 of 10000  #> Gibbs sampler iteration 2790 of 10000  #> Gibbs sampler iteration 2800 of 10000  #> Gibbs sampler iteration 2810 of 10000  #> Gibbs sampler iteration 2820 of 10000  #> Gibbs sampler iteration 2830 of 10000  #> Gibbs sampler iteration 2840 of 10000  #> Gibbs sampler iteration 2850 of 10000  #> Gibbs sampler iteration 2860 of 10000  #> Gibbs sampler iteration 2870 of 10000  #> Gibbs sampler iteration 2880 of 10000  #> Gibbs sampler iteration 2890 of 10000  #> Gibbs sampler iteration 2900 of 10000  #> Gibbs sampler iteration 2910 of 10000  #> Gibbs sampler iteration 2920 of 10000  #> Gibbs sampler iteration 2930 of 10000  #> Gibbs sampler iteration 2940 of 10000  #> Gibbs sampler iteration 2950 of 10000  #> Gibbs sampler iteration 2960 of 10000  #> Gibbs sampler iteration 2970 of 10000  #> Gibbs sampler iteration 2980 of 10000  #> Gibbs sampler iteration 2990 of 10000  #> Gibbs sampler iteration 3000 of 10000  #> Gibbs sampler iteration 3010 of 10000  #> Gibbs sampler iteration 3020 of 10000  #> Gibbs sampler iteration 3030 of 10000  #> Gibbs sampler iteration 3040 of 10000  #> Gibbs sampler iteration 3050 of 10000  #> Gibbs sampler iteration 3060 of 10000  #> Gibbs sampler iteration 3070 of 10000  #> Gibbs sampler iteration 3080 of 10000  #> Gibbs sampler iteration 3090 of 10000  #> Gibbs sampler iteration 3100 of 10000  #> Gibbs sampler iteration 3110 of 10000  #> Gibbs sampler iteration 3120 of 10000  #> Gibbs sampler iteration 3130 of 10000  #> Gibbs sampler iteration 3140 of 10000  #> Gibbs sampler iteration 3150 of 10000  #> Gibbs sampler iteration 3160 of 10000  #> Gibbs sampler iteration 3170 of 10000  #> Gibbs sampler iteration 3180 of 10000  #> Gibbs sampler iteration 3190 of 10000  #> Gibbs sampler iteration 3200 of 10000  #> Gibbs sampler iteration 3210 of 10000  #> Gibbs sampler iteration 3220 of 10000  #> Gibbs sampler iteration 3230 of 10000  #> Gibbs sampler iteration 3240 of 10000  #> Gibbs sampler iteration 3250 of 10000  #> Gibbs sampler iteration 3260 of 10000  #> Gibbs sampler iteration 3270 of 10000  #> Gibbs sampler iteration 3280 of 10000  #> Gibbs sampler iteration 3290 of 10000  #> Gibbs sampler iteration 3300 of 10000  #> Gibbs sampler iteration 3310 of 10000  #> Gibbs sampler iteration 3320 of 10000  #> Gibbs sampler iteration 3330 of 10000  #> Gibbs sampler iteration 3340 of 10000  #> Gibbs sampler iteration 3350 of 10000  #> Gibbs sampler iteration 3360 of 10000  #> Gibbs sampler iteration 3370 of 10000  #> Gibbs sampler iteration 3380 of 10000  #> Gibbs sampler iteration 3390 of 10000  #> Gibbs sampler iteration 3400 of 10000  #> Gibbs sampler iteration 3410 of 10000  #> Gibbs sampler iteration 3420 of 10000  #> Gibbs sampler iteration 3430 of 10000  #> Gibbs sampler iteration 3440 of 10000  #> Gibbs sampler iteration 3450 of 10000  #> Gibbs sampler iteration 3460 of 10000  #> Gibbs sampler iteration 3470 of 10000  #> Gibbs sampler iteration 3480 of 10000  #> Gibbs sampler iteration 3490 of 10000  #> Gibbs sampler iteration 3500 of 10000  #> Gibbs sampler iteration 3510 of 10000  #> Gibbs sampler iteration 3520 of 10000  #> Gibbs sampler iteration 3530 of 10000  #> Gibbs sampler iteration 3540 of 10000  #> Gibbs sampler iteration 3550 of 10000  #> Gibbs sampler iteration 3560 of 10000  #> Gibbs sampler iteration 3570 of 10000  #> Gibbs sampler iteration 3580 of 10000  #> Gibbs sampler iteration 3590 of 10000  #> Gibbs sampler iteration 3600 of 10000  #> Gibbs sampler iteration 3610 of 10000  #> Gibbs sampler iteration 3620 of 10000  #> Gibbs sampler iteration 3630 of 10000  #> Gibbs sampler iteration 3640 of 10000  #> Gibbs sampler iteration 3650 of 10000  #> Gibbs sampler iteration 3660 of 10000  #> Gibbs sampler iteration 3670 of 10000  #> Gibbs sampler iteration 3680 of 10000  #> Gibbs sampler iteration 3690 of 10000  #> Gibbs sampler iteration 3700 of 10000  #> Gibbs sampler iteration 3710 of 10000  #> Gibbs sampler iteration 3720 of 10000  #> Gibbs sampler iteration 3730 of 10000  #> Gibbs sampler iteration 3740 of 10000  #> Gibbs sampler iteration 3750 of 10000  #> Gibbs sampler iteration 3760 of 10000  #> Gibbs sampler iteration 3770 of 10000  #> Gibbs sampler iteration 3780 of 10000  #> Gibbs sampler iteration 3790 of 10000  #> Gibbs sampler iteration 3800 of 10000  #> Gibbs sampler iteration 3810 of 10000  #> Gibbs sampler iteration 3820 of 10000  #> Gibbs sampler iteration 3830 of 10000  #> Gibbs sampler iteration 3840 of 10000  #> Gibbs sampler iteration 3850 of 10000  #> Gibbs sampler iteration 3860 of 10000  #> Gibbs sampler iteration 3870 of 10000  #> Gibbs sampler iteration 3880 of 10000  #> Gibbs sampler iteration 3890 of 10000  #> Gibbs sampler iteration 3900 of 10000  #> Gibbs sampler iteration 3910 of 10000  #> Gibbs sampler iteration 3920 of 10000  #> Gibbs sampler iteration 3930 of 10000  #> Gibbs sampler iteration 3940 of 10000  #> Gibbs sampler iteration 3950 of 10000  #> Gibbs sampler iteration 3960 of 10000  #> Gibbs sampler iteration 3970 of 10000  #> Gibbs sampler iteration 3980 of 10000  #> Gibbs sampler iteration 3990 of 10000  #> Gibbs sampler iteration 4000 of 10000  #> Gibbs sampler iteration 4010 of 10000  #> Gibbs sampler iteration 4020 of 10000  #> Gibbs sampler iteration 4030 of 10000  #> Gibbs sampler iteration 4040 of 10000  #> Gibbs sampler iteration 4050 of 10000  #> Gibbs sampler iteration 4060 of 10000  #> Gibbs sampler iteration 4070 of 10000  #> Gibbs sampler iteration 4080 of 10000  #> Gibbs sampler iteration 4090 of 10000  #> Gibbs sampler iteration 4100 of 10000  #> Gibbs sampler iteration 4110 of 10000  #> Gibbs sampler iteration 4120 of 10000  #> Gibbs sampler iteration 4130 of 10000  #> Gibbs sampler iteration 4140 of 10000  #> Gibbs sampler iteration 4150 of 10000  #> Gibbs sampler iteration 4160 of 10000  #> Gibbs sampler iteration 4170 of 10000  #> Gibbs sampler iteration 4180 of 10000  #> Gibbs sampler iteration 4190 of 10000  #> Gibbs sampler iteration 4200 of 10000  #> Gibbs sampler iteration 4210 of 10000  #> Gibbs sampler iteration 4220 of 10000  #> Gibbs sampler iteration 4230 of 10000  #> Gibbs sampler iteration 4240 of 10000  #> Gibbs sampler iteration 4250 of 10000  #> Gibbs sampler iteration 4260 of 10000  #> Gibbs sampler iteration 4270 of 10000  #> Gibbs sampler iteration 4280 of 10000  #> Gibbs sampler iteration 4290 of 10000  #> Gibbs sampler iteration 4300 of 10000  #> Gibbs sampler iteration 4310 of 10000  #> Gibbs sampler iteration 4320 of 10000  #> Gibbs sampler iteration 4330 of 10000  #> Gibbs sampler iteration 4340 of 10000  #> Gibbs sampler iteration 4350 of 10000  #> Gibbs sampler iteration 4360 of 10000  #> Gibbs sampler iteration 4370 of 10000  #> Gibbs sampler iteration 4380 of 10000  #> Gibbs sampler iteration 4390 of 10000  #> Gibbs sampler iteration 4400 of 10000  #> Gibbs sampler iteration 4410 of 10000  #> Gibbs sampler iteration 4420 of 10000  #> Gibbs sampler iteration 4430 of 10000  #> Gibbs sampler iteration 4440 of 10000  #> Gibbs sampler iteration 4450 of 10000  #> Gibbs sampler iteration 4460 of 10000  #> Gibbs sampler iteration 4470 of 10000  #> Gibbs sampler iteration 4480 of 10000  #> Gibbs sampler iteration 4490 of 10000  #> Gibbs sampler iteration 4500 of 10000  #> Gibbs sampler iteration 4510 of 10000  #> Gibbs sampler iteration 4520 of 10000  #> Gibbs sampler iteration 4530 of 10000  #> Gibbs sampler iteration 4540 of 10000  #> Gibbs sampler iteration 4550 of 10000  #> Gibbs sampler iteration 4560 of 10000  #> Gibbs sampler iteration 4570 of 10000  #> Gibbs sampler iteration 4580 of 10000  #> Gibbs sampler iteration 4590 of 10000  #> Gibbs sampler iteration 4600 of 10000  #> Gibbs sampler iteration 4610 of 10000  #> Gibbs sampler iteration 4620 of 10000  #> Gibbs sampler iteration 4630 of 10000  #> Gibbs sampler iteration 4640 of 10000  #> Gibbs sampler iteration 4650 of 10000  #> Gibbs sampler iteration 4660 of 10000  #> Gibbs sampler iteration 4670 of 10000  #> Gibbs sampler iteration 4680 of 10000  #> Gibbs sampler iteration 4690 of 10000  #> Gibbs sampler iteration 4700 of 10000  #> Gibbs sampler iteration 4710 of 10000  #> Gibbs sampler iteration 4720 of 10000  #> Gibbs sampler iteration 4730 of 10000  #> Gibbs sampler iteration 4740 of 10000  #> Gibbs sampler iteration 4750 of 10000  #> Gibbs sampler iteration 4760 of 10000  #> Gibbs sampler iteration 4770 of 10000  #> Gibbs sampler iteration 4780 of 10000  #> Gibbs sampler iteration 4790 of 10000  #> Gibbs sampler iteration 4800 of 10000  #> Gibbs sampler iteration 4810 of 10000  #> Gibbs sampler iteration 4820 of 10000  #> Gibbs sampler iteration 4830 of 10000  #> Gibbs sampler iteration 4840 of 10000  #> Gibbs sampler iteration 4850 of 10000  #> Gibbs sampler iteration 4860 of 10000  #> Gibbs sampler iteration 4870 of 10000  #> Gibbs sampler iteration 4880 of 10000  #> Gibbs sampler iteration 4890 of 10000  #> Gibbs sampler iteration 4900 of 10000  #> Gibbs sampler iteration 4910 of 10000  #> Gibbs sampler iteration 4920 of 10000  #> Gibbs sampler iteration 4930 of 10000  #> Gibbs sampler iteration 4940 of 10000  #> Gibbs sampler iteration 4950 of 10000  #> Gibbs sampler iteration 4960 of 10000  #> Gibbs sampler iteration 4970 of 10000  #> Gibbs sampler iteration 4980 of 10000  #> Gibbs sampler iteration 4990 of 10000  #> Gibbs sampler iteration 5000 of 10000  #> Gibbs sampler iteration 5010 of 10000  #> Gibbs sampler iteration 5020 of 10000  #> Gibbs sampler iteration 5030 of 10000  #> Gibbs sampler iteration 5040 of 10000  #> Gibbs sampler iteration 5050 of 10000  #> Gibbs sampler iteration 5060 of 10000  #> Gibbs sampler iteration 5070 of 10000  #> Gibbs sampler iteration 5080 of 10000  #> Gibbs sampler iteration 5090 of 10000  #> Gibbs sampler iteration 5100 of 10000  #> Gibbs sampler iteration 5110 of 10000  #> Gibbs sampler iteration 5120 of 10000  #> Gibbs sampler iteration 5130 of 10000  #> Gibbs sampler iteration 5140 of 10000  #> Gibbs sampler iteration 5150 of 10000  #> Gibbs sampler iteration 5160 of 10000  #> Gibbs sampler iteration 5170 of 10000  #> Gibbs sampler iteration 5180 of 10000  #> Gibbs sampler iteration 5190 of 10000  #> Gibbs sampler iteration 5200 of 10000  #> Gibbs sampler iteration 5210 of 10000  #> Gibbs sampler iteration 5220 of 10000  #> Gibbs sampler iteration 5230 of 10000  #> Gibbs sampler iteration 5240 of 10000  #> Gibbs sampler iteration 5250 of 10000  #> Gibbs sampler iteration 5260 of 10000  #> Gibbs sampler iteration 5270 of 10000  #> Gibbs sampler iteration 5280 of 10000  #> Gibbs sampler iteration 5290 of 10000  #> Gibbs sampler iteration 5300 of 10000  #> Gibbs sampler iteration 5310 of 10000  #> Gibbs sampler iteration 5320 of 10000  #> Gibbs sampler iteration 5330 of 10000  #> Gibbs sampler iteration 5340 of 10000  #> Gibbs sampler iteration 5350 of 10000  #> Gibbs sampler iteration 5360 of 10000  #> Gibbs sampler iteration 5370 of 10000  #> Gibbs sampler iteration 5380 of 10000  #> Gibbs sampler iteration 5390 of 10000  #> Gibbs sampler iteration 5400 of 10000  #> Gibbs sampler iteration 5410 of 10000  #> Gibbs sampler iteration 5420 of 10000  #> Gibbs sampler iteration 5430 of 10000  #> Gibbs sampler iteration 5440 of 10000  #> Gibbs sampler iteration 5450 of 10000  #> Gibbs sampler iteration 5460 of 10000  #> Gibbs sampler iteration 5470 of 10000  #> Gibbs sampler iteration 5480 of 10000  #> Gibbs sampler iteration 5490 of 10000  #> Gibbs sampler iteration 5500 of 10000  #> Gibbs sampler iteration 5510 of 10000  #> Gibbs sampler iteration 5520 of 10000  #> Gibbs sampler iteration 5530 of 10000  #> Gibbs sampler iteration 5540 of 10000  #> Gibbs sampler iteration 5550 of 10000  #> Gibbs sampler iteration 5560 of 10000  #> Gibbs sampler iteration 5570 of 10000  #> Gibbs sampler iteration 5580 of 10000  #> Gibbs sampler iteration 5590 of 10000  #> Gibbs sampler iteration 5600 of 10000  #> Gibbs sampler iteration 5610 of 10000  #> Gibbs sampler iteration 5620 of 10000  #> Gibbs sampler iteration 5630 of 10000  #> Gibbs sampler iteration 5640 of 10000  #> Gibbs sampler iteration 5650 of 10000  #> Gibbs sampler iteration 5660 of 10000  #> Gibbs sampler iteration 5670 of 10000  #> Gibbs sampler iteration 5680 of 10000  #> Gibbs sampler iteration 5690 of 10000  #> Gibbs sampler iteration 5700 of 10000  #> Gibbs sampler iteration 5710 of 10000  #> Gibbs sampler iteration 5720 of 10000  #> Gibbs sampler iteration 5730 of 10000  #> Gibbs sampler iteration 5740 of 10000  #> Gibbs sampler iteration 5750 of 10000  #> Gibbs sampler iteration 5760 of 10000  #> Gibbs sampler iteration 5770 of 10000  #> Gibbs sampler iteration 5780 of 10000  #> Gibbs sampler iteration 5790 of 10000  #> Gibbs sampler iteration 5800 of 10000  #> Gibbs sampler iteration 5810 of 10000  #> Gibbs sampler iteration 5820 of 10000  #> Gibbs sampler iteration 5830 of 10000  #> Gibbs sampler iteration 5840 of 10000  #> Gibbs sampler iteration 5850 of 10000  #> Gibbs sampler iteration 5860 of 10000  #> Gibbs sampler iteration 5870 of 10000  #> Gibbs sampler iteration 5880 of 10000  #> Gibbs sampler iteration 5890 of 10000  #> Gibbs sampler iteration 5900 of 10000  #> Gibbs sampler iteration 5910 of 10000  #> Gibbs sampler iteration 5920 of 10000  #> Gibbs sampler iteration 5930 of 10000  #> Gibbs sampler iteration 5940 of 10000  #> Gibbs sampler iteration 5950 of 10000  #> Gibbs sampler iteration 5960 of 10000  #> Gibbs sampler iteration 5970 of 10000  #> Gibbs sampler iteration 5980 of 10000  #> Gibbs sampler iteration 5990 of 10000  #> Gibbs sampler iteration 6000 of 10000  #> Gibbs sampler iteration 6010 of 10000  #> Gibbs sampler iteration 6020 of 10000  #> Gibbs sampler iteration 6030 of 10000  #> Gibbs sampler iteration 6040 of 10000  #> Gibbs sampler iteration 6050 of 10000  #> Gibbs sampler iteration 6060 of 10000  #> Gibbs sampler iteration 6070 of 10000  #> Gibbs sampler iteration 6080 of 10000  #> Gibbs sampler iteration 6090 of 10000  #> Gibbs sampler iteration 6100 of 10000  #> Gibbs sampler iteration 6110 of 10000  #> Gibbs sampler iteration 6120 of 10000  #> Gibbs sampler iteration 6130 of 10000  #> Gibbs sampler iteration 6140 of 10000  #> Gibbs sampler iteration 6150 of 10000  #> Gibbs sampler iteration 6160 of 10000  #> Gibbs sampler iteration 6170 of 10000  #> Gibbs sampler iteration 6180 of 10000  #> Gibbs sampler iteration 6190 of 10000  #> Gibbs sampler iteration 6200 of 10000  #> Gibbs sampler iteration 6210 of 10000  #> Gibbs sampler iteration 6220 of 10000  #> Gibbs sampler iteration 6230 of 10000  #> Gibbs sampler iteration 6240 of 10000  #> Gibbs sampler iteration 6250 of 10000  #> Gibbs sampler iteration 6260 of 10000  #> Gibbs sampler iteration 6270 of 10000  #> Gibbs sampler iteration 6280 of 10000  #> Gibbs sampler iteration 6290 of 10000  #> Gibbs sampler iteration 6300 of 10000  #> Gibbs sampler iteration 6310 of 10000  #> Gibbs sampler iteration 6320 of 10000  #> Gibbs sampler iteration 6330 of 10000  #> Gibbs sampler iteration 6340 of 10000  #> Gibbs sampler iteration 6350 of 10000  #> Gibbs sampler iteration 6360 of 10000  #> Gibbs sampler iteration 6370 of 10000  #> Gibbs sampler iteration 6380 of 10000  #> Gibbs sampler iteration 6390 of 10000  #> Gibbs sampler iteration 6400 of 10000  #> Gibbs sampler iteration 6410 of 10000  #> Gibbs sampler iteration 6420 of 10000  #> Gibbs sampler iteration 6430 of 10000  #> Gibbs sampler iteration 6440 of 10000  #> Gibbs sampler iteration 6450 of 10000  #> Gibbs sampler iteration 6460 of 10000  #> Gibbs sampler iteration 6470 of 10000  #> Gibbs sampler iteration 6480 of 10000  #> Gibbs sampler iteration 6490 of 10000  #> Gibbs sampler iteration 6500 of 10000  #> Gibbs sampler iteration 6510 of 10000  #> Gibbs sampler iteration 6520 of 10000  #> Gibbs sampler iteration 6530 of 10000  #> Gibbs sampler iteration 6540 of 10000  #> Gibbs sampler iteration 6550 of 10000  #> Gibbs sampler iteration 6560 of 10000  #> Gibbs sampler iteration 6570 of 10000  #> Gibbs sampler iteration 6580 of 10000  #> Gibbs sampler iteration 6590 of 10000  #> Gibbs sampler iteration 6600 of 10000  #> Gibbs sampler iteration 6610 of 10000  #> Gibbs sampler iteration 6620 of 10000  #> Gibbs sampler iteration 6630 of 10000  #> Gibbs sampler iteration 6640 of 10000  #> Gibbs sampler iteration 6650 of 10000  #> Gibbs sampler iteration 6660 of 10000  #> Gibbs sampler iteration 6670 of 10000  #> Gibbs sampler iteration 6680 of 10000  #> Gibbs sampler iteration 6690 of 10000  #> Gibbs sampler iteration 6700 of 10000  #> Gibbs sampler iteration 6710 of 10000  #> Gibbs sampler iteration 6720 of 10000  #> Gibbs sampler iteration 6730 of 10000  #> Gibbs sampler iteration 6740 of 10000  #> Gibbs sampler iteration 6750 of 10000  #> Gibbs sampler iteration 6760 of 10000  #> Gibbs sampler iteration 6770 of 10000  #> Gibbs sampler iteration 6780 of 10000  #> Gibbs sampler iteration 6790 of 10000  #> Gibbs sampler iteration 6800 of 10000  #> Gibbs sampler iteration 6810 of 10000  #> Gibbs sampler iteration 6820 of 10000  #> Gibbs sampler iteration 6830 of 10000  #> Gibbs sampler iteration 6840 of 10000  #> Gibbs sampler iteration 6850 of 10000  #> Gibbs sampler iteration 6860 of 10000  #> Gibbs sampler iteration 6870 of 10000  #> Gibbs sampler iteration 6880 of 10000  #> Gibbs sampler iteration 6890 of 10000  #> Gibbs sampler iteration 6900 of 10000  #> Gibbs sampler iteration 6910 of 10000  #> Gibbs sampler iteration 6920 of 10000  #> Gibbs sampler iteration 6930 of 10000  #> Gibbs sampler iteration 6940 of 10000  #> Gibbs sampler iteration 6950 of 10000  #> Gibbs sampler iteration 6960 of 10000  #> Gibbs sampler iteration 6970 of 10000  #> Gibbs sampler iteration 6980 of 10000  #> Gibbs sampler iteration 6990 of 10000  #> Gibbs sampler iteration 7000 of 10000  #> Gibbs sampler iteration 7010 of 10000  #> Gibbs sampler iteration 7020 of 10000  #> Gibbs sampler iteration 7030 of 10000  #> Gibbs sampler iteration 7040 of 10000  #> Gibbs sampler iteration 7050 of 10000  #> Gibbs sampler iteration 7060 of 10000  #> Gibbs sampler iteration 7070 of 10000  #> Gibbs sampler iteration 7080 of 10000  #> Gibbs sampler iteration 7090 of 10000  #> Gibbs sampler iteration 7100 of 10000  #> Gibbs sampler iteration 7110 of 10000  #> Gibbs sampler iteration 7120 of 10000  #> Gibbs sampler iteration 7130 of 10000  #> Gibbs sampler iteration 7140 of 10000  #> Gibbs sampler iteration 7150 of 10000  #> Gibbs sampler iteration 7160 of 10000  #> Gibbs sampler iteration 7170 of 10000  #> Gibbs sampler iteration 7180 of 10000  #> Gibbs sampler iteration 7190 of 10000  #> Gibbs sampler iteration 7200 of 10000  #> Gibbs sampler iteration 7210 of 10000  #> Gibbs sampler iteration 7220 of 10000  #> Gibbs sampler iteration 7230 of 10000  #> Gibbs sampler iteration 7240 of 10000  #> Gibbs sampler iteration 7250 of 10000  #> Gibbs sampler iteration 7260 of 10000  #> Gibbs sampler iteration 7270 of 10000  #> Gibbs sampler iteration 7280 of 10000  #> Gibbs sampler iteration 7290 of 10000  #> Gibbs sampler iteration 7300 of 10000  #> Gibbs sampler iteration 7310 of 10000  #> Gibbs sampler iteration 7320 of 10000  #> Gibbs sampler iteration 7330 of 10000  #> Gibbs sampler iteration 7340 of 10000  #> Gibbs sampler iteration 7350 of 10000  #> Gibbs sampler iteration 7360 of 10000  #> Gibbs sampler iteration 7370 of 10000  #> Gibbs sampler iteration 7380 of 10000  #> Gibbs sampler iteration 7390 of 10000  #> Gibbs sampler iteration 7400 of 10000  #> Gibbs sampler iteration 7410 of 10000  #> Gibbs sampler iteration 7420 of 10000  #> Gibbs sampler iteration 7430 of 10000  #> Gibbs sampler iteration 7440 of 10000  #> Gibbs sampler iteration 7450 of 10000  #> Gibbs sampler iteration 7460 of 10000  #> Gibbs sampler iteration 7470 of 10000  #> Gibbs sampler iteration 7480 of 10000  #> Gibbs sampler iteration 7490 of 10000  #> Gibbs sampler iteration 7500 of 10000  #> Gibbs sampler iteration 7510 of 10000  #> Gibbs sampler iteration 7520 of 10000  #> Gibbs sampler iteration 7530 of 10000  #> Gibbs sampler iteration 7540 of 10000  #> Gibbs sampler iteration 7550 of 10000  #> Gibbs sampler iteration 7560 of 10000  #> Gibbs sampler iteration 7570 of 10000  #> Gibbs sampler iteration 7580 of 10000  #> Gibbs sampler iteration 7590 of 10000  #> Gibbs sampler iteration 7600 of 10000  #> Gibbs sampler iteration 7610 of 10000  #> Gibbs sampler iteration 7620 of 10000  #> Gibbs sampler iteration 7630 of 10000  #> Gibbs sampler iteration 7640 of 10000  #> Gibbs sampler iteration 7650 of 10000  #> Gibbs sampler iteration 7660 of 10000  #> Gibbs sampler iteration 7670 of 10000  #> Gibbs sampler iteration 7680 of 10000  #> Gibbs sampler iteration 7690 of 10000  #> Gibbs sampler iteration 7700 of 10000  #> Gibbs sampler iteration 7710 of 10000  #> Gibbs sampler iteration 7720 of 10000  #> Gibbs sampler iteration 7730 of 10000  #> Gibbs sampler iteration 7740 of 10000  #> Gibbs sampler iteration 7750 of 10000  #> Gibbs sampler iteration 7760 of 10000  #> Gibbs sampler iteration 7770 of 10000  #> Gibbs sampler iteration 7780 of 10000  #> Gibbs sampler iteration 7790 of 10000  #> Gibbs sampler iteration 7800 of 10000  #> Gibbs sampler iteration 7810 of 10000  #> Gibbs sampler iteration 7820 of 10000  #> Gibbs sampler iteration 7830 of 10000  #> Gibbs sampler iteration 7840 of 10000  #> Gibbs sampler iteration 7850 of 10000  #> Gibbs sampler iteration 7860 of 10000  #> Gibbs sampler iteration 7870 of 10000  #> Gibbs sampler iteration 7880 of 10000  #> Gibbs sampler iteration 7890 of 10000  #> Gibbs sampler iteration 7900 of 10000  #> Gibbs sampler iteration 7910 of 10000  #> Gibbs sampler iteration 7920 of 10000  #> Gibbs sampler iteration 7930 of 10000  #> Gibbs sampler iteration 7940 of 10000  #> Gibbs sampler iteration 7950 of 10000  #> Gibbs sampler iteration 7960 of 10000  #> Gibbs sampler iteration 7970 of 10000  #> Gibbs sampler iteration 7980 of 10000  #> Gibbs sampler iteration 7990 of 10000  #> Gibbs sampler iteration 8000 of 10000  #> Gibbs sampler iteration 8010 of 10000  #> Gibbs sampler iteration 8020 of 10000  #> Gibbs sampler iteration 8030 of 10000  #> Gibbs sampler iteration 8040 of 10000  #> Gibbs sampler iteration 8050 of 10000  #> Gibbs sampler iteration 8060 of 10000  #> Gibbs sampler iteration 8070 of 10000  #> Gibbs sampler iteration 8080 of 10000  #> Gibbs sampler iteration 8090 of 10000  #> Gibbs sampler iteration 8100 of 10000  #> Gibbs sampler iteration 8110 of 10000  #> Gibbs sampler iteration 8120 of 10000  #> Gibbs sampler iteration 8130 of 10000  #> Gibbs sampler iteration 8140 of 10000  #> Gibbs sampler iteration 8150 of 10000  #> Gibbs sampler iteration 8160 of 10000  #> Gibbs sampler iteration 8170 of 10000  #> Gibbs sampler iteration 8180 of 10000  #> Gibbs sampler iteration 8190 of 10000  #> Gibbs sampler iteration 8200 of 10000  #> Gibbs sampler iteration 8210 of 10000  #> Gibbs sampler iteration 8220 of 10000  #> Gibbs sampler iteration 8230 of 10000  #> Gibbs sampler iteration 8240 of 10000  #> Gibbs sampler iteration 8250 of 10000  #> Gibbs sampler iteration 8260 of 10000  #> Gibbs sampler iteration 8270 of 10000  #> Gibbs sampler iteration 8280 of 10000  #> Gibbs sampler iteration 8290 of 10000  #> Gibbs sampler iteration 8300 of 10000  #> Gibbs sampler iteration 8310 of 10000  #> Gibbs sampler iteration 8320 of 10000  #> Gibbs sampler iteration 8330 of 10000  #> Gibbs sampler iteration 8340 of 10000  #> Gibbs sampler iteration 8350 of 10000  #> Gibbs sampler iteration 8360 of 10000  #> Gibbs sampler iteration 8370 of 10000  #> Gibbs sampler iteration 8380 of 10000  #> Gibbs sampler iteration 8390 of 10000  #> Gibbs sampler iteration 8400 of 10000  #> Gibbs sampler iteration 8410 of 10000  #> Gibbs sampler iteration 8420 of 10000  #> Gibbs sampler iteration 8430 of 10000  #> Gibbs sampler iteration 8440 of 10000  #> Gibbs sampler iteration 8450 of 10000  #> Gibbs sampler iteration 8460 of 10000  #> Gibbs sampler iteration 8470 of 10000  #> Gibbs sampler iteration 8480 of 10000  #> Gibbs sampler iteration 8490 of 10000  #> Gibbs sampler iteration 8500 of 10000  #> Gibbs sampler iteration 8510 of 10000  #> Gibbs sampler iteration 8520 of 10000  #> Gibbs sampler iteration 8530 of 10000  #> Gibbs sampler iteration 8540 of 10000  #> Gibbs sampler iteration 8550 of 10000  #> Gibbs sampler iteration 8560 of 10000  #> Gibbs sampler iteration 8570 of 10000  #> Gibbs sampler iteration 8580 of 10000  #> Gibbs sampler iteration 8590 of 10000  #> Gibbs sampler iteration 8600 of 10000  #> Gibbs sampler iteration 8610 of 10000  #> Gibbs sampler iteration 8620 of 10000  #> Gibbs sampler iteration 8630 of 10000  #> Gibbs sampler iteration 8640 of 10000  #> Gibbs sampler iteration 8650 of 10000  #> Gibbs sampler iteration 8660 of 10000  #> Gibbs sampler iteration 8670 of 10000  #> Gibbs sampler iteration 8680 of 10000  #> Gibbs sampler iteration 8690 of 10000  #> Gibbs sampler iteration 8700 of 10000  #> Gibbs sampler iteration 8710 of 10000  #> Gibbs sampler iteration 8720 of 10000  #> Gibbs sampler iteration 8730 of 10000  #> Gibbs sampler iteration 8740 of 10000  #> Gibbs sampler iteration 8750 of 10000  #> Gibbs sampler iteration 8760 of 10000  #> Gibbs sampler iteration 8770 of 10000  #> Gibbs sampler iteration 8780 of 10000  #> Gibbs sampler iteration 8790 of 10000  #> Gibbs sampler iteration 8800 of 10000  #> Gibbs sampler iteration 8810 of 10000  #> Gibbs sampler iteration 8820 of 10000  #> Gibbs sampler iteration 8830 of 10000  #> Gibbs sampler iteration 8840 of 10000  #> Gibbs sampler iteration 8850 of 10000  #> Gibbs sampler iteration 8860 of 10000  #> Gibbs sampler iteration 8870 of 10000  #> Gibbs sampler iteration 8880 of 10000  #> Gibbs sampler iteration 8890 of 10000  #> Gibbs sampler iteration 8900 of 10000  #> Gibbs sampler iteration 8910 of 10000  #> Gibbs sampler iteration 8920 of 10000  #> Gibbs sampler iteration 8930 of 10000  #> Gibbs sampler iteration 8940 of 10000  #> Gibbs sampler iteration 8950 of 10000  #> Gibbs sampler iteration 8960 of 10000  #> Gibbs sampler iteration 8970 of 10000  #> Gibbs sampler iteration 8980 of 10000  #> Gibbs sampler iteration 8990 of 10000  #> Gibbs sampler iteration 9000 of 10000  #> Gibbs sampler iteration 9010 of 10000  #> Gibbs sampler iteration 9020 of 10000  #> Gibbs sampler iteration 9030 of 10000  #> Gibbs sampler iteration 9040 of 10000  #> Gibbs sampler iteration 9050 of 10000  #> Gibbs sampler iteration 9060 of 10000  #> Gibbs sampler iteration 9070 of 10000  #> Gibbs sampler iteration 9080 of 10000  #> Gibbs sampler iteration 9090 of 10000  #> Gibbs sampler iteration 9100 of 10000  #> Gibbs sampler iteration 9110 of 10000  #> Gibbs sampler iteration 9120 of 10000  #> Gibbs sampler iteration 9130 of 10000  #> Gibbs sampler iteration 9140 of 10000  #> Gibbs sampler iteration 9150 of 10000  #> Gibbs sampler iteration 9160 of 10000  #> Gibbs sampler iteration 9170 of 10000  #> Gibbs sampler iteration 9180 of 10000  #> Gibbs sampler iteration 9190 of 10000  #> Gibbs sampler iteration 9200 of 10000  #> Gibbs sampler iteration 9210 of 10000  #> Gibbs sampler iteration 9220 of 10000  #> Gibbs sampler iteration 9230 of 10000  #> Gibbs sampler iteration 9240 of 10000  #> Gibbs sampler iteration 9250 of 10000  #> Gibbs sampler iteration 9260 of 10000  #> Gibbs sampler iteration 9270 of 10000  #> Gibbs sampler iteration 9280 of 10000  #> Gibbs sampler iteration 9290 of 10000  #> Gibbs sampler iteration 9300 of 10000  #> Gibbs sampler iteration 9310 of 10000  #> Gibbs sampler iteration 9320 of 10000  #> Gibbs sampler iteration 9330 of 10000  #> Gibbs sampler iteration 9340 of 10000  #> Gibbs sampler iteration 9350 of 10000  #> Gibbs sampler iteration 9360 of 10000  #> Gibbs sampler iteration 9370 of 10000  #> Gibbs sampler iteration 9380 of 10000  #> Gibbs sampler iteration 9390 of 10000  #> Gibbs sampler iteration 9400 of 10000  #> Gibbs sampler iteration 9410 of 10000  #> Gibbs sampler iteration 9420 of 10000  #> Gibbs sampler iteration 9430 of 10000  #> Gibbs sampler iteration 9440 of 10000  #> Gibbs sampler iteration 9450 of 10000  #> Gibbs sampler iteration 9460 of 10000  #> Gibbs sampler iteration 9470 of 10000  #> Gibbs sampler iteration 9480 of 10000  #> Gibbs sampler iteration 9490 of 10000  #> Gibbs sampler iteration 9500 of 10000  #> Gibbs sampler iteration 9510 of 10000  #> Gibbs sampler iteration 9520 of 10000  #> Gibbs sampler iteration 9530 of 10000  #> Gibbs sampler iteration 9540 of 10000  #> Gibbs sampler iteration 9550 of 10000  #> Gibbs sampler iteration 9560 of 10000  #> Gibbs sampler iteration 9570 of 10000  #> Gibbs sampler iteration 9580 of 10000  #> Gibbs sampler iteration 9590 of 10000  #> Gibbs sampler iteration 9600 of 10000  #> Gibbs sampler iteration 9610 of 10000  #> Gibbs sampler iteration 9620 of 10000  #> Gibbs sampler iteration 9630 of 10000  #> Gibbs sampler iteration 9640 of 10000  #> Gibbs sampler iteration 9650 of 10000  #> Gibbs sampler iteration 9660 of 10000  #> Gibbs sampler iteration 9670 of 10000  #> Gibbs sampler iteration 9680 of 10000  #> Gibbs sampler iteration 9690 of 10000  #> Gibbs sampler iteration 9700 of 10000  #> Gibbs sampler iteration 9710 of 10000  #> Gibbs sampler iteration 9720 of 10000  #> Gibbs sampler iteration 9730 of 10000  #> Gibbs sampler iteration 9740 of 10000  #> Gibbs sampler iteration 9750 of 10000  #> Gibbs sampler iteration 9760 of 10000  #> Gibbs sampler iteration 9770 of 10000  #> Gibbs sampler iteration 9780 of 10000  #> Gibbs sampler iteration 9790 of 10000  #> Gibbs sampler iteration 9800 of 10000  #> Gibbs sampler iteration 9810 of 10000  #> Gibbs sampler iteration 9820 of 10000  #> Gibbs sampler iteration 9830 of 10000  #> Gibbs sampler iteration 9840 of 10000  #> Gibbs sampler iteration 9850 of 10000  #> Gibbs sampler iteration 9860 of 10000  #> Gibbs sampler iteration 9870 of 10000  #> Gibbs sampler iteration 9880 of 10000  #> Gibbs sampler iteration 9890 of 10000  #> Gibbs sampler iteration 9900 of 10000  #> Gibbs sampler iteration 9910 of 10000  #> Gibbs sampler iteration 9920 of 10000  #> Gibbs sampler iteration 9930 of 10000  #> Gibbs sampler iteration 9940 of 10000  #> Gibbs sampler iteration 9950 of 10000  #> Gibbs sampler iteration 9960 of 10000  #> Gibbs sampler iteration 9970 of 10000  #> Gibbs sampler iteration 9980 of 10000  #> Gibbs sampler iteration 9990 of 10000  #> Gibbs sampler iteration 10000 of 10000  choice_probabilities(x) #>     id idc            A            B #> 1    1   1 3.488471e-01 6.511529e-01 #> 2    1   2 9.901432e-01 9.856754e-03 #> 3    1   3 3.361375e-02 9.663863e-01 #> 4    1   4 7.170946e-01 2.829054e-01 #> 5    1   5 9.891488e-01 1.085119e-02 #> 6    1   6 8.218406e-03 9.917816e-01 #> 7    1   7 9.833668e-01 1.663322e-02 #> 8    1   8 5.889822e-01 4.110178e-01 #> 9    1   9 5.974491e-02 9.402551e-01 #> 10   1  10 3.716036e-01 6.283964e-01 #> 11   2   1 9.996039e-01 3.961023e-04 #> 12   2   2 1.000000e+00 1.999041e-09 #> 13   2   3 7.040252e-02 9.295975e-01 #> 14   2   4 9.379436e-01 6.205643e-02 #> 15   2   5 9.999996e-01 3.679227e-07 #> 16   2   6 9.976947e-01 2.305288e-03 #> 17   2   7 6.257812e-01 3.742188e-01 #> 18   2   8 9.999946e-01 5.396004e-06 #> 19   2   9 8.212444e-02 9.178756e-01 #> 20   2  10 9.999147e-01 8.530336e-05 #> 21   3   1 9.352424e-01 6.475765e-02 #> 22   3   2 9.999966e-01 3.390629e-06 #> 23   3   3 7.568379e-04 9.992432e-01 #> 24   3   4 3.691238e-04 9.996309e-01 #> 25   3   5 8.012606e-01 1.987394e-01 #> 26   3   6 7.715874e-01 2.284126e-01 #> 27   3   7 2.860758e-02 9.713924e-01 #> 28   3   8 1.568068e-04 9.998432e-01 #> 29   3   9 9.999987e-01 1.330422e-06 #> 30   3  10 6.551036e-01 3.448964e-01 #> 31   4   1 9.969204e-01 3.079646e-03 #> 32   4   2 1.000000e+00 4.996779e-08 #> 33   4   3 6.459813e-04 9.993540e-01 #> 34   4   4 9.628861e-01 3.711389e-02 #> 35   4   5 1.000000e+00 2.625367e-12 #> 36   4   6 9.999858e-01 1.415871e-05 #> 37   4   7 3.432759e-01 6.567241e-01 #> 38   4   8 1.293913e-09 1.000000e+00 #> 39   4   9 9.999973e-01 2.668315e-06 #> 40   4  10 1.469630e-04 9.998530e-01 #> 41   5   1 1.135634e-07 9.999999e-01 #> 42   5   2 9.998697e-01 1.303333e-04 #> 43   5   3 6.335703e-03 9.936643e-01 #> 44   5   4 9.552811e-01 4.471888e-02 #> 45   5   5 6.259935e-03 9.937401e-01 #> 46   5   6 9.999850e-01 1.499168e-05 #> 47   5   7 9.994364e-01 5.636385e-04 #> 48   5   8 1.000000e+00 4.472188e-21 #> 49   5   9 2.718768e-05 9.999728e-01 #> 50   5  10 9.429821e-01 5.701795e-02 #> 51   6   1 4.232371e-04 9.995768e-01 #> 52   6   2 2.116331e-01 7.883669e-01 #> 53   6   3 9.999929e-01 7.099427e-06 #> 54   6   4 5.933817e-04 9.994066e-01 #> 55   6   5 1.000000e+00 5.034079e-19 #> 56   6   6 4.772523e-02 9.522748e-01 #> 57   6   7 9.999222e-01 7.784106e-05 #> 58   6   8 9.999937e-01 6.322413e-06 #> 59   6   9 7.598272e-01 2.401728e-01 #> 60   6  10 9.999654e-01 3.461628e-05 #> 61   7   1 9.994158e-01 5.841549e-04 #> 62   7   2 9.004963e-01 9.950372e-02 #> 63   7   3 4.927569e-05 9.999507e-01 #> 64   7   4 9.999450e-01 5.500719e-05 #> 65   7   5 1.000000e+00 1.735801e-12 #> 66   7   6 1.272504e-01 8.727496e-01 #> 67   7   7 1.000000e+00 1.911193e-15 #> 68   7   8 6.798343e-12 1.000000e+00 #> 69   7   9 9.999806e-01 1.942561e-05 #> 70   7  10 1.000000e+00 3.561515e-09 #> 71   8   1 1.000000e+00 3.245385e-39 #> 72   8   2 8.024921e-01 1.975079e-01 #> 73   8   3 2.465093e-01 7.534907e-01 #> 74   8   4 9.436019e-01 5.639805e-02 #> 75   8   5 9.999627e-01 3.732249e-05 #> 76   8   6 1.000000e+00 2.633048e-12 #> 77   8   7 1.000000e+00 8.956387e-22 #> 78   8   8 9.995915e-01 4.085071e-04 #> 79   8   9 9.769377e-01 2.306228e-02 #> 80   8  10 4.055465e-01 5.944535e-01 #> 81   9   1 9.730071e-01 2.699289e-02 #> 82   9   2 8.657898e-01 1.342102e-01 #> 83   9   3 9.984530e-01 1.547004e-03 #> 84   9   4 9.998054e-01 1.945988e-04 #> 85   9   5 5.976460e-01 4.023540e-01 #> 86   9   6 1.000000e+00 1.559494e-08 #> 87   9   7 1.000000e+00 2.988606e-08 #> 88   9   8 1.000000e+00 5.580065e-09 #> 89   9   9 9.997445e-01 2.554936e-04 #> 90   9  10 9.975366e-01 2.463447e-03 #> 91  10   1 1.000000e+00 1.306351e-11 #> 92  10   2 9.999951e-01 4.878660e-06 #> 93  10   3 6.670097e-03 9.933299e-01 #> 94  10   4 9.903565e-01 9.643453e-03 #> 95  10   5 9.996045e-01 3.954868e-04 #> 96  10   6 9.999682e-01 3.175809e-05 #> 97  10   7 9.772253e-01 2.277474e-02 #> 98  10   8 2.384261e-04 9.997616e-01 #> 99  10   9 9.885641e-01 1.143590e-02 #> 100 10  10 1.000000e+00 1.625220e-13"},{"path":"https://loelschlaeger.de/RprobitB/reference/coef.RprobitB_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Linear coefficients — coef.RprobitB_fit","title":"Linear coefficients — coef.RprobitB_fit","text":"function returns estimated linear coefficients.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/coef.RprobitB_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Linear coefficients — coef.RprobitB_fit","text":"","code":"# S3 method for RprobitB_fit coef(object, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/coef.RprobitB_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Linear coefficients — coef.RprobitB_fit","text":"object object class RprobitB_fit. ... Ignored.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/coef.RprobitB_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Linear coefficients — coef.RprobitB_fit","text":"object class RprobitB_coef.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_choice_probabilities.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute probit choice probabilities for a single choice situation. — compute_choice_probabilities","title":"Compute probit choice probabilities for a single choice situation. — compute_choice_probabilities","text":"function computes probit choice probabilities single choice situation J alternatives.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_choice_probabilities.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute probit choice probabilities for a single choice situation. — compute_choice_probabilities","text":"","code":"compute_choice_probabilities(X, alternatives, parameter)"},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_choice_probabilities.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute probit choice probabilities for a single choice situation. — compute_choice_probabilities","text":"X matrix covariates J rows P_f + P_r columns, first P_f columns connected fixed coefficients last P_r columns connected random coefficients. alternatives vector unique integers 1 J, indicating alternatives choice probabilities computed. parameter object class RprobitB_parameter.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_choice_probabilities.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute probit choice probabilities for a single choice situation. — compute_choice_probabilities","text":"probability vector length length(alternatives).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_p_si.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute probability for each observed choice at posterior samples — compute_p_si","title":"Compute probability for each observed choice at posterior samples — compute_p_si","text":"function computes probability observed choice (normalized, burned thinned) samples posterior. probabilities required compute WAIC marginal model likelihood mml.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_p_si.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute probability for each observed choice at posterior samples — compute_p_si","text":"","code":"compute_p_si(x, ncores = parallel::detectCores() - 1, recompute = FALSE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_p_si.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute probability for each observed choice at posterior samples — compute_p_si","text":"x object class RprobitB_fit. ncores function parallelized, set number cores . recompute Set TRUE recompute probabilities.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_p_si.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute probability for each observed choice at posterior samples — compute_p_si","text":"object x, including object p_si, matrix probabilities, observations rows posterior samples columns.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/compute_p_si.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute probability for each observed choice at posterior samples — compute_p_si","text":"","code":"if (FALSE) { ### takes ~5 min computation time data(\"model_train\", package = \"RprobitB\") model_train <- compute_p_si(model_train, ncores = 1, recompute = TRUE) }"},{"path":"https://loelschlaeger.de/RprobitB/reference/cov_mix.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimated covariance matrix of the mixing distribution — cov_mix","title":"Estimated covariance matrix of the mixing distribution — cov_mix","text":"convenience function returns estimated covariance matrix mixing distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/cov_mix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimated covariance matrix of the mixing distribution — cov_mix","text":"","code":"cov_mix(x, cor = FALSE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/cov_mix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimated covariance matrix of the mixing distribution — cov_mix","text":"x object class RprobitB_fit. cor TRUE, returns correlation matrix instead.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/cov_mix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimated covariance matrix of the mixing distribution — cov_mix","text":"estimated covariance matrix mixing distribution. case multiple classes, list matrices class.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Omega.html","id":null,"dir":"Reference","previous_headings":"","what":"Create labels for Omega — create_labels_Omega","title":"Create labels for Omega — create_labels_Omega","text":"function creates labels model parameter Omega.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Omega.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create labels for Omega — create_labels_Omega","text":"","code":"create_labels_Omega(P_r, C, cov_sym)"},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Omega.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create labels for Omega — create_labels_Omega","text":"P_r number covariates connected random coefficient (can 0). cov_sym Set TRUE labels symmetric covariance elements.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Omega.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create labels for Omega — create_labels_Omega","text":"vector labels model parameter Omega length P_r^2 * C P_r > 0 cov_sym = TRUEor length P_r*(P_r+1)/2*C cov_sym = FALSE NULLotherwise.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Omega.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create labels for Omega — create_labels_Omega","text":"labels form \"c.p1,p2\", c latent class number p1,p2 indeces two random coefficients.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Omega.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create labels for Omega — create_labels_Omega","text":"","code":"RprobitB:::create_labels_Omega(2, 3, cov_sym = TRUE) #>  [1] \"1.1,1\" \"1.1,2\" \"1.2,1\" \"1.2,2\" \"2.1,1\" \"2.1,2\" \"2.2,1\" \"2.2,2\" \"3.1,1\" #> [10] \"3.1,2\" \"3.2,1\" \"3.2,2\" RprobitB:::create_labels_Omega(2, 3, cov_sym = FALSE) #> [1] \"1.1,1\" \"1.1,2\" \"1.2,2\" \"2.1,1\" \"2.1,2\" \"2.2,2\" \"3.1,1\" \"3.1,2\" \"3.2,2\""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Sigma.html","id":null,"dir":"Reference","previous_headings":"","what":"Create labels for Sigma — create_labels_Sigma","title":"Create labels for Sigma — create_labels_Sigma","text":"function creates labels model parameter Sigma.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Sigma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create labels for Sigma — create_labels_Sigma","text":"","code":"create_labels_Sigma(J, cov_sym)"},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Sigma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create labels for Sigma — create_labels_Sigma","text":"J number (greater equal 2) choice alternatives. cov_sym Set TRUE labels symmetric covariance elements.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Sigma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create labels for Sigma — create_labels_Sigma","text":"vector labels model parameter Sigma length (J-1)^2 cov_sym = TRUE length P_r*(P_r+1)/2*Cif cov_sym = FALSE.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Sigma.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create labels for Sigma — create_labels_Sigma","text":"labels form \"j1,j2\", j1,j2 indices two alternatives.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_Sigma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create labels for Sigma — create_labels_Sigma","text":"","code":"RprobitB:::create_labels_Sigma(3, cov_sym = TRUE) #> [1] \"1,1\" \"1,2\" \"2,1\" \"2,2\" RprobitB:::create_labels_Sigma(4, cov_sym = FALSE) #> [1] \"1,1\" \"1,2\" \"1,3\" \"2,2\" \"2,3\" \"3,3\""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_alpha.html","id":null,"dir":"Reference","previous_headings":"","what":"Create labels for alpha — create_labels_alpha","title":"Create labels for alpha — create_labels_alpha","text":"function creates labels model parameter alpha.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_alpha.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create labels for alpha — create_labels_alpha","text":"","code":"create_labels_alpha(P_f)"},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_alpha.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create labels for alpha — create_labels_alpha","text":"P_f number covariates connected fixed coefficient (can 0).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_alpha.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create labels for alpha — create_labels_alpha","text":"vector labels model parameter alpha length P_fif P_f > 0 NULL otherwise.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_alpha.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create labels for alpha — create_labels_alpha","text":"","code":"RprobitB:::create_labels_alpha(P_f = 3) #> [1] \"1\" \"2\" \"3\""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_b.html","id":null,"dir":"Reference","previous_headings":"","what":"Create labels for b — create_labels_b","title":"Create labels for b — create_labels_b","text":"function creates labels model parameter b.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_b.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create labels for b — create_labels_b","text":"","code":"create_labels_b(P_r, C)"},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_b.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create labels for b — create_labels_b","text":"P_r number covariates connected random coefficient (can 0).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_b.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create labels for b — create_labels_b","text":"vector labels model parameter b length P_r * Cif P_r > 0 NULL otherwise.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_b.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create labels for b — create_labels_b","text":"labels form \"c.p\", c latent class number p index random coefficient.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_b.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create labels for b — create_labels_b","text":"","code":"RprobitB:::create_labels_b(2,3) #> [1] \"1.1\" \"1.2\" \"2.1\" \"2.2\" \"3.1\" \"3.2\""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_s.html","id":null,"dir":"Reference","previous_headings":"","what":"Create labels for s — create_labels_s","title":"Create labels for s — create_labels_s","text":"function creates labels model parameter s.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_s.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create labels for s — create_labels_s","text":"","code":"create_labels_s(P_r, C)"},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_s.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create labels for s — create_labels_s","text":"P_r number covariates connected random coefficient (can 0).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_s.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create labels for s — create_labels_s","text":"vector labels model parameter s length C P_r > 0 NULL otherwise.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_labels_s.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create labels for s — create_labels_s","text":"","code":"RprobitB:::create_labels_s(1,3) #> [1] \"1\" \"2\" \"3\""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_lagged_cov.html","id":null,"dir":"Reference","previous_headings":"","what":"Create lagged choice covariates — create_lagged_cov","title":"Create lagged choice covariates — create_lagged_cov","text":"convenience function creates lagged choice covariates data frame choice_data, assumed sorted choice occasions: First choice occasions top. function vectorized column k.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_lagged_cov.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create lagged choice covariates — create_lagged_cov","text":"","code":"create_lagged_cov(choice_data, column, k = 1, id = \"id\")"},{"path":"https://loelschlaeger.de/RprobitB/reference/create_lagged_cov.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create lagged choice covariates — create_lagged_cov","text":"choice_data data frame choice data wide format, .e. row represents one choice occasion. column character, column name choice_data, .e. covariate name. Can vector. k positive number, number lags (units observations), see details. Can vector. default k = 1. id character, name column choice_data contains unique identifier decision maker. default \"id\".","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_lagged_cov.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create lagged choice covariates — create_lagged_cov","text":"input data frame choice_data additional columns named column.k element column number kcontaining lagged covariates.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/create_lagged_cov.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create lagged choice covariates — create_lagged_cov","text":"Say choice_data contains column column. , function call returns input choice_data includes new column named column.k. column contains decider (based id) choice occasion covariate faced k choice occasions. data point available, set NA. particular, first k values column.k NA (initial condition problem).","code":"create_lagged_cov(choice_data, column, k, id)"},{"path":"https://loelschlaeger.de/RprobitB/reference/create_lagged_cov.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create lagged choice covariates — create_lagged_cov","text":"","code":"### add covariate 'lost' and 'berserk' from previous choice occasion if (FALSE) { choice_data <- create_lagged_cov(   choice_data = choice_berserk,   column = c(\"lost\", \"berserk\"),   k = 1,   id = \"player_id\" ) }"},{"path":"https://loelschlaeger.de/RprobitB/reference/delta.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix difference operator — delta","title":"Matrix difference operator — delta","text":"function creates difference operator matrix delta subtracting matrix row matrix rows.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/delta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix difference operator — delta","text":"","code":"delta(J, i)"},{"path":"https://loelschlaeger.de/RprobitB/reference/delta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix difference operator — delta","text":"J number matrix rows. row number respect differences computed.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/delta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix difference operator — delta","text":"matrix J-1 rows.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/delta.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Matrix difference operator — delta","text":"Given matrix x J rows, delta(,J) %*% x computes differences respect row .","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/delta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Matrix difference operator — delta","text":"","code":"J <- 2 x <- matrix(1, nrow = J, ncol = 2) RprobitB:::delta(J, 1) %*% x #>      [,1] [,2] #> [1,]    0    0"},{"path":"https://loelschlaeger.de/RprobitB/reference/dmvnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Density of multivariate normal distribution — dmvnorm","title":"Density of multivariate normal distribution — dmvnorm","text":"function computes density multivariate normal distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/dmvnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Density of multivariate normal distribution — dmvnorm","text":"","code":"dmvnorm(x, mean, Sigma, log = FALSE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/dmvnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Density of multivariate normal distribution — dmvnorm","text":"x quantile vector length n. mean mean vector length n. Sigma covariance matrix dimension n x n. log boolean, TRUE logarithm density value returned.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/dmvnorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Density of multivariate normal distribution — dmvnorm","text":"density value.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/dmvnorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Density of multivariate normal distribution — dmvnorm","text":"","code":"x = c(0,0) mean = c(0,0) Sigma = diag(2) dmvnorm(x = x, mean = mean, Sigma = Sigma) #> [1] 0.1591549 dmvnorm(x = x, mean = mean, Sigma = Sigma, log = TRUE) #> [1] -1.837877"},{"path":"https://loelschlaeger.de/RprobitB/reference/draw_from_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from prior distributions — draw_from_prior","title":"Sample from prior distributions — draw_from_prior","text":"function returns sample parameter's prior distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/draw_from_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from prior distributions — draw_from_prior","text":"","code":"draw_from_prior(prior, C = 1)"},{"path":"https://loelschlaeger.de/RprobitB/reference/draw_from_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample from prior distributions — draw_from_prior","text":"prior object class RprobitB_prior, output check_prior. C number latent classes.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/draw_from_prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample from prior distributions — draw_from_prior","text":"list draws alpha, s, b, Omega, Sigma (specified model).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/draw_from_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from prior distributions — draw_from_prior","text":"","code":"prior <- check_prior(P_f = 1, P_r = 2, J = 3) RprobitB:::draw_from_prior(prior, C = 2) #> $alpha #>            [,1] #> [1,] -0.5736237 #>  #> $s #> [1] 0.7925544 0.2074456 #>  #> $b #>           [,1]        [,2] #> [1,] -1.186368  0.82852193 #> [2,]  1.924412 -0.05230646 #>  #> $Omega #>           [,1]      [,2] #> [1,] 0.3508545 0.2102035 #> [2,] 0.1090167 0.3179140 #> [3,] 0.1090167 0.3179140 #> [4,] 0.6737812 1.7453458 #>  #> $Sigma #>           [,1]      [,2] #> [1,] 1.0893445 0.2177464 #> [2,] 0.2177464 0.2412615 #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/euc_dist.html","id":null,"dir":"Reference","previous_headings":"","what":"Euclidean distance — euc_dist","title":"Euclidean distance — euc_dist","text":"function computes euclidean distance two vectors.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/euc_dist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Euclidean distance — euc_dist","text":"","code":"euc_dist(a, b)"},{"path":"https://loelschlaeger.de/RprobitB/reference/euc_dist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Euclidean distance — euc_dist","text":"numeric vector. b Another numeric vector length .","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/euc_dist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Euclidean distance — euc_dist","text":"euclidean distance.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/filter_gibbs_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter Gibbs samples — filter_gibbs_samples","title":"Filter Gibbs samples — filter_gibbs_samples","text":"helper function filters Gibbs samples.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/filter_gibbs_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter Gibbs samples — filter_gibbs_samples","text":"","code":"filter_gibbs_samples(   x,   P_f,   P_r,   J,   C,   cov_sym,   keep_par = c(\"s\", \"alpha\", \"b\", \"Omega\", \"Sigma\"),   drop_par = NULL )"},{"path":"https://loelschlaeger.de/RprobitB/reference/filter_gibbs_samples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter Gibbs samples — filter_gibbs_samples","text":"x object class RprobitB_gibbs_samples. P_f number covariates connected fixed coefficient (can 0). P_r number covariates connected random coefficient (can 0). J number (greater equal 2) choice alternatives. cov_sym Set TRUE labels symmetric covariance elements. keep_par vector parameter names kept. drop_par vector parameter names get dropped.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/filter_gibbs_samples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter Gibbs samples — filter_gibbs_samples","text":"object class RprobitB_gibbs_samples filtered labels parameter_labels(P_f, P_r, J, C, cov_sym, keep_par, drop_par).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/get_cov.html","id":null,"dir":"Reference","previous_headings":"","what":"Get covariates of choice situation — get_cov","title":"Get covariates of choice situation — get_cov","text":"convenience function returns covariates choices specific choice occasions.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/get_cov.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get covariates of choice situation — get_cov","text":"","code":"get_cov(x, id, idc, idc_label)"},{"path":"https://loelschlaeger.de/RprobitB/reference/get_cov.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get covariates of choice situation — get_cov","text":"x Either object class RprobitB_data RprobitB_fit. id numeric (vector), specifies decider(s). idc numeric (vector), specifies choice occasion(s). idc_label name column contains choice occasion identifier.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/get_cov.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get covariates of choice situation — get_cov","text":"subset choice_data data frame specified prepare_data().","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/get_cov.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get covariates of choice situation — get_cov","text":"","code":"data(\"model_train\", package = \"RprobitB\") get_cov(model_train, id = 1:2, idc = 1:2, idc_label = \"choiceid\") #>    id choiceid choice  price_A time_A change_A comfort_A  price_B   time_B #> 1   1        1      A 52.88904   2.50        0         1 88.14840 2.500000 #> 2   1        2      A 52.88904   2.50        0         1 70.51872 2.166667 #> 11  2        1      A 65.56037   1.80        0         0 77.12985 2.016667 #> 12  2        2      A 65.56037   1.55        0         1 77.12985 1.550000 #>    change_B comfort_B #> 1         0         1 #> 2         0         1 #> 11        0         1 #> 12        0         0"},{"path":"https://loelschlaeger.de/RprobitB/reference/gibbs_sampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Gibbs sampler for the (mixed) multinomial probit model — gibbs_sampling","title":"Gibbs sampler for the (mixed) multinomial probit model — gibbs_sampling","text":"function draws Gibbs samples posterior distribution (mixed) multinomial probit model parameters.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/gibbs_sampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gibbs sampler for the (mixed) multinomial probit model — gibbs_sampling","text":"","code":"gibbs_sampling(   sufficient_statistics,   prior,   latent_classes,   init,   R,   B,   print_progress )"},{"path":"https://loelschlaeger.de/RprobitB/reference/gibbs_sampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Gibbs sampler for the (mixed) multinomial probit model — gibbs_sampling","text":"sufficient_statistics output sufficient_statistics. prior named list parameters prior distributions. See documentation check_prior details parameters can specified. latent_classes Either NULL (latent classes) list parameters specifying number latent classes updating scheme: C: fixed number (greater equal 1) latent classes, set 1 per default. either weight_update = TRUE dp_update = TRUE (.e. classes updated), C equals initial number latent classes. weight_update: boolean, set TRUE weight-based update latent classes. See ... details. dp_update: boolean, set TRUE update latent classes based Dirichlet process. See ... details. Cmax: maximum number latent classes. buffer: number iterations wait next weight-based update latent classes. epsmin: threshold weight (0 1) removing latent class weight-based updating scheme. epsmax: threshold weight (0 1) splitting latent class weight-based updating scheme. distmin: (non-negative) threshold difference class means joining two latent classes weight-based updating scheme. init output set_initial_gibbs_values. R number iterations Gibbs sampler. B length burn-period, .e. non-negative number samples discarded. print_progress boolean, determining whether print Gibbs sampler progress estimated remaining computation time.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/gibbs_sampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Gibbs sampler for the (mixed) multinomial probit model — gibbs_sampling","text":"list Gibbs samples Sigma, alpha (P_f>0), s, z, b, Omega (P_r>0), vector class_sequence length R, rth entry number latent classes iteration r.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/gibbs_sampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Gibbs sampler for the (mixed) multinomial probit model — gibbs_sampling","text":"function supposed called directly, rather via mcmc.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/is_covariance_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Check covariance matrix properties — is_covariance_matrix","title":"Check covariance matrix properties — is_covariance_matrix","text":"function checks input proper covariance matrix, .e. symmetric, numeric matrix non-negative eigenvalues.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/is_covariance_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check covariance matrix properties — is_covariance_matrix","text":"","code":"is_covariance_matrix(x)"},{"path":"https://loelschlaeger.de/RprobitB/reference/is_covariance_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check covariance matrix properties — is_covariance_matrix","text":"x matrix.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/is_covariance_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check covariance matrix properties — is_covariance_matrix","text":"boolean, TRUE x proper covariance matrix.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/is_covariance_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check covariance matrix properties — is_covariance_matrix","text":"","code":"x <- diag(2) RprobitB:::is_covariance_matrix(x) #> [1] TRUE"},{"path":"https://loelschlaeger.de/RprobitB/reference/logLik.html","id":null,"dir":"Reference","previous_headings":"","what":"Log-likelihood value — logLik","title":"Log-likelihood value — logLik","text":"function computes log-likelihood value RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/logLik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Log-likelihood value — logLik","text":"","code":"logLik(object, par_set, recompute, ...)  # S3 method for RprobitB_fit logLik(object, par_set = mean, recompute = FALSE, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/logLik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Log-likelihood value — logLik","text":"object object class RprobitB_fit. par_set Specifying parameter set calculation either function computes posterior point estimate (default mean()), \"true\" select true parameter set, object class RprobitB_parameter. recompute Set TRUE recompute log-likelihood value already saved object. ... Ignored.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/logLik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Log-likelihood value — logLik","text":"numeric.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/logLik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Log-likelihood value — logLik","text":"","code":"data(\"model_train\", package = \"RprobitB\") logLik(model_train) #> [1] -1727.742"},{"path":"https://loelschlaeger.de/RprobitB/reference/mcmc.html","id":null,"dir":"Reference","previous_headings":"","what":"Probit model fitting via Markov chain Monte Carlo simulation — mcmc","title":"Probit model fitting via Markov chain Monte Carlo simulation — mcmc","text":"function performs Markov chain Monte Carlo simulation fitting (latent class) (mixed) (multinomial) probit model discrete choice data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/mcmc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probit model fitting via Markov chain Monte Carlo simulation — mcmc","text":"","code":"mcmc(   data,   scale = list(parameter = \"s\", index = 1, value = 1),   R = 10000,   B = R/2,   Q = 1,   print_progress = getOption(\"RprobitB_progress\"),   prior = NULL,   latent_classes = NULL,   seed = NULL )"},{"path":"https://loelschlaeger.de/RprobitB/reference/mcmc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probit model fitting via Markov chain Monte Carlo simulation — mcmc","text":"data object class RprobitB_data. scale named list three elements, determining parameter normalization respect utility scale: parameter: Either \"\" (linear coefficient \"alpha\") \"s\" (variance error-term covariance matrix \"Sigma\"). index: index parameter gets fixed. value: value fixed parameter. R number iterations Gibbs sampler. B length burn-period, .e. non-negative number samples discarded. Q thinning factor Gibbs samples, .e. every Qth sample kept. print_progress boolean, determining whether print Gibbs sampler progress estimated remaining computation time. prior named list parameters prior distributions. See documentation check_prior details parameters can specified. latent_classes Either NULL (latent classes) list parameters specifying number latent classes updating scheme: C: fixed number (greater equal 1) latent classes, set 1 per default. either weight_update = TRUE dp_update = TRUE (.e. classes updated), C equals initial number latent classes. weight_update: boolean, set TRUE weight-based update latent classes. See ... details. dp_update: boolean, set TRUE update latent classes based Dirichlet process. See ... details. Cmax: maximum number latent classes. buffer: number iterations wait next weight-based update latent classes. epsmin: threshold weight (0 1) removing latent class weight-based updating scheme. epsmax: threshold weight (0 1) splitting latent class weight-based updating scheme. distmin: (non-negative) threshold difference class means joining two latent classes weight-based updating scheme. seed Set seed Gibbs sampling.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/mcmc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probit model fitting via Markov chain Monte Carlo simulation — mcmc","text":"object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/mcmc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Probit model fitting via Markov chain Monte Carlo simulation — mcmc","text":"See vignette model fitting details.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/reference/mcmc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probit model fitting via Markov chain Monte Carlo simulation — mcmc","text":"","code":"data <- simulate_choices(   form = choice ~ var | 0, N = 100, T = 10, J = 3, seed = 1 ) mod <- mcmc(data = data, R = 1000, seed = 1) #> Computing sufficient statistics 0 of 4  #> Computing sufficient statistics 1 of 4  #> Computing sufficient statistics 2 of 4  #> Computing sufficient statistics 3 of 4  #> Computing sufficient statistics 4 of 4  #> Gibbs sampler iteration 1 of 1000  #> Gibbs sampler iteration 10 of 1000  #> Gibbs sampler iteration 20 of 1000  #> Gibbs sampler iteration 30 of 1000  #> Gibbs sampler iteration 40 of 1000  #> Gibbs sampler iteration 50 of 1000  #> Gibbs sampler iteration 60 of 1000  #> Gibbs sampler iteration 70 of 1000  #> Gibbs sampler iteration 80 of 1000  #> Gibbs sampler iteration 90 of 1000  #> Gibbs sampler iteration 100 of 1000  #> Gibbs sampler iteration 110 of 1000  #> Gibbs sampler iteration 120 of 1000  #> Gibbs sampler iteration 130 of 1000  #> Gibbs sampler iteration 140 of 1000  #> Gibbs sampler iteration 150 of 1000  #> Gibbs sampler iteration 160 of 1000  #> Gibbs sampler iteration 170 of 1000  #> Gibbs sampler iteration 180 of 1000  #> Gibbs sampler iteration 190 of 1000  #> Gibbs sampler iteration 200 of 1000  #> Gibbs sampler iteration 210 of 1000  #> Gibbs sampler iteration 220 of 1000  #> Gibbs sampler iteration 230 of 1000  #> Gibbs sampler iteration 240 of 1000  #> Gibbs sampler iteration 250 of 1000  #> Gibbs sampler iteration 260 of 1000  #> Gibbs sampler iteration 270 of 1000  #> Gibbs sampler iteration 280 of 1000  #> Gibbs sampler iteration 290 of 1000  #> Gibbs sampler iteration 300 of 1000  #> Gibbs sampler iteration 310 of 1000  #> Gibbs sampler iteration 320 of 1000  #> Gibbs sampler iteration 330 of 1000  #> Gibbs sampler iteration 340 of 1000  #> Gibbs sampler iteration 350 of 1000  #> Gibbs sampler iteration 360 of 1000  #> Gibbs sampler iteration 370 of 1000  #> Gibbs sampler iteration 380 of 1000  #> Gibbs sampler iteration 390 of 1000  #> Gibbs sampler iteration 400 of 1000  #> Gibbs sampler iteration 410 of 1000  #> Gibbs sampler iteration 420 of 1000  #> Gibbs sampler iteration 430 of 1000  #> Gibbs sampler iteration 440 of 1000  #> Gibbs sampler iteration 450 of 1000  #> Gibbs sampler iteration 460 of 1000  #> Gibbs sampler iteration 470 of 1000  #> Gibbs sampler iteration 480 of 1000  #> Gibbs sampler iteration 490 of 1000  #> Gibbs sampler iteration 500 of 1000  #> Gibbs sampler iteration 510 of 1000  #> Gibbs sampler iteration 520 of 1000  #> Gibbs sampler iteration 530 of 1000  #> Gibbs sampler iteration 540 of 1000  #> Gibbs sampler iteration 550 of 1000  #> Gibbs sampler iteration 560 of 1000  #> Gibbs sampler iteration 570 of 1000  #> Gibbs sampler iteration 580 of 1000  #> Gibbs sampler iteration 590 of 1000  #> Gibbs sampler iteration 600 of 1000  #> Gibbs sampler iteration 610 of 1000  #> Gibbs sampler iteration 620 of 1000  #> Gibbs sampler iteration 630 of 1000  #> Gibbs sampler iteration 640 of 1000  #> Gibbs sampler iteration 650 of 1000  #> Gibbs sampler iteration 660 of 1000  #> Gibbs sampler iteration 670 of 1000  #> Gibbs sampler iteration 680 of 1000  #> Gibbs sampler iteration 690 of 1000  #> Gibbs sampler iteration 700 of 1000  #> Gibbs sampler iteration 710 of 1000  #> Gibbs sampler iteration 720 of 1000  #> Gibbs sampler iteration 730 of 1000  #> Gibbs sampler iteration 740 of 1000  #> Gibbs sampler iteration 750 of 1000  #> Gibbs sampler iteration 760 of 1000  #> Gibbs sampler iteration 770 of 1000  #> Gibbs sampler iteration 780 of 1000  #> Gibbs sampler iteration 790 of 1000  #> Gibbs sampler iteration 800 of 1000  #> Gibbs sampler iteration 810 of 1000  #> Gibbs sampler iteration 820 of 1000  #> Gibbs sampler iteration 830 of 1000  #> Gibbs sampler iteration 840 of 1000  #> Gibbs sampler iteration 850 of 1000  #> Gibbs sampler iteration 860 of 1000  #> Gibbs sampler iteration 870 of 1000  #> Gibbs sampler iteration 880 of 1000  #> Gibbs sampler iteration 890 of 1000  #> Gibbs sampler iteration 900 of 1000  #> Gibbs sampler iteration 910 of 1000  #> Gibbs sampler iteration 920 of 1000  #> Gibbs sampler iteration 930 of 1000  #> Gibbs sampler iteration 940 of 1000  #> Gibbs sampler iteration 950 of 1000  #> Gibbs sampler iteration 960 of 1000  #> Gibbs sampler iteration 970 of 1000  #> Gibbs sampler iteration 980 of 1000  #> Gibbs sampler iteration 990 of 1000  #> Gibbs sampler iteration 1000 of 1000  summary(mod) #> Probit model #> choice ~ var | 0  #> R: 1000  #> B: 500  #> Q: 1  #>  #> Normalization #> Level: Utility differences with respect to alternative 3. #> Scale: Coefficient of the 1. error term variance in Sigma fixed to 1. #>  #> Gibbs sample statistics #>           true    mean      sd      R^ #>  alpha #>                                        #>      1   -0.94   -0.85    0.06    1.16 #>  #>  Sigma #>                                        #>    1,1    1.00    1.00    0.00    1.00 #>    1,2   -0.42   -0.30    0.06    2.15 #>    2,2    0.27    0.19    0.04    1.62"},{"path":"https://loelschlaeger.de/RprobitB/reference/missing_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Handle missing choice data — missing_data","title":"Handle missing choice data — missing_data","text":"function checks replaces missing entries choice_data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/missing_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Handle missing choice data — missing_data","text":"","code":"missing_data(   choice_data,   impute = \"complete_cases\",   as_missing = c(NA, NaN, -Inf, Inf) )"},{"path":"https://loelschlaeger.de/RprobitB/reference/missing_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Handle missing choice data — missing_data","text":"choice_data data frame choice data wide format, .e. row represents one choice occasion. impute character specifies handle missing entries (elements ) as_missing) choice_data, one : \"complete_cases\", removes rows containing missing entries (default), \"zero_out\", replaces missing entries zero (numeric columns), \"mean\", imputes missing entries covariate mean (numeric columns). as_missing vector elements interpreted missing data entries, default as_missing = c(NA, NaN, -Inf, Inf).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/missing_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Handle missing choice data — missing_data","text":"input choice_data, missing entries addressed.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/missing_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Handle missing choice data — missing_data","text":"","code":"choice_data <- data.frame(\"A\" = c(1,NA,3), \"B\" = c(1,2,Inf)) missing_data(choice_data, \"complete_cases\") #>   A B #> 1 1 1 missing_data(choice_data, \"zero_out\") #>   A B #> 1 1 1 #> 2 0 2 #> 3 3 0 missing_data(choice_data, \"mean\") #>   A   B #> 1 1 1.0 #> 2 2 2.0 #> 3 3 1.5"},{"path":"https://loelschlaeger.de/RprobitB/reference/mml.html","id":null,"dir":"Reference","previous_headings":"","what":"Marginal model likelihood — mml","title":"Marginal model likelihood — mml","text":"function approximates model's marginal likelihood.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/mml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Marginal model likelihood — mml","text":"","code":"mml(x, S = 0, ncores = parallel::detectCores() - 1, recompute = FALSE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/mml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Marginal model likelihood — mml","text":"x object class RprobitB_fit. S number prior samples prior arithmetic mean estimate. Per default, S = 0. case, posterior samples used approximation via posterior harmonic mean estimator, see details section. ncores Computation prior arithmetic mean estimate parallelized, set number cores. recompute Set TRUE recompute likelihood.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/mml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Marginal model likelihood — mml","text":"object x, including object mml, model's approximated marginal likelihood value.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/mml.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Marginal model likelihood — mml","text":"model's marginal likelihood \\(p(y\\mid M)\\) model \\(M\\) data \\(y\\) required computation Bayes factors. general, term closed form must approximated numerically. function uses posterior Gibbs samples approximate model's marginal likelihood via posterior harmonic mean estimator: Let S denote numbers posterior samples \\(\\theta_1,\\dots,\\theta_S\\). , $$p(y\\mid M) = \\left(\\mathbb{E}_\\text{posterior} p(y\\mid \\theta,M)^{-1} \\right)^{-1} \\approx \\left( \\frac{1}{S} \\sum_s 1/p(y\\mid \\theta_s,M) \\right) ^{-1} = \\tilde{p}(y\\mid M).$$ law large numbers, \\(\\tilde{p}(y\\mid M) \\p(y\\mid M)\\) almost surely \\(S \\\\infty\\). check convergence, call plot(x$mml), x output function. estimation seem converged, can improve approximation combining value prior arithmetic mean estimator: approach, S samples \\(\\theta_1,\\dots,\\theta_S\\) drawn model's prior distribution. , $$p(y\\mid M) = \\mathbb{E}_\\text{prior} p(y\\mid \\theta,M) \\approx \\frac{1}{S} \\sum_s p(y\\mid \\theta_s,M) = \\tilde{p}(y\\mid M).$$ , hols law large numbers, \\(\\tilde{p}(y\\mid M) \\p(y\\mid M)\\) almost surely \\(S \\\\infty\\). final approximation model's marginal likelihood weighted sum posterior harmonic mean estimate prior arithmetic mean estimate, weights determined sample sizes.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/mml.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Marginal model likelihood — mml","text":"","code":"data(\"model_train\", package = \"RprobitB\") model_train <- mml(model_train, recompute = TRUE) model_train$mml #> 3.54e-117 * exp(-1464) print(model_train$mml, log = TRUE) #> -1732.138"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_elec.html","id":null,"dir":"Reference","previous_headings":"","what":"Mixed probit model for multivariate choice between electricity suppliers — model_elec","title":"Mixed probit model for multivariate choice between electricity suppliers — model_elec","text":"object fitted mixed probit model Electricity dataset mlogit package model formula choice ~ pf + cl + loc + wk + tod + seas | 0.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_elec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mixed probit model for multivariate choice between electricity suppliers — model_elec","text":"","code":"data(model_elec)"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_elec.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Mixed probit model for multivariate choice between electricity suppliers — model_elec","text":"object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_elec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Mixed probit model for multivariate choice between electricity suppliers — model_elec","text":"model derived via","code":"data(\"Electricity\", package = \"mlogit\") Electricity <- as_cov_names(Electricity, c(\"pf\",\"cl\",\"loc\",\"wk\",\"tod\",\"seas\"), 1:4) data <- prepare_data(   form = choice ~ pf + cl + loc + wk + tod + seas | 0,   choice_data = Electricity,   re = c(\"cl\",\"loc\",\"wk\",\"tod\",\"seas\") ) model <- mcmc(   data = data,   R = 5000,   Q = 10,   scale = list(parameter = \"a\", index = 1, value = -1))"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_selection.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare fitted models — model_selection","title":"Compare fitted models — model_selection","text":"function returns table several criteria model comparison.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_selection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare fitted models — model_selection","text":"","code":"model_selection(   ...,   criteria = c(\"npar\", \"LL\", \"AIC\", \"BIC\"),   add_form = FALSE )"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_selection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare fitted models — model_selection","text":"... One objects class RprobitB_fit. criteria vector one following characters: \"npar\" number model parameters (see npar), \"LL\" log-likelihood value (see logLik), \"AIC\" AIC value (see AIC), \"BIC\" BIC value (see BIC), \"WAIC\" WAIC value (also shows standard error sd(WAIC) number pWAIC effective model parameters, see WAIC), \"MMLL\" marginal model log-likelihood, \"BF\" Bayes factor, \"pred_acc\" prediction accuracy (see pred_acc). add_form Set TRUE add model formulas.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_selection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare fitted models — model_selection","text":"data frame, criteria columns, models rows.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_selection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compare fitted models — model_selection","text":"See vignette model selection details.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_selection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare fitted models — model_selection","text":"","code":"data(\"model_train\", package = \"RprobitB\") data(\"model_train_sparse\", package = \"RprobitB\") criteria <- c(\"npar\", \"LL\", \"AIC\", \"BIC\", \"WAIC\", \"MMLL\", \"BF\", \"pred_acc\") model_selection(model_train, model_train_sparse, criteria = criteria) #>                          model_train model_train_sparse #> npar                               4                  1 #> LL                          -1727.74           -1865.86 #> AIC                          3463.48            3733.72 #> BIC                          3487.41            3739.70 #> WAIC                         3463.76            3733.91 #> se(WAIC)                        0.18               0.07 #> pWAIC                           4.32               1.15 #> MMLL                        -1732.14           -1867.48 #> BF(*,model_train)                  1             < 0.01 #> BF(*,model_train_sparse)       > 100                  1 #> pred_acc                      69.55%             63.37%"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Probit model for binary choice between Train trip alternatives — model_train","title":"Probit model for binary choice between Train trip alternatives — model_train","text":"object fitted probit model Train data set mlogit package model formula choice ~ price + time + change + comfort | 0.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probit model for binary choice between Train trip alternatives — model_train","text":"","code":"data(model_train)"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Probit model for binary choice between Train trip alternatives — model_train","text":"object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Probit model for binary choice between Train trip alternatives — model_train","text":"model derived via","code":"data(\"Train\", package = \"mlogit\") Train$price_A <- Train$price_A / 100 * 2.20371 Train$price_B <- Train$price_B / 100 * 2.20371 Train$time_A <- Train$time_A / 60 Train$time_B <- Train$time_B / 60 form <- choice ~ price + time + change + comfort | 0 data <- prepare_data(   form = form,   choice_data = Train,   idc = \"choiceid\"   ) model_train <- mcmc(   data = data,   R = 10000,   Q = 10,   scale = list(\"parameter\" = \"a\", index = 1, value = -1)   )"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train_sparse.html","id":null,"dir":"Reference","previous_headings":"","what":"Probit model for binary choice between Train trip alternatives with the price\nas the only explanatory variable — model_train_sparse","title":"Probit model for binary choice between Train trip alternatives with the price\nas the only explanatory variable — model_train_sparse","text":"object fitted probit model Train data set mlogit package model formula choice ~ price | 0.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train_sparse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probit model for binary choice between Train trip alternatives with the price\nas the only explanatory variable — model_train_sparse","text":"","code":"data(model_train_sparse)"},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train_sparse.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Probit model for binary choice between Train trip alternatives with the price\nas the only explanatory variable — model_train_sparse","text":"object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/model_train_sparse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Probit model for binary choice between Train trip alternatives with the price\nas the only explanatory variable — model_train_sparse","text":"model derived model_train object via","code":"data(\"model_train\", package = \"RprobitB\") nested_model(model_train, form = choice ~ price | 0)"},{"path":"https://loelschlaeger.de/RprobitB/reference/nested_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimating a nested model — nested_model","title":"Estimating a nested model — nested_model","text":"function wrapper prepare_data mcmc estimate nested probit model based given RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/nested_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimating a nested model — nested_model","text":"","code":"nested_model(   x,   form,   re,   alternatives,   id,   idc,   standardize,   impute,   scale,   R,   B,   Q,   print_progress,   prior,   latent_classes,   seed )"},{"path":"https://loelschlaeger.de/RprobitB/reference/nested_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimating a nested model — nested_model","text":"x object class RprobitB_fit. form formula object used specify probit model. structure choice ~ | B | C, names alternative choice situation specific covariates generic coefficient, B names choice situation specific covariates alternative specific coefficients, C names alternative choice situation specific covariates alternative specific coefficients. Separate multiple covariates one type + sign. default, alternative specific constants (ASCs) added model (except last alternative due identifiability). can removed adding +0 second spot. See vignette choice data details. re character (vector) covariates form random effects. re = NULL (default), random effects. random effects alternative specific constants, include \"ASC\" re. alternatives character vector names choice alternatives. specified, choice set defined observed choices. id character, name column choice_data contains unique identifier decision maker. default \"id\". idc character, name column choice_data contains unique identifier choice situation decision maker. default NULL, case identifier generated automatically. standardize character vector names covariates get standardized. Covariates type 1 3 addressed <covariate>_<alternative>. standardize = \"\", covariates get standardized. impute character specifies handle missing entries (elements ) as_missing) choice_data, one : \"complete_cases\", removes rows containing missing entries (default), \"zero_out\", replaces missing entries zero (numeric columns), \"mean\", imputes missing entries covariate mean (numeric columns). scale named list three elements, determining parameter normalization respect utility scale: parameter: Either \"\" (linear coefficient \"alpha\") \"s\" (variance error-term covariance matrix \"Sigma\"). index: index parameter gets fixed. value: value fixed parameter. R number iterations Gibbs sampler. B length burn-period, .e. non-negative number samples discarded. Q thinning factor Gibbs samples, .e. every Qth sample kept. print_progress boolean, determining whether print Gibbs sampler progress estimated remaining computation time. prior named list parameters prior distributions. See documentation check_prior details parameters can specified. latent_classes Either NULL (latent classes) list parameters specifying number latent classes updating scheme: C: fixed number (greater equal 1) latent classes, set 1 per default. either weight_update = TRUE dp_update = TRUE (.e. classes updated), C equals initial number latent classes. weight_update: boolean, set TRUE weight-based update latent classes. See ... details. dp_update: boolean, set TRUE update latent classes based Dirichlet process. See ... details. Cmax: maximum number latent classes. buffer: number iterations wait next weight-based update latent classes. epsmin: threshold weight (0 1) removing latent class weight-based updating scheme. epsmax: threshold weight (0 1) splitting latent class weight-based updating scheme. distmin: (non-negative) threshold difference class means joining two latent classes weight-based updating scheme. seed Set seed Gibbs sampling.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/nested_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimating a nested model — nested_model","text":"object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/nested_model.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimating a nested model — nested_model","text":"parameters (except x) optional specified retrieved specification x.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/nested_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimating a nested model — nested_model","text":"","code":"nested_model(model_train, form = choice ~ time, R = 100, B = 50) #> Computing sufficient statistics 0 of 4  #> Computing sufficient statistics 1 of 4  #> Computing sufficient statistics 2 of 4  #> Computing sufficient statistics 3 of 4  #> Computing sufficient statistics 4 of 4  #> Gibbs sampler iteration 1 of 100  #> Gibbs sampler iteration 10 of 100  #> Gibbs sampler iteration 20 of 100  #> Gibbs sampler iteration 30 of 100  #> Gibbs sampler iteration 40 of 100  #> Gibbs sampler iteration 50 of 100  #> Gibbs sampler iteration 60 of 100  #> Gibbs sampler iteration 70 of 100  #> Gibbs sampler iteration 80 of 100  #> Gibbs sampler iteration 90 of 100  #> Gibbs sampler iteration 100 of 100  #> Probit model 'choice ~ time'."},{"path":"https://loelschlaeger.de/RprobitB/reference/nobs.html","id":null,"dir":"Reference","previous_headings":"","what":"Number of observations — nobs","title":"Number of observations — nobs","text":"function extracts number observations RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/nobs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Number of observations — nobs","text":"","code":"nobs(object, ...)  # S3 method for RprobitB_fit nobs(object, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/nobs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Number of observations — nobs","text":"object object class RprobitB_fit. ... Ignored.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/nobs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Number of observations — nobs","text":"integer.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/nobs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Number of observations — nobs","text":"","code":"data(\"model_train\", package = \"RprobitB\") nobs(model_train) #> [1] 2929"},{"path":"https://loelschlaeger.de/RprobitB/reference/npar.html","id":null,"dir":"Reference","previous_headings":"","what":"Number of model parameters — npar","title":"Number of model parameters — npar","text":"function extracts number model parameters RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/npar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Number of model parameters — npar","text":"","code":"npar(object, ...)  # S3 method for RprobitB_fit npar(object, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/npar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Number of model parameters — npar","text":"object object class RprobitB_fit. ... Optionally objects class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/npar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Number of model parameters — npar","text":"Either numeric value (just one object provided) numeric vector.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/npar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Number of model parameters — npar","text":"","code":"data(\"model_train\", package = \"RprobitB\") npar(model_train) #> [1] 4"},{"path":"https://loelschlaeger.de/RprobitB/reference/overview_effects.html","id":null,"dir":"Reference","previous_headings":"","what":"Effect overview — overview_effects","title":"Effect overview — overview_effects","text":"function gives overview model coefficients whether connected random effects.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/overview_effects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Effect overview — overview_effects","text":"","code":"overview_effects(form, re = NULL, alternatives)"},{"path":"https://loelschlaeger.de/RprobitB/reference/overview_effects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Effect overview — overview_effects","text":"form formula object used specify probit model. structure choice ~ | B | C, names alternative choice situation specific covariates generic coefficient, B names choice situation specific covariates alternative specific coefficients, C names alternative choice situation specific covariates alternative specific coefficients. Separate multiple covariates one type + sign. default, alternative specific constants (ASCs) added model (except last alternative due identifiability). can removed adding +0 second spot. See vignette choice data details. re character (vector) covariates form random effects. re = NULL (default), random effects. random effects alternative specific constants, include \"ASC\" re. alternatives character vector names choice alternatives. specified, choice set defined observed choices.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/overview_effects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Effect overview — overview_effects","text":"data frame coefficient names booleans indicating whether connected random effects.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/reference/overview_effects.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Effect overview — overview_effects","text":"","code":"form <- choice ~ price + time + comfort + change | 1 re <- c(\"price\", \"time\") alternatives <- c(\"A\", \"B\") overview_effects(form = form, re = re, alternatives = alternatives) #>      name    re #> 1 comfort FALSE #> 2  change FALSE #> 3   ASC_A FALSE #> 4   price  TRUE #> 5    time  TRUE"},{"path":"https://loelschlaeger.de/RprobitB/reference/parameter_labels.html","id":null,"dir":"Reference","previous_headings":"","what":"Create parameters labels — parameter_labels","title":"Create parameters labels — parameter_labels","text":"function model parameter labels.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/parameter_labels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create parameters labels — parameter_labels","text":"","code":"parameter_labels(   P_f,   P_r,   J,   C,   cov_sym,   keep_par = c(\"s\", \"alpha\", \"b\", \"Omega\", \"Sigma\"),   drop_par = NULL )"},{"path":"https://loelschlaeger.de/RprobitB/reference/parameter_labels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create parameters labels — parameter_labels","text":"P_f number covariates connected fixed coefficient (can 0). P_r number covariates connected random coefficient (can 0). J number (greater equal 2) choice alternatives. cov_sym Set TRUE labels symmetric covariance elements. keep_par vector parameter names kept. drop_par vector parameter names get dropped.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/parameter_labels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create parameters labels — parameter_labels","text":"list labels selected model parameters.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/parameter_labels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create parameters labels — parameter_labels","text":"","code":"RprobitB:::parameter_labels(P_f = 1, P_r = 2, J = 3, C = 2, cov_sym = TRUE) #> $s #> [1] \"1\" \"2\" #>  #> $alpha #> [1] \"1\" #>  #> $b #> [1] \"1.1\" \"1.2\" \"2.1\" \"2.2\" #>  #> $Omega #> [1] \"1.1,1\" \"1.1,2\" \"1.2,1\" \"1.2,2\" \"2.1,1\" \"2.1,2\" \"2.2,1\" \"2.2,2\" #>  #> $Sigma #> [1] \"1,1\" \"1,2\" \"2,1\" \"2,2\" #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for RprobitB_data — plot.RprobitB_data","title":"Plot method for RprobitB_data — plot.RprobitB_data","text":"function plot method object class RprobitB_data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for RprobitB_data — plot.RprobitB_data","text":"","code":"# S3 method for RprobitB_data plot(x, by_choice = FALSE, alpha = 1, position = \"dodge\", ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for RprobitB_data — plot.RprobitB_data","text":"x object class RprobitB_data. by_choice Set TRUE group covariates chosen alternatives. alpha, position Passed ggplot. ... Ignored.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot method for RprobitB_data — plot.RprobitB_data","text":"return value. Draws plot current device.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot method for RprobitB_data — plot.RprobitB_data","text":"","code":"data <- simulate_choices(  form = choice ~ cost | 0,  N = 100,  T = 10,  J = 2,  alternatives = c(\"bus\", \"car\"),  alpha = -1 ) plot(data, by_choice = TRUE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for RprobitB_fit — plot.RprobitB_fit","title":"Plot method for RprobitB_fit — plot.RprobitB_fit","text":"function plot method object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for RprobitB_fit — plot.RprobitB_fit","text":"","code":"# S3 method for RprobitB_fit plot(x, type, ignore = NULL, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for RprobitB_fit — plot.RprobitB_fit","text":"x object class RprobitB_fit. type type plot, can one : \"mixture\" visualize mixing distribution, \"acf\" autocorrelation plots Gibbs samples, \"trace\" trace plots Gibbs samples, \"class_seq\" visualize sequence class numbers, \"class_allocation\" visualize class allocation (P_r = 2). See details section visualization options. ignore character (vector) covariate parameter names get visualized. ... Additional parameters, see details section.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot method for RprobitB_fit — plot.RprobitB_fit","text":"return value. Draws plot current device.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot method for RprobitB_fit — plot.RprobitB_fit","text":"plot types additional options can specified via submitting following parameters ellipsis arguments.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot.RprobitB_fit.html","id":"-type-class-allocation-","dir":"Reference","previous_headings":"","what":"\"type = class_allocation\"","title":"Plot method for RprobitB_fit — plot.RprobitB_fit","text":"numeric vector iterations plotting class allocation different iterations Gibbs sampler. numeric perc 0 1 draw perc percentile ellipsoids underlying Gaussian distributions (perc = 0.95 per default). numeric sleep, number seconds pause plotting. default 1.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_class_allocation.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot class allocation (for P_r = 2 only) — plot_class_allocation","title":"Plot class allocation (for P_r = 2 only) — plot_class_allocation","text":"function plots allocation decision-maker specific coefficient vectors beta given allocation vector z, class means b, class covariance matrices Omega.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_class_allocation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot class allocation (for P_r = 2 only) — plot_class_allocation","text":"","code":"plot_class_allocation(beta, z, b, Omega, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_class_allocation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot class allocation (for P_r = 2 only) — plot_class_allocation","text":"beta matrix decision-maker specific coefficient vectors dimension P_r x N. Set NA P_r = 0. z vector allocation variables length N. Set NA P_r = 0. b matrix class means columns dimension P_r x C. Set NA P_r = 0. Omega matrix class covariance matrices columns dimension P_r*P_r x C. Set NA P_r = 0. ... Optional visualization parameters: colors, character vector color specifications, perc, numeric 0 1 draw perc percentile ellipsoids underlying Gaussian distributions (perc = 0.95 per default), r, current iteration number Gibbs sampler displayed legend, sleep, number seconds pause plotting.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_class_allocation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot class allocation (for P_r = 2 only) — plot_class_allocation","text":"return value. Draws plot current device.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_class_allocation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot class allocation (for P_r = 2 only) — plot_class_allocation","text":"two-dimensional case, .e. P_r = 2.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_class_allocation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot class allocation (for P_r = 2 only) — plot_class_allocation","text":"","code":"b <- matrix(c(-1,1,1,1), ncol = 2) Omega <- matrix(c(0.8,0.5,0.5,1,0.5,-0.2,-0.2,0.3), ncol = 2) z <- rep(1:2, each = 10) beta <- sapply(z, function(z) rmvnorm(mu = b[,z], Sigma = matrix(Omega[,z], ncol = 2))) RprobitB:::plot_class_allocation(beta = beta, z = z, b = b, Omega = Omega,                                  colors = c(\"red\",\"blue\"), perc = 0.5, r = 1)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_contour.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot bivariate contour of mixing distributions — plot_mixture_contour","title":"Plot bivariate contour of mixing distributions — plot_mixture_contour","text":"function plots estimated ivariate contour mixing distributions.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_contour.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot bivariate contour of mixing distributions — plot_mixture_contour","text":"","code":"plot_mixture_contour(means, covs, weights, names)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_contour.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot bivariate contour of mixing distributions — plot_mixture_contour","text":"means  covs  weights  names","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_contour.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot bivariate contour of mixing distributions — plot_mixture_contour","text":"return value. Draws plot current device.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_contour.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot bivariate contour of mixing distributions — plot_mixture_contour","text":"","code":"means <- list(c(0,0),c(1,1)) covs <- list(diag(2),0.5*diag(2)) weights <- c(0.3,0.7) names <- c(\"A\",\"B\") RprobitB:::plot_mixture_contour(means, covs, weights, names)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_marginal.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot marginal mixing distributions — plot_mixture_marginal","title":"Plot marginal mixing distributions — plot_mixture_marginal","text":"function plots estimated marginal mixing distributions.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_marginal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot marginal mixing distributions — plot_mixture_marginal","text":"","code":"plot_mixture_marginal(mean, cov, weights, name)"},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_marginal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot marginal mixing distributions — plot_mixture_marginal","text":"mean  cov  weights  name","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_marginal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot marginal mixing distributions — plot_mixture_marginal","text":"return value. Draws plot current device.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/plot_mixture_marginal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot marginal mixing distributions — plot_mixture_marginal","text":"","code":"mean <- list(1,2) cov <- list(0.1,1) weights <- c(0.3,0.7) name <- \"test\" RprobitB:::plot_mixture_marginal(mean, cov, weights, name)"},{"path":"https://loelschlaeger.de/RprobitB/reference/point_estimates.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute point estimates based on Gibbs samples of an RprobitB_fit\nobject — point_estimates","title":"Compute point estimates based on Gibbs samples of an RprobitB_fit\nobject — point_estimates","text":"function computes point estimates RprobitB_fit. Per default, mean Gibbs samples used point estimate. However, statistic computes single numeric value vector Gibbs samples can specified FUN.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/point_estimates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute point estimates based on Gibbs samples of an RprobitB_fit\nobject — point_estimates","text":"","code":"point_estimates(x, FUN = mean)"},{"path":"https://loelschlaeger.de/RprobitB/reference/point_estimates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute point estimates based on Gibbs samples of an RprobitB_fit\nobject — point_estimates","text":"x object class RprobitB_fit. FUN function computes single numeric value vector numeric values.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/point_estimates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute point estimates based on Gibbs samples of an RprobitB_fit\nobject — point_estimates","text":"object class RprobitB_parameter.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/point_estimates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute point estimates based on Gibbs samples of an RprobitB_fit\nobject — point_estimates","text":"","code":"data <- simulate_choices(form = choice ~ covariate, N = 10, T = 10, J = 2) model <- mcmc(data) #> Computing sufficient statistics 0 of 4  #> Computing sufficient statistics 1 of 4  #> Computing sufficient statistics 2 of 4  #> Computing sufficient statistics 3 of 4  #> Computing sufficient statistics 4 of 4  #> Gibbs sampler iteration 1 of 10000  #> Gibbs sampler iteration 10 of 10000  #> Gibbs sampler iteration 20 of 10000  #> Gibbs sampler iteration 30 of 10000  #> Gibbs sampler iteration 40 of 10000  #> Gibbs sampler iteration 50 of 10000  #> Gibbs sampler iteration 60 of 10000  #> Gibbs sampler iteration 70 of 10000  #> Gibbs sampler iteration 80 of 10000  #> Gibbs sampler iteration 90 of 10000  #> Gibbs sampler iteration 100 of 10000  #> Gibbs sampler iteration 110 of 10000  #> Gibbs sampler iteration 120 of 10000  #> Gibbs sampler iteration 130 of 10000  #> Gibbs sampler iteration 140 of 10000  #> Gibbs sampler iteration 150 of 10000  #> Gibbs sampler iteration 160 of 10000  #> Gibbs sampler iteration 170 of 10000  #> Gibbs sampler iteration 180 of 10000  #> Gibbs sampler iteration 190 of 10000  #> Gibbs sampler iteration 200 of 10000  #> Gibbs sampler iteration 210 of 10000  #> Gibbs sampler iteration 220 of 10000  #> Gibbs sampler iteration 230 of 10000  #> Gibbs sampler iteration 240 of 10000  #> Gibbs sampler iteration 250 of 10000  #> Gibbs sampler iteration 260 of 10000  #> Gibbs sampler iteration 270 of 10000  #> Gibbs sampler iteration 280 of 10000  #> Gibbs sampler iteration 290 of 10000  #> Gibbs sampler iteration 300 of 10000  #> Gibbs sampler iteration 310 of 10000  #> Gibbs sampler iteration 320 of 10000  #> Gibbs sampler iteration 330 of 10000  #> Gibbs sampler iteration 340 of 10000  #> Gibbs sampler iteration 350 of 10000  #> Gibbs sampler iteration 360 of 10000  #> Gibbs sampler iteration 370 of 10000  #> Gibbs sampler iteration 380 of 10000  #> Gibbs sampler iteration 390 of 10000  #> Gibbs sampler iteration 400 of 10000  #> Gibbs sampler iteration 410 of 10000  #> Gibbs sampler iteration 420 of 10000  #> Gibbs sampler iteration 430 of 10000  #> Gibbs sampler iteration 440 of 10000  #> Gibbs sampler iteration 450 of 10000  #> Gibbs sampler iteration 460 of 10000  #> Gibbs sampler iteration 470 of 10000  #> Gibbs sampler iteration 480 of 10000  #> Gibbs sampler iteration 490 of 10000  #> Gibbs sampler iteration 500 of 10000  #> Gibbs sampler iteration 510 of 10000  #> Gibbs sampler iteration 520 of 10000  #> Gibbs sampler iteration 530 of 10000  #> Gibbs sampler iteration 540 of 10000  #> Gibbs sampler iteration 550 of 10000  #> Gibbs sampler iteration 560 of 10000  #> Gibbs sampler iteration 570 of 10000  #> Gibbs sampler iteration 580 of 10000  #> Gibbs sampler iteration 590 of 10000  #> Gibbs sampler iteration 600 of 10000  #> Gibbs sampler iteration 610 of 10000  #> Gibbs sampler iteration 620 of 10000  #> Gibbs sampler iteration 630 of 10000  #> Gibbs sampler iteration 640 of 10000  #> Gibbs sampler iteration 650 of 10000  #> Gibbs sampler iteration 660 of 10000  #> Gibbs sampler iteration 670 of 10000  #> Gibbs sampler iteration 680 of 10000  #> Gibbs sampler iteration 690 of 10000  #> Gibbs sampler iteration 700 of 10000  #> Gibbs sampler iteration 710 of 10000  #> Gibbs sampler iteration 720 of 10000  #> Gibbs sampler iteration 730 of 10000  #> Gibbs sampler iteration 740 of 10000  #> Gibbs sampler iteration 750 of 10000  #> Gibbs sampler iteration 760 of 10000  #> Gibbs sampler iteration 770 of 10000  #> Gibbs sampler iteration 780 of 10000  #> Gibbs sampler iteration 790 of 10000  #> Gibbs sampler iteration 800 of 10000  #> Gibbs sampler iteration 810 of 10000  #> Gibbs sampler iteration 820 of 10000  #> Gibbs sampler iteration 830 of 10000  #> Gibbs sampler iteration 840 of 10000  #> Gibbs sampler iteration 850 of 10000  #> Gibbs sampler iteration 860 of 10000  #> Gibbs sampler iteration 870 of 10000  #> Gibbs sampler iteration 880 of 10000  #> Gibbs sampler iteration 890 of 10000  #> Gibbs sampler iteration 900 of 10000  #> Gibbs sampler iteration 910 of 10000  #> Gibbs sampler iteration 920 of 10000  #> Gibbs sampler iteration 930 of 10000  #> Gibbs sampler iteration 940 of 10000  #> Gibbs sampler iteration 950 of 10000  #> Gibbs sampler iteration 960 of 10000  #> Gibbs sampler iteration 970 of 10000  #> Gibbs sampler iteration 980 of 10000  #> Gibbs sampler iteration 990 of 10000  #> Gibbs sampler iteration 1000 of 10000  #> Gibbs sampler iteration 1010 of 10000  #> Gibbs sampler iteration 1020 of 10000  #> Gibbs sampler iteration 1030 of 10000  #> Gibbs sampler iteration 1040 of 10000  #> Gibbs sampler iteration 1050 of 10000  #> Gibbs sampler iteration 1060 of 10000  #> Gibbs sampler iteration 1070 of 10000  #> Gibbs sampler iteration 1080 of 10000  #> Gibbs sampler iteration 1090 of 10000  #> Gibbs sampler iteration 1100 of 10000  #> Gibbs sampler iteration 1110 of 10000  #> Gibbs sampler iteration 1120 of 10000  #> Gibbs sampler iteration 1130 of 10000  #> Gibbs sampler iteration 1140 of 10000  #> Gibbs sampler iteration 1150 of 10000  #> Gibbs sampler iteration 1160 of 10000  #> Gibbs sampler iteration 1170 of 10000  #> Gibbs sampler iteration 1180 of 10000  #> Gibbs sampler iteration 1190 of 10000  #> Gibbs sampler iteration 1200 of 10000  #> Gibbs sampler iteration 1210 of 10000  #> Gibbs sampler iteration 1220 of 10000  #> Gibbs sampler iteration 1230 of 10000  #> Gibbs sampler iteration 1240 of 10000  #> Gibbs sampler iteration 1250 of 10000  #> Gibbs sampler iteration 1260 of 10000  #> Gibbs sampler iteration 1270 of 10000  #> Gibbs sampler iteration 1280 of 10000  #> Gibbs sampler iteration 1290 of 10000  #> Gibbs sampler iteration 1300 of 10000  #> Gibbs sampler iteration 1310 of 10000  #> Gibbs sampler iteration 1320 of 10000  #> Gibbs sampler iteration 1330 of 10000  #> Gibbs sampler iteration 1340 of 10000  #> Gibbs sampler iteration 1350 of 10000  #> Gibbs sampler iteration 1360 of 10000  #> Gibbs sampler iteration 1370 of 10000  #> Gibbs sampler iteration 1380 of 10000  #> Gibbs sampler iteration 1390 of 10000  #> Gibbs sampler iteration 1400 of 10000  #> Gibbs sampler iteration 1410 of 10000  #> Gibbs sampler iteration 1420 of 10000  #> Gibbs sampler iteration 1430 of 10000  #> Gibbs sampler iteration 1440 of 10000  #> Gibbs sampler iteration 1450 of 10000  #> Gibbs sampler iteration 1460 of 10000  #> Gibbs sampler iteration 1470 of 10000  #> Gibbs sampler iteration 1480 of 10000  #> Gibbs sampler iteration 1490 of 10000  #> Gibbs sampler iteration 1500 of 10000  #> Gibbs sampler iteration 1510 of 10000  #> Gibbs sampler iteration 1520 of 10000  #> Gibbs sampler iteration 1530 of 10000  #> Gibbs sampler iteration 1540 of 10000  #> Gibbs sampler iteration 1550 of 10000  #> Gibbs sampler iteration 1560 of 10000  #> Gibbs sampler iteration 1570 of 10000  #> Gibbs sampler iteration 1580 of 10000  #> Gibbs sampler iteration 1590 of 10000  #> Gibbs sampler iteration 1600 of 10000  #> Gibbs sampler iteration 1610 of 10000  #> Gibbs sampler iteration 1620 of 10000  #> Gibbs sampler iteration 1630 of 10000  #> Gibbs sampler iteration 1640 of 10000  #> Gibbs sampler iteration 1650 of 10000  #> Gibbs sampler iteration 1660 of 10000  #> Gibbs sampler iteration 1670 of 10000  #> Gibbs sampler iteration 1680 of 10000  #> Gibbs sampler iteration 1690 of 10000  #> Gibbs sampler iteration 1700 of 10000  #> Gibbs sampler iteration 1710 of 10000  #> Gibbs sampler iteration 1720 of 10000  #> Gibbs sampler iteration 1730 of 10000  #> Gibbs sampler iteration 1740 of 10000  #> Gibbs sampler iteration 1750 of 10000  #> Gibbs sampler iteration 1760 of 10000  #> Gibbs sampler iteration 1770 of 10000  #> Gibbs sampler iteration 1780 of 10000  #> Gibbs sampler iteration 1790 of 10000  #> Gibbs sampler iteration 1800 of 10000  #> Gibbs sampler iteration 1810 of 10000  #> Gibbs sampler iteration 1820 of 10000  #> Gibbs sampler iteration 1830 of 10000  #> Gibbs sampler iteration 1840 of 10000  #> Gibbs sampler iteration 1850 of 10000  #> Gibbs sampler iteration 1860 of 10000  #> Gibbs sampler iteration 1870 of 10000  #> Gibbs sampler iteration 1880 of 10000  #> Gibbs sampler iteration 1890 of 10000  #> Gibbs sampler iteration 1900 of 10000  #> Gibbs sampler iteration 1910 of 10000  #> Gibbs sampler iteration 1920 of 10000  #> Gibbs sampler iteration 1930 of 10000  #> Gibbs sampler iteration 1940 of 10000  #> Gibbs sampler iteration 1950 of 10000  #> Gibbs sampler iteration 1960 of 10000  #> Gibbs sampler iteration 1970 of 10000  #> Gibbs sampler iteration 1980 of 10000  #> Gibbs sampler iteration 1990 of 10000  #> Gibbs sampler iteration 2000 of 10000  #> Gibbs sampler iteration 2010 of 10000  #> Gibbs sampler iteration 2020 of 10000  #> Gibbs sampler iteration 2030 of 10000  #> Gibbs sampler iteration 2040 of 10000  #> Gibbs sampler iteration 2050 of 10000  #> Gibbs sampler iteration 2060 of 10000  #> Gibbs sampler iteration 2070 of 10000  #> Gibbs sampler iteration 2080 of 10000  #> Gibbs sampler iteration 2090 of 10000  #> Gibbs sampler iteration 2100 of 10000  #> Gibbs sampler iteration 2110 of 10000  #> Gibbs sampler iteration 2120 of 10000  #> Gibbs sampler iteration 2130 of 10000  #> Gibbs sampler iteration 2140 of 10000  #> Gibbs sampler iteration 2150 of 10000  #> Gibbs sampler iteration 2160 of 10000  #> Gibbs sampler iteration 2170 of 10000  #> Gibbs sampler iteration 2180 of 10000  #> Gibbs sampler iteration 2190 of 10000  #> Gibbs sampler iteration 2200 of 10000  #> Gibbs sampler iteration 2210 of 10000  #> Gibbs sampler iteration 2220 of 10000  #> Gibbs sampler iteration 2230 of 10000  #> Gibbs sampler iteration 2240 of 10000  #> Gibbs sampler iteration 2250 of 10000  #> Gibbs sampler iteration 2260 of 10000  #> Gibbs sampler iteration 2270 of 10000  #> Gibbs sampler iteration 2280 of 10000  #> Gibbs sampler iteration 2290 of 10000  #> Gibbs sampler iteration 2300 of 10000  #> Gibbs sampler iteration 2310 of 10000  #> Gibbs sampler iteration 2320 of 10000  #> Gibbs sampler iteration 2330 of 10000  #> Gibbs sampler iteration 2340 of 10000  #> Gibbs sampler iteration 2350 of 10000  #> Gibbs sampler iteration 2360 of 10000  #> Gibbs sampler iteration 2370 of 10000  #> Gibbs sampler iteration 2380 of 10000  #> Gibbs sampler iteration 2390 of 10000  #> Gibbs sampler iteration 2400 of 10000  #> Gibbs sampler iteration 2410 of 10000  #> Gibbs sampler iteration 2420 of 10000  #> Gibbs sampler iteration 2430 of 10000  #> Gibbs sampler iteration 2440 of 10000  #> Gibbs sampler iteration 2450 of 10000  #> Gibbs sampler iteration 2460 of 10000  #> Gibbs sampler iteration 2470 of 10000  #> Gibbs sampler iteration 2480 of 10000  #> Gibbs sampler iteration 2490 of 10000  #> Gibbs sampler iteration 2500 of 10000  #> Gibbs sampler iteration 2510 of 10000  #> Gibbs sampler iteration 2520 of 10000  #> Gibbs sampler iteration 2530 of 10000  #> Gibbs sampler iteration 2540 of 10000  #> Gibbs sampler iteration 2550 of 10000  #> Gibbs sampler iteration 2560 of 10000  #> Gibbs sampler iteration 2570 of 10000  #> Gibbs sampler iteration 2580 of 10000  #> Gibbs sampler iteration 2590 of 10000  #> Gibbs sampler iteration 2600 of 10000  #> Gibbs sampler iteration 2610 of 10000  #> Gibbs sampler iteration 2620 of 10000  #> Gibbs sampler iteration 2630 of 10000  #> Gibbs sampler iteration 2640 of 10000  #> Gibbs sampler iteration 2650 of 10000  #> Gibbs sampler iteration 2660 of 10000  #> Gibbs sampler iteration 2670 of 10000  #> Gibbs sampler iteration 2680 of 10000  #> Gibbs sampler iteration 2690 of 10000  #> Gibbs sampler iteration 2700 of 10000  #> Gibbs sampler iteration 2710 of 10000  #> Gibbs sampler iteration 2720 of 10000  #> Gibbs sampler iteration 2730 of 10000  #> Gibbs sampler iteration 2740 of 10000  #> Gibbs sampler iteration 2750 of 10000  #> Gibbs sampler iteration 2760 of 10000  #> Gibbs sampler iteration 2770 of 10000  #> Gibbs sampler iteration 2780 of 10000  #> Gibbs sampler iteration 2790 of 10000  #> Gibbs sampler iteration 2800 of 10000  #> Gibbs sampler iteration 2810 of 10000  #> Gibbs sampler iteration 2820 of 10000  #> Gibbs sampler iteration 2830 of 10000  #> Gibbs sampler iteration 2840 of 10000  #> Gibbs sampler iteration 2850 of 10000  #> Gibbs sampler iteration 2860 of 10000  #> Gibbs sampler iteration 2870 of 10000  #> Gibbs sampler iteration 2880 of 10000  #> Gibbs sampler iteration 2890 of 10000  #> Gibbs sampler iteration 2900 of 10000  #> Gibbs sampler iteration 2910 of 10000  #> Gibbs sampler iteration 2920 of 10000  #> Gibbs sampler iteration 2930 of 10000  #> Gibbs sampler iteration 2940 of 10000  #> Gibbs sampler iteration 2950 of 10000  #> Gibbs sampler iteration 2960 of 10000  #> Gibbs sampler iteration 2970 of 10000  #> Gibbs sampler iteration 2980 of 10000  #> Gibbs sampler iteration 2990 of 10000  #> Gibbs sampler iteration 3000 of 10000  #> Gibbs sampler iteration 3010 of 10000  #> Gibbs sampler iteration 3020 of 10000  #> Gibbs sampler iteration 3030 of 10000  #> Gibbs sampler iteration 3040 of 10000  #> Gibbs sampler iteration 3050 of 10000  #> Gibbs sampler iteration 3060 of 10000  #> Gibbs sampler iteration 3070 of 10000  #> Gibbs sampler iteration 3080 of 10000  #> Gibbs sampler iteration 3090 of 10000  #> Gibbs sampler iteration 3100 of 10000  #> Gibbs sampler iteration 3110 of 10000  #> Gibbs sampler iteration 3120 of 10000  #> Gibbs sampler iteration 3130 of 10000  #> Gibbs sampler iteration 3140 of 10000  #> Gibbs sampler iteration 3150 of 10000  #> Gibbs sampler iteration 3160 of 10000  #> Gibbs sampler iteration 3170 of 10000  #> Gibbs sampler iteration 3180 of 10000  #> Gibbs sampler iteration 3190 of 10000  #> Gibbs sampler iteration 3200 of 10000  #> Gibbs sampler iteration 3210 of 10000  #> Gibbs sampler iteration 3220 of 10000  #> Gibbs sampler iteration 3230 of 10000  #> Gibbs sampler iteration 3240 of 10000  #> Gibbs sampler iteration 3250 of 10000  #> Gibbs sampler iteration 3260 of 10000  #> Gibbs sampler iteration 3270 of 10000  #> Gibbs sampler iteration 3280 of 10000  #> Gibbs sampler iteration 3290 of 10000  #> Gibbs sampler iteration 3300 of 10000  #> Gibbs sampler iteration 3310 of 10000  #> Gibbs sampler iteration 3320 of 10000  #> Gibbs sampler iteration 3330 of 10000  #> Gibbs sampler iteration 3340 of 10000  #> Gibbs sampler iteration 3350 of 10000  #> Gibbs sampler iteration 3360 of 10000  #> Gibbs sampler iteration 3370 of 10000  #> Gibbs sampler iteration 3380 of 10000  #> Gibbs sampler iteration 3390 of 10000  #> Gibbs sampler iteration 3400 of 10000  #> Gibbs sampler iteration 3410 of 10000  #> Gibbs sampler iteration 3420 of 10000  #> Gibbs sampler iteration 3430 of 10000  #> Gibbs sampler iteration 3440 of 10000  #> Gibbs sampler iteration 3450 of 10000  #> Gibbs sampler iteration 3460 of 10000  #> Gibbs sampler iteration 3470 of 10000  #> Gibbs sampler iteration 3480 of 10000  #> Gibbs sampler iteration 3490 of 10000  #> Gibbs sampler iteration 3500 of 10000  #> Gibbs sampler iteration 3510 of 10000  #> Gibbs sampler iteration 3520 of 10000  #> Gibbs sampler iteration 3530 of 10000  #> Gibbs sampler iteration 3540 of 10000  #> Gibbs sampler iteration 3550 of 10000  #> Gibbs sampler iteration 3560 of 10000  #> Gibbs sampler iteration 3570 of 10000  #> Gibbs sampler iteration 3580 of 10000  #> Gibbs sampler iteration 3590 of 10000  #> Gibbs sampler iteration 3600 of 10000  #> Gibbs sampler iteration 3610 of 10000  #> Gibbs sampler iteration 3620 of 10000  #> Gibbs sampler iteration 3630 of 10000  #> Gibbs sampler iteration 3640 of 10000  #> Gibbs sampler iteration 3650 of 10000  #> Gibbs sampler iteration 3660 of 10000  #> Gibbs sampler iteration 3670 of 10000  #> Gibbs sampler iteration 3680 of 10000  #> Gibbs sampler iteration 3690 of 10000  #> Gibbs sampler iteration 3700 of 10000  #> Gibbs sampler iteration 3710 of 10000  #> Gibbs sampler iteration 3720 of 10000  #> Gibbs sampler iteration 3730 of 10000  #> Gibbs sampler iteration 3740 of 10000  #> Gibbs sampler iteration 3750 of 10000  #> Gibbs sampler iteration 3760 of 10000  #> Gibbs sampler iteration 3770 of 10000  #> Gibbs sampler iteration 3780 of 10000  #> Gibbs sampler iteration 3790 of 10000  #> Gibbs sampler iteration 3800 of 10000  #> Gibbs sampler iteration 3810 of 10000  #> Gibbs sampler iteration 3820 of 10000  #> Gibbs sampler iteration 3830 of 10000  #> Gibbs sampler iteration 3840 of 10000  #> Gibbs sampler iteration 3850 of 10000  #> Gibbs sampler iteration 3860 of 10000  #> Gibbs sampler iteration 3870 of 10000  #> Gibbs sampler iteration 3880 of 10000  #> Gibbs sampler iteration 3890 of 10000  #> Gibbs sampler iteration 3900 of 10000  #> Gibbs sampler iteration 3910 of 10000  #> Gibbs sampler iteration 3920 of 10000  #> Gibbs sampler iteration 3930 of 10000  #> Gibbs sampler iteration 3940 of 10000  #> Gibbs sampler iteration 3950 of 10000  #> Gibbs sampler iteration 3960 of 10000  #> Gibbs sampler iteration 3970 of 10000  #> Gibbs sampler iteration 3980 of 10000  #> Gibbs sampler iteration 3990 of 10000  #> Gibbs sampler iteration 4000 of 10000  #> Gibbs sampler iteration 4010 of 10000  #> Gibbs sampler iteration 4020 of 10000  #> Gibbs sampler iteration 4030 of 10000  #> Gibbs sampler iteration 4040 of 10000  #> Gibbs sampler iteration 4050 of 10000  #> Gibbs sampler iteration 4060 of 10000  #> Gibbs sampler iteration 4070 of 10000  #> Gibbs sampler iteration 4080 of 10000  #> Gibbs sampler iteration 4090 of 10000  #> Gibbs sampler iteration 4100 of 10000  #> Gibbs sampler iteration 4110 of 10000  #> Gibbs sampler iteration 4120 of 10000  #> Gibbs sampler iteration 4130 of 10000  #> Gibbs sampler iteration 4140 of 10000  #> Gibbs sampler iteration 4150 of 10000  #> Gibbs sampler iteration 4160 of 10000  #> Gibbs sampler iteration 4170 of 10000  #> Gibbs sampler iteration 4180 of 10000  #> Gibbs sampler iteration 4190 of 10000  #> Gibbs sampler iteration 4200 of 10000  #> Gibbs sampler iteration 4210 of 10000  #> Gibbs sampler iteration 4220 of 10000  #> Gibbs sampler iteration 4230 of 10000  #> Gibbs sampler iteration 4240 of 10000  #> Gibbs sampler iteration 4250 of 10000  #> Gibbs sampler iteration 4260 of 10000  #> Gibbs sampler iteration 4270 of 10000  #> Gibbs sampler iteration 4280 of 10000  #> Gibbs sampler iteration 4290 of 10000  #> Gibbs sampler iteration 4300 of 10000  #> Gibbs sampler iteration 4310 of 10000  #> Gibbs sampler iteration 4320 of 10000  #> Gibbs sampler iteration 4330 of 10000  #> Gibbs sampler iteration 4340 of 10000  #> Gibbs sampler iteration 4350 of 10000  #> Gibbs sampler iteration 4360 of 10000  #> Gibbs sampler iteration 4370 of 10000  #> Gibbs sampler iteration 4380 of 10000  #> Gibbs sampler iteration 4390 of 10000  #> Gibbs sampler iteration 4400 of 10000  #> Gibbs sampler iteration 4410 of 10000  #> Gibbs sampler iteration 4420 of 10000  #> Gibbs sampler iteration 4430 of 10000  #> Gibbs sampler iteration 4440 of 10000  #> Gibbs sampler iteration 4450 of 10000  #> Gibbs sampler iteration 4460 of 10000  #> Gibbs sampler iteration 4470 of 10000  #> Gibbs sampler iteration 4480 of 10000  #> Gibbs sampler iteration 4490 of 10000  #> Gibbs sampler iteration 4500 of 10000  #> Gibbs sampler iteration 4510 of 10000  #> Gibbs sampler iteration 4520 of 10000  #> Gibbs sampler iteration 4530 of 10000  #> Gibbs sampler iteration 4540 of 10000  #> Gibbs sampler iteration 4550 of 10000  #> Gibbs sampler iteration 4560 of 10000  #> Gibbs sampler iteration 4570 of 10000  #> Gibbs sampler iteration 4580 of 10000  #> Gibbs sampler iteration 4590 of 10000  #> Gibbs sampler iteration 4600 of 10000  #> Gibbs sampler iteration 4610 of 10000  #> Gibbs sampler iteration 4620 of 10000  #> Gibbs sampler iteration 4630 of 10000  #> Gibbs sampler iteration 4640 of 10000  #> Gibbs sampler iteration 4650 of 10000  #> Gibbs sampler iteration 4660 of 10000  #> Gibbs sampler iteration 4670 of 10000  #> Gibbs sampler iteration 4680 of 10000  #> Gibbs sampler iteration 4690 of 10000  #> Gibbs sampler iteration 4700 of 10000  #> Gibbs sampler iteration 4710 of 10000  #> Gibbs sampler iteration 4720 of 10000  #> Gibbs sampler iteration 4730 of 10000  #> Gibbs sampler iteration 4740 of 10000  #> Gibbs sampler iteration 4750 of 10000  #> Gibbs sampler iteration 4760 of 10000  #> Gibbs sampler iteration 4770 of 10000  #> Gibbs sampler iteration 4780 of 10000  #> Gibbs sampler iteration 4790 of 10000  #> Gibbs sampler iteration 4800 of 10000  #> Gibbs sampler iteration 4810 of 10000  #> Gibbs sampler iteration 4820 of 10000  #> Gibbs sampler iteration 4830 of 10000  #> Gibbs sampler iteration 4840 of 10000  #> Gibbs sampler iteration 4850 of 10000  #> Gibbs sampler iteration 4860 of 10000  #> Gibbs sampler iteration 4870 of 10000  #> Gibbs sampler iteration 4880 of 10000  #> Gibbs sampler iteration 4890 of 10000  #> Gibbs sampler iteration 4900 of 10000  #> Gibbs sampler iteration 4910 of 10000  #> Gibbs sampler iteration 4920 of 10000  #> Gibbs sampler iteration 4930 of 10000  #> Gibbs sampler iteration 4940 of 10000  #> Gibbs sampler iteration 4950 of 10000  #> Gibbs sampler iteration 4960 of 10000  #> Gibbs sampler iteration 4970 of 10000  #> Gibbs sampler iteration 4980 of 10000  #> Gibbs sampler iteration 4990 of 10000  #> Gibbs sampler iteration 5000 of 10000  #> Gibbs sampler iteration 5010 of 10000  #> Gibbs sampler iteration 5020 of 10000  #> Gibbs sampler iteration 5030 of 10000  #> Gibbs sampler iteration 5040 of 10000  #> Gibbs sampler iteration 5050 of 10000  #> Gibbs sampler iteration 5060 of 10000  #> Gibbs sampler iteration 5070 of 10000  #> Gibbs sampler iteration 5080 of 10000  #> Gibbs sampler iteration 5090 of 10000  #> Gibbs sampler iteration 5100 of 10000  #> Gibbs sampler iteration 5110 of 10000  #> Gibbs sampler iteration 5120 of 10000  #> Gibbs sampler iteration 5130 of 10000  #> Gibbs sampler iteration 5140 of 10000  #> Gibbs sampler iteration 5150 of 10000  #> Gibbs sampler iteration 5160 of 10000  #> Gibbs sampler iteration 5170 of 10000  #> Gibbs sampler iteration 5180 of 10000  #> Gibbs sampler iteration 5190 of 10000  #> Gibbs sampler iteration 5200 of 10000  #> Gibbs sampler iteration 5210 of 10000  #> Gibbs sampler iteration 5220 of 10000  #> Gibbs sampler iteration 5230 of 10000  #> Gibbs sampler iteration 5240 of 10000  #> Gibbs sampler iteration 5250 of 10000  #> Gibbs sampler iteration 5260 of 10000  #> Gibbs sampler iteration 5270 of 10000  #> Gibbs sampler iteration 5280 of 10000  #> Gibbs sampler iteration 5290 of 10000  #> Gibbs sampler iteration 5300 of 10000  #> Gibbs sampler iteration 5310 of 10000  #> Gibbs sampler iteration 5320 of 10000  #> Gibbs sampler iteration 5330 of 10000  #> Gibbs sampler iteration 5340 of 10000  #> Gibbs sampler iteration 5350 of 10000  #> Gibbs sampler iteration 5360 of 10000  #> Gibbs sampler iteration 5370 of 10000  #> Gibbs sampler iteration 5380 of 10000  #> Gibbs sampler iteration 5390 of 10000  #> Gibbs sampler iteration 5400 of 10000  #> Gibbs sampler iteration 5410 of 10000  #> Gibbs sampler iteration 5420 of 10000  #> Gibbs sampler iteration 5430 of 10000  #> Gibbs sampler iteration 5440 of 10000  #> Gibbs sampler iteration 5450 of 10000  #> Gibbs sampler iteration 5460 of 10000  #> Gibbs sampler iteration 5470 of 10000  #> Gibbs sampler iteration 5480 of 10000  #> Gibbs sampler iteration 5490 of 10000  #> Gibbs sampler iteration 5500 of 10000  #> Gibbs sampler iteration 5510 of 10000  #> Gibbs sampler iteration 5520 of 10000  #> Gibbs sampler iteration 5530 of 10000  #> Gibbs sampler iteration 5540 of 10000  #> Gibbs sampler iteration 5550 of 10000  #> Gibbs sampler iteration 5560 of 10000  #> Gibbs sampler iteration 5570 of 10000  #> Gibbs sampler iteration 5580 of 10000  #> Gibbs sampler iteration 5590 of 10000  #> Gibbs sampler iteration 5600 of 10000  #> Gibbs sampler iteration 5610 of 10000  #> Gibbs sampler iteration 5620 of 10000  #> Gibbs sampler iteration 5630 of 10000  #> Gibbs sampler iteration 5640 of 10000  #> Gibbs sampler iteration 5650 of 10000  #> Gibbs sampler iteration 5660 of 10000  #> Gibbs sampler iteration 5670 of 10000  #> Gibbs sampler iteration 5680 of 10000  #> Gibbs sampler iteration 5690 of 10000  #> Gibbs sampler iteration 5700 of 10000  #> Gibbs sampler iteration 5710 of 10000  #> Gibbs sampler iteration 5720 of 10000  #> Gibbs sampler iteration 5730 of 10000  #> Gibbs sampler iteration 5740 of 10000  #> Gibbs sampler iteration 5750 of 10000  #> Gibbs sampler iteration 5760 of 10000  #> Gibbs sampler iteration 5770 of 10000  #> Gibbs sampler iteration 5780 of 10000  #> Gibbs sampler iteration 5790 of 10000  #> Gibbs sampler iteration 5800 of 10000  #> Gibbs sampler iteration 5810 of 10000  #> Gibbs sampler iteration 5820 of 10000  #> Gibbs sampler iteration 5830 of 10000  #> Gibbs sampler iteration 5840 of 10000  #> Gibbs sampler iteration 5850 of 10000  #> Gibbs sampler iteration 5860 of 10000  #> Gibbs sampler iteration 5870 of 10000  #> Gibbs sampler iteration 5880 of 10000  #> Gibbs sampler iteration 5890 of 10000  #> Gibbs sampler iteration 5900 of 10000  #> Gibbs sampler iteration 5910 of 10000  #> Gibbs sampler iteration 5920 of 10000  #> Gibbs sampler iteration 5930 of 10000  #> Gibbs sampler iteration 5940 of 10000  #> Gibbs sampler iteration 5950 of 10000  #> Gibbs sampler iteration 5960 of 10000  #> Gibbs sampler iteration 5970 of 10000  #> Gibbs sampler iteration 5980 of 10000  #> Gibbs sampler iteration 5990 of 10000  #> Gibbs sampler iteration 6000 of 10000  #> Gibbs sampler iteration 6010 of 10000  #> Gibbs sampler iteration 6020 of 10000  #> Gibbs sampler iteration 6030 of 10000  #> Gibbs sampler iteration 6040 of 10000  #> Gibbs sampler iteration 6050 of 10000  #> Gibbs sampler iteration 6060 of 10000  #> Gibbs sampler iteration 6070 of 10000  #> Gibbs sampler iteration 6080 of 10000  #> Gibbs sampler iteration 6090 of 10000  #> Gibbs sampler iteration 6100 of 10000  #> Gibbs sampler iteration 6110 of 10000  #> Gibbs sampler iteration 6120 of 10000  #> Gibbs sampler iteration 6130 of 10000  #> Gibbs sampler iteration 6140 of 10000  #> Gibbs sampler iteration 6150 of 10000  #> Gibbs sampler iteration 6160 of 10000  #> Gibbs sampler iteration 6170 of 10000  #> Gibbs sampler iteration 6180 of 10000  #> Gibbs sampler iteration 6190 of 10000  #> Gibbs sampler iteration 6200 of 10000  #> Gibbs sampler iteration 6210 of 10000  #> Gibbs sampler iteration 6220 of 10000  #> Gibbs sampler iteration 6230 of 10000  #> Gibbs sampler iteration 6240 of 10000  #> Gibbs sampler iteration 6250 of 10000  #> Gibbs sampler iteration 6260 of 10000  #> Gibbs sampler iteration 6270 of 10000  #> Gibbs sampler iteration 6280 of 10000  #> Gibbs sampler iteration 6290 of 10000  #> Gibbs sampler iteration 6300 of 10000  #> Gibbs sampler iteration 6310 of 10000  #> Gibbs sampler iteration 6320 of 10000  #> Gibbs sampler iteration 6330 of 10000  #> Gibbs sampler iteration 6340 of 10000  #> Gibbs sampler iteration 6350 of 10000  #> Gibbs sampler iteration 6360 of 10000  #> Gibbs sampler iteration 6370 of 10000  #> Gibbs sampler iteration 6380 of 10000  #> Gibbs sampler iteration 6390 of 10000  #> Gibbs sampler iteration 6400 of 10000  #> Gibbs sampler iteration 6410 of 10000  #> Gibbs sampler iteration 6420 of 10000  #> Gibbs sampler iteration 6430 of 10000  #> Gibbs sampler iteration 6440 of 10000  #> Gibbs sampler iteration 6450 of 10000  #> Gibbs sampler iteration 6460 of 10000  #> Gibbs sampler iteration 6470 of 10000  #> Gibbs sampler iteration 6480 of 10000  #> Gibbs sampler iteration 6490 of 10000  #> Gibbs sampler iteration 6500 of 10000  #> Gibbs sampler iteration 6510 of 10000  #> Gibbs sampler iteration 6520 of 10000  #> Gibbs sampler iteration 6530 of 10000  #> Gibbs sampler iteration 6540 of 10000  #> Gibbs sampler iteration 6550 of 10000  #> Gibbs sampler iteration 6560 of 10000  #> Gibbs sampler iteration 6570 of 10000  #> Gibbs sampler iteration 6580 of 10000  #> Gibbs sampler iteration 6590 of 10000  #> Gibbs sampler iteration 6600 of 10000  #> Gibbs sampler iteration 6610 of 10000  #> Gibbs sampler iteration 6620 of 10000  #> Gibbs sampler iteration 6630 of 10000  #> Gibbs sampler iteration 6640 of 10000  #> Gibbs sampler iteration 6650 of 10000  #> Gibbs sampler iteration 6660 of 10000  #> Gibbs sampler iteration 6670 of 10000  #> Gibbs sampler iteration 6680 of 10000  #> Gibbs sampler iteration 6690 of 10000  #> Gibbs sampler iteration 6700 of 10000  #> Gibbs sampler iteration 6710 of 10000  #> Gibbs sampler iteration 6720 of 10000  #> Gibbs sampler iteration 6730 of 10000  #> Gibbs sampler iteration 6740 of 10000  #> Gibbs sampler iteration 6750 of 10000  #> Gibbs sampler iteration 6760 of 10000  #> Gibbs sampler iteration 6770 of 10000  #> Gibbs sampler iteration 6780 of 10000  #> Gibbs sampler iteration 6790 of 10000  #> Gibbs sampler iteration 6800 of 10000  #> Gibbs sampler iteration 6810 of 10000  #> Gibbs sampler iteration 6820 of 10000  #> Gibbs sampler iteration 6830 of 10000  #> Gibbs sampler iteration 6840 of 10000  #> Gibbs sampler iteration 6850 of 10000  #> Gibbs sampler iteration 6860 of 10000  #> Gibbs sampler iteration 6870 of 10000  #> Gibbs sampler iteration 6880 of 10000  #> Gibbs sampler iteration 6890 of 10000  #> Gibbs sampler iteration 6900 of 10000  #> Gibbs sampler iteration 6910 of 10000  #> Gibbs sampler iteration 6920 of 10000  #> Gibbs sampler iteration 6930 of 10000  #> Gibbs sampler iteration 6940 of 10000  #> Gibbs sampler iteration 6950 of 10000  #> Gibbs sampler iteration 6960 of 10000  #> Gibbs sampler iteration 6970 of 10000  #> Gibbs sampler iteration 6980 of 10000  #> Gibbs sampler iteration 6990 of 10000  #> Gibbs sampler iteration 7000 of 10000  #> Gibbs sampler iteration 7010 of 10000  #> Gibbs sampler iteration 7020 of 10000  #> Gibbs sampler iteration 7030 of 10000  #> Gibbs sampler iteration 7040 of 10000  #> Gibbs sampler iteration 7050 of 10000  #> Gibbs sampler iteration 7060 of 10000  #> Gibbs sampler iteration 7070 of 10000  #> Gibbs sampler iteration 7080 of 10000  #> Gibbs sampler iteration 7090 of 10000  #> Gibbs sampler iteration 7100 of 10000  #> Gibbs sampler iteration 7110 of 10000  #> Gibbs sampler iteration 7120 of 10000  #> Gibbs sampler iteration 7130 of 10000  #> Gibbs sampler iteration 7140 of 10000  #> Gibbs sampler iteration 7150 of 10000  #> Gibbs sampler iteration 7160 of 10000  #> Gibbs sampler iteration 7170 of 10000  #> Gibbs sampler iteration 7180 of 10000  #> Gibbs sampler iteration 7190 of 10000  #> Gibbs sampler iteration 7200 of 10000  #> Gibbs sampler iteration 7210 of 10000  #> Gibbs sampler iteration 7220 of 10000  #> Gibbs sampler iteration 7230 of 10000  #> Gibbs sampler iteration 7240 of 10000  #> Gibbs sampler iteration 7250 of 10000  #> Gibbs sampler iteration 7260 of 10000  #> Gibbs sampler iteration 7270 of 10000  #> Gibbs sampler iteration 7280 of 10000  #> Gibbs sampler iteration 7290 of 10000  #> Gibbs sampler iteration 7300 of 10000  #> Gibbs sampler iteration 7310 of 10000  #> Gibbs sampler iteration 7320 of 10000  #> Gibbs sampler iteration 7330 of 10000  #> Gibbs sampler iteration 7340 of 10000  #> Gibbs sampler iteration 7350 of 10000  #> Gibbs sampler iteration 7360 of 10000  #> Gibbs sampler iteration 7370 of 10000  #> Gibbs sampler iteration 7380 of 10000  #> Gibbs sampler iteration 7390 of 10000  #> Gibbs sampler iteration 7400 of 10000  #> Gibbs sampler iteration 7410 of 10000  #> Gibbs sampler iteration 7420 of 10000  #> Gibbs sampler iteration 7430 of 10000  #> Gibbs sampler iteration 7440 of 10000  #> Gibbs sampler iteration 7450 of 10000  #> Gibbs sampler iteration 7460 of 10000  #> Gibbs sampler iteration 7470 of 10000  #> Gibbs sampler iteration 7480 of 10000  #> Gibbs sampler iteration 7490 of 10000  #> Gibbs sampler iteration 7500 of 10000  #> Gibbs sampler iteration 7510 of 10000  #> Gibbs sampler iteration 7520 of 10000  #> Gibbs sampler iteration 7530 of 10000  #> Gibbs sampler iteration 7540 of 10000  #> Gibbs sampler iteration 7550 of 10000  #> Gibbs sampler iteration 7560 of 10000  #> Gibbs sampler iteration 7570 of 10000  #> Gibbs sampler iteration 7580 of 10000  #> Gibbs sampler iteration 7590 of 10000  #> Gibbs sampler iteration 7600 of 10000  #> Gibbs sampler iteration 7610 of 10000  #> Gibbs sampler iteration 7620 of 10000  #> Gibbs sampler iteration 7630 of 10000  #> Gibbs sampler iteration 7640 of 10000  #> Gibbs sampler iteration 7650 of 10000  #> Gibbs sampler iteration 7660 of 10000  #> Gibbs sampler iteration 7670 of 10000  #> Gibbs sampler iteration 7680 of 10000  #> Gibbs sampler iteration 7690 of 10000  #> Gibbs sampler iteration 7700 of 10000  #> Gibbs sampler iteration 7710 of 10000  #> Gibbs sampler iteration 7720 of 10000  #> Gibbs sampler iteration 7730 of 10000  #> Gibbs sampler iteration 7740 of 10000  #> Gibbs sampler iteration 7750 of 10000  #> Gibbs sampler iteration 7760 of 10000  #> Gibbs sampler iteration 7770 of 10000  #> Gibbs sampler iteration 7780 of 10000  #> Gibbs sampler iteration 7790 of 10000  #> Gibbs sampler iteration 7800 of 10000  #> Gibbs sampler iteration 7810 of 10000  #> Gibbs sampler iteration 7820 of 10000  #> Gibbs sampler iteration 7830 of 10000  #> Gibbs sampler iteration 7840 of 10000  #> Gibbs sampler iteration 7850 of 10000  #> Gibbs sampler iteration 7860 of 10000  #> Gibbs sampler iteration 7870 of 10000  #> Gibbs sampler iteration 7880 of 10000  #> Gibbs sampler iteration 7890 of 10000  #> Gibbs sampler iteration 7900 of 10000  #> Gibbs sampler iteration 7910 of 10000  #> Gibbs sampler iteration 7920 of 10000  #> Gibbs sampler iteration 7930 of 10000  #> Gibbs sampler iteration 7940 of 10000  #> Gibbs sampler iteration 7950 of 10000  #> Gibbs sampler iteration 7960 of 10000  #> Gibbs sampler iteration 7970 of 10000  #> Gibbs sampler iteration 7980 of 10000  #> Gibbs sampler iteration 7990 of 10000  #> Gibbs sampler iteration 8000 of 10000  #> Gibbs sampler iteration 8010 of 10000  #> Gibbs sampler iteration 8020 of 10000  #> Gibbs sampler iteration 8030 of 10000  #> Gibbs sampler iteration 8040 of 10000  #> Gibbs sampler iteration 8050 of 10000  #> Gibbs sampler iteration 8060 of 10000  #> Gibbs sampler iteration 8070 of 10000  #> Gibbs sampler iteration 8080 of 10000  #> Gibbs sampler iteration 8090 of 10000  #> Gibbs sampler iteration 8100 of 10000  #> Gibbs sampler iteration 8110 of 10000  #> Gibbs sampler iteration 8120 of 10000  #> Gibbs sampler iteration 8130 of 10000  #> Gibbs sampler iteration 8140 of 10000  #> Gibbs sampler iteration 8150 of 10000  #> Gibbs sampler iteration 8160 of 10000  #> Gibbs sampler iteration 8170 of 10000  #> Gibbs sampler iteration 8180 of 10000  #> Gibbs sampler iteration 8190 of 10000  #> Gibbs sampler iteration 8200 of 10000  #> Gibbs sampler iteration 8210 of 10000  #> Gibbs sampler iteration 8220 of 10000  #> Gibbs sampler iteration 8230 of 10000  #> Gibbs sampler iteration 8240 of 10000  #> Gibbs sampler iteration 8250 of 10000  #> Gibbs sampler iteration 8260 of 10000  #> Gibbs sampler iteration 8270 of 10000  #> Gibbs sampler iteration 8280 of 10000  #> Gibbs sampler iteration 8290 of 10000  #> Gibbs sampler iteration 8300 of 10000  #> Gibbs sampler iteration 8310 of 10000  #> Gibbs sampler iteration 8320 of 10000  #> Gibbs sampler iteration 8330 of 10000  #> Gibbs sampler iteration 8340 of 10000  #> Gibbs sampler iteration 8350 of 10000  #> Gibbs sampler iteration 8360 of 10000  #> Gibbs sampler iteration 8370 of 10000  #> Gibbs sampler iteration 8380 of 10000  #> Gibbs sampler iteration 8390 of 10000  #> Gibbs sampler iteration 8400 of 10000  #> Gibbs sampler iteration 8410 of 10000  #> Gibbs sampler iteration 8420 of 10000  #> Gibbs sampler iteration 8430 of 10000  #> Gibbs sampler iteration 8440 of 10000  #> Gibbs sampler iteration 8450 of 10000  #> Gibbs sampler iteration 8460 of 10000  #> Gibbs sampler iteration 8470 of 10000  #> Gibbs sampler iteration 8480 of 10000  #> Gibbs sampler iteration 8490 of 10000  #> Gibbs sampler iteration 8500 of 10000  #> Gibbs sampler iteration 8510 of 10000  #> Gibbs sampler iteration 8520 of 10000  #> Gibbs sampler iteration 8530 of 10000  #> Gibbs sampler iteration 8540 of 10000  #> Gibbs sampler iteration 8550 of 10000  #> Gibbs sampler iteration 8560 of 10000  #> Gibbs sampler iteration 8570 of 10000  #> Gibbs sampler iteration 8580 of 10000  #> Gibbs sampler iteration 8590 of 10000  #> Gibbs sampler iteration 8600 of 10000  #> Gibbs sampler iteration 8610 of 10000  #> Gibbs sampler iteration 8620 of 10000  #> Gibbs sampler iteration 8630 of 10000  #> Gibbs sampler iteration 8640 of 10000  #> Gibbs sampler iteration 8650 of 10000  #> Gibbs sampler iteration 8660 of 10000  #> Gibbs sampler iteration 8670 of 10000  #> Gibbs sampler iteration 8680 of 10000  #> Gibbs sampler iteration 8690 of 10000  #> Gibbs sampler iteration 8700 of 10000  #> Gibbs sampler iteration 8710 of 10000  #> Gibbs sampler iteration 8720 of 10000  #> Gibbs sampler iteration 8730 of 10000  #> Gibbs sampler iteration 8740 of 10000  #> Gibbs sampler iteration 8750 of 10000  #> Gibbs sampler iteration 8760 of 10000  #> Gibbs sampler iteration 8770 of 10000  #> Gibbs sampler iteration 8780 of 10000  #> Gibbs sampler iteration 8790 of 10000  #> Gibbs sampler iteration 8800 of 10000  #> Gibbs sampler iteration 8810 of 10000  #> Gibbs sampler iteration 8820 of 10000  #> Gibbs sampler iteration 8830 of 10000  #> Gibbs sampler iteration 8840 of 10000  #> Gibbs sampler iteration 8850 of 10000  #> Gibbs sampler iteration 8860 of 10000  #> Gibbs sampler iteration 8870 of 10000  #> Gibbs sampler iteration 8880 of 10000  #> Gibbs sampler iteration 8890 of 10000  #> Gibbs sampler iteration 8900 of 10000  #> Gibbs sampler iteration 8910 of 10000  #> Gibbs sampler iteration 8920 of 10000  #> Gibbs sampler iteration 8930 of 10000  #> Gibbs sampler iteration 8940 of 10000  #> Gibbs sampler iteration 8950 of 10000  #> Gibbs sampler iteration 8960 of 10000  #> Gibbs sampler iteration 8970 of 10000  #> Gibbs sampler iteration 8980 of 10000  #> Gibbs sampler iteration 8990 of 10000  #> Gibbs sampler iteration 9000 of 10000  #> Gibbs sampler iteration 9010 of 10000  #> Gibbs sampler iteration 9020 of 10000  #> Gibbs sampler iteration 9030 of 10000  #> Gibbs sampler iteration 9040 of 10000  #> Gibbs sampler iteration 9050 of 10000  #> Gibbs sampler iteration 9060 of 10000  #> Gibbs sampler iteration 9070 of 10000  #> Gibbs sampler iteration 9080 of 10000  #> Gibbs sampler iteration 9090 of 10000  #> Gibbs sampler iteration 9100 of 10000  #> Gibbs sampler iteration 9110 of 10000  #> Gibbs sampler iteration 9120 of 10000  #> Gibbs sampler iteration 9130 of 10000  #> Gibbs sampler iteration 9140 of 10000  #> Gibbs sampler iteration 9150 of 10000  #> Gibbs sampler iteration 9160 of 10000  #> Gibbs sampler iteration 9170 of 10000  #> Gibbs sampler iteration 9180 of 10000  #> Gibbs sampler iteration 9190 of 10000  #> Gibbs sampler iteration 9200 of 10000  #> Gibbs sampler iteration 9210 of 10000  #> Gibbs sampler iteration 9220 of 10000  #> Gibbs sampler iteration 9230 of 10000  #> Gibbs sampler iteration 9240 of 10000  #> Gibbs sampler iteration 9250 of 10000  #> Gibbs sampler iteration 9260 of 10000  #> Gibbs sampler iteration 9270 of 10000  #> Gibbs sampler iteration 9280 of 10000  #> Gibbs sampler iteration 9290 of 10000  #> Gibbs sampler iteration 9300 of 10000  #> Gibbs sampler iteration 9310 of 10000  #> Gibbs sampler iteration 9320 of 10000  #> Gibbs sampler iteration 9330 of 10000  #> Gibbs sampler iteration 9340 of 10000  #> Gibbs sampler iteration 9350 of 10000  #> Gibbs sampler iteration 9360 of 10000  #> Gibbs sampler iteration 9370 of 10000  #> Gibbs sampler iteration 9380 of 10000  #> Gibbs sampler iteration 9390 of 10000  #> Gibbs sampler iteration 9400 of 10000  #> Gibbs sampler iteration 9410 of 10000  #> Gibbs sampler iteration 9420 of 10000  #> Gibbs sampler iteration 9430 of 10000  #> Gibbs sampler iteration 9440 of 10000  #> Gibbs sampler iteration 9450 of 10000  #> Gibbs sampler iteration 9460 of 10000  #> Gibbs sampler iteration 9470 of 10000  #> Gibbs sampler iteration 9480 of 10000  #> Gibbs sampler iteration 9490 of 10000  #> Gibbs sampler iteration 9500 of 10000  #> Gibbs sampler iteration 9510 of 10000  #> Gibbs sampler iteration 9520 of 10000  #> Gibbs sampler iteration 9530 of 10000  #> Gibbs sampler iteration 9540 of 10000  #> Gibbs sampler iteration 9550 of 10000  #> Gibbs sampler iteration 9560 of 10000  #> Gibbs sampler iteration 9570 of 10000  #> Gibbs sampler iteration 9580 of 10000  #> Gibbs sampler iteration 9590 of 10000  #> Gibbs sampler iteration 9600 of 10000  #> Gibbs sampler iteration 9610 of 10000  #> Gibbs sampler iteration 9620 of 10000  #> Gibbs sampler iteration 9630 of 10000  #> Gibbs sampler iteration 9640 of 10000  #> Gibbs sampler iteration 9650 of 10000  #> Gibbs sampler iteration 9660 of 10000  #> Gibbs sampler iteration 9670 of 10000  #> Gibbs sampler iteration 9680 of 10000  #> Gibbs sampler iteration 9690 of 10000  #> Gibbs sampler iteration 9700 of 10000  #> Gibbs sampler iteration 9710 of 10000  #> Gibbs sampler iteration 9720 of 10000  #> Gibbs sampler iteration 9730 of 10000  #> Gibbs sampler iteration 9740 of 10000  #> Gibbs sampler iteration 9750 of 10000  #> Gibbs sampler iteration 9760 of 10000  #> Gibbs sampler iteration 9770 of 10000  #> Gibbs sampler iteration 9780 of 10000  #> Gibbs sampler iteration 9790 of 10000  #> Gibbs sampler iteration 9800 of 10000  #> Gibbs sampler iteration 9810 of 10000  #> Gibbs sampler iteration 9820 of 10000  #> Gibbs sampler iteration 9830 of 10000  #> Gibbs sampler iteration 9840 of 10000  #> Gibbs sampler iteration 9850 of 10000  #> Gibbs sampler iteration 9860 of 10000  #> Gibbs sampler iteration 9870 of 10000  #> Gibbs sampler iteration 9880 of 10000  #> Gibbs sampler iteration 9890 of 10000  #> Gibbs sampler iteration 9900 of 10000  #> Gibbs sampler iteration 9910 of 10000  #> Gibbs sampler iteration 9920 of 10000  #> Gibbs sampler iteration 9930 of 10000  #> Gibbs sampler iteration 9940 of 10000  #> Gibbs sampler iteration 9950 of 10000  #> Gibbs sampler iteration 9960 of 10000  #> Gibbs sampler iteration 9970 of 10000  #> Gibbs sampler iteration 9980 of 10000  #> Gibbs sampler iteration 9990 of 10000  #> Gibbs sampler iteration 10000 of 10000  point_estimates(model) #> RprobitB model parameter #>  #> alpha : double vector of length 2  #>  #> 0.1527 -0.2265 #>  #> C : NA #>  #> s : NA #>  #> b : NA #>  #> Omega : NA #>  #> Sigma : 1 #>  #> Sigma_full : 2 x 2 matrix of doubles  #>  #>      [,1] [,2] #> [1,]    2    1 #> [2,]    1    1 #>  #>  #> beta : NA #>  #> z : NA #>  point_estimates(model, FUN = median) #> RprobitB model parameter #>  #> alpha : double vector of length 2  #>  #> 0.1522 -0.2245 #>  #> C : NA #>  #> s : NA #>  #> b : NA #>  #> Omega : NA #>  #> Sigma : 1 #>  #> Sigma_full : 2 x 2 matrix of doubles  #>  #>      [,1] [,2] #> [1,]    2    1 #> [2,]    1    1 #>  #>  #> beta : NA #>  #> z : NA #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/posterior_pars.html","id":null,"dir":"Reference","previous_headings":"","what":"Parameter sets from posterior samples — posterior_pars","title":"Parameter sets from posterior samples — posterior_pars","text":"function builds parameter sets normalized, burned thinned posterior samples.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/posterior_pars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter sets from posterior samples — posterior_pars","text":"","code":"posterior_pars(x)"},{"path":"https://loelschlaeger.de/RprobitB/reference/posterior_pars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter sets from posterior samples — posterior_pars","text":"x object class RprobitB_fit.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/posterior_pars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parameter sets from posterior samples — posterior_pars","text":"list RprobitB_parameter-objects.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/posterior_pars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parameter sets from posterior samples — posterior_pars","text":"","code":"data(\"model_train\", package = \"RprobitB\") pars <- RprobitB:::posterior_pars(model_train) pars[[1]] #> RprobitB model parameter #>  #> alpha : double vector of length 4  #>  #> -1 -27.2017 -3.6965 -14.0946 #>  #> C : NA #>  #> s : NA #>  #> b : NA #>  #> Omega : NA #>  #> Sigma : 606.7234 #>  #> Sigma_full : 2 x 2 matrix of doubles  #>  #>          [,1] [,2] #> [1,] 607.7234    1 #> [2,]   1.0000    1 #>  #>  #> beta : NA #>  #> z : NA #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/pprint.html","id":null,"dir":"Reference","previous_headings":"","what":"Print abbreviated matrices and vectors — pprint","title":"Print abbreviated matrices and vectors — pprint","text":"function prints abbreviated matrices vectors.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/pprint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print abbreviated matrices and vectors — pprint","text":"","code":"pprint(x, rowdots = 4, coldots = 4, digits = 4, name = NULL, desc = TRUE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/pprint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print abbreviated matrices and vectors — pprint","text":"x (numeric character) matrix vector. rowdots row number replaced dots. coldots column number replaced dots. digits x numeric, sets number decimal places. name Either NULL label x. printed desc = TRUE. desc Set TRUE print name dimension x.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/pprint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print abbreviated matrices and vectors — pprint","text":"Invisibly returns x.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/pprint.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Print abbreviated matrices and vectors — pprint","text":"function modified version pprint.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/pprint.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print abbreviated matrices and vectors — pprint","text":"","code":"RprobitB:::pprint(x = 1, name = \"single integer\") #> single integer : 1 RprobitB:::pprint(x = LETTERS[1:26], name = \"letters\") #> letters : character vector of length 26  #>  #> A B C ... Z RprobitB:::pprint(x = matrix(rnorm(100), ncol = 1),                   name = \"single column matrix\") #> single column matrix : 100 x 1 matrix of doubles  #>  #>           [,1] #> [1,]   -0.2807 #> [2,]   -0.4937 #> [3,]   -0.4492 #> ...        ... #> [100,] -1.5750 RprobitB:::pprint(x = matrix(1:100, nrow = 1), name = \"single row matrix\") #> single row matrix : 1 x 100 matrix of integers  #>  #>      [,1] [,2] [,3] ... [,100] #> [1,]    1    2    3 ...    100 RprobitB:::pprint(x = matrix(LETTERS[1:24], ncol = 6), name = \"big matrix\") #> big matrix : 4 x 6 matrix of characters  #>  #>      [,1] [,2] [,3] ... [,6] #> [1,]    A    E    I ...    U #> [2,]    B    F    J ...    V #> [3,]    C    G    K ...    W #> [4,]    D    H    L ...    X"},{"path":"https://loelschlaeger.de/RprobitB/reference/pred_acc.html","id":null,"dir":"Reference","previous_headings":"","what":"Prediction accuracy — pred_acc","title":"Prediction accuracy — pred_acc","text":"function computes prediction accuracy RprobitB_fit object. Prediction accuracy means share choices correctly predicted model, prediction based maximum choice probability.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/pred_acc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prediction accuracy — pred_acc","text":"","code":"pred_acc(x, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/pred_acc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prediction accuracy — pred_acc","text":"x object class RprobitB_fit. ... Optionally specify RprobitB_fit objects.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/pred_acc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prediction accuracy — pred_acc","text":"numeric.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/pred_acc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prediction accuracy — pred_acc","text":"","code":"data(\"model_train\", package = \"RprobitB\") pred_acc(model_train) #> [1] 0.6954592"},{"path":"https://loelschlaeger.de/RprobitB/reference/predict.RprobitB_fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Choice prediction — predict.RprobitB_fit","title":"Choice prediction — predict.RprobitB_fit","text":"function predicts discrete choice behavior","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/predict.RprobitB_fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Choice prediction — predict.RprobitB_fit","text":"","code":"# S3 method for RprobitB_fit predict(object, data = NULL, overview = TRUE, ...)"},{"path":"https://loelschlaeger.de/RprobitB/reference/predict.RprobitB_fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Choice prediction — predict.RprobitB_fit","text":"object object class RprobitB_fit. data Either NULL, using data object, object class RprobitB_data, example test part generated train_test, data frame custom choice characteristics. must structure choice_data used prepare_data. Missing columns NA values set 0. overview TRUE, returns confusion matrix. ... Ignored.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/predict.RprobitB_fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Choice prediction — predict.RprobitB_fit","text":"Either table overview = TRUE data frame otherwise.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/predict.RprobitB_fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Choice prediction — predict.RprobitB_fit","text":"Predictions made based maximum predicted probability choice alternative. See vignette choice prediction demonstration visualize model's sensitivity specificity means receiver operating characteristic (ROC) curve.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/predict.RprobitB_fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Choice prediction — predict.RprobitB_fit","text":"","code":"data <- simulate_choices(form = choice ~ cov, N = 10, T = 10, J = 2, seed = 1) data <- train_test(data, test_proportion = 0.5) model <- mcmc(data$train) #> Computing sufficient statistics 0 of 4  #> Computing sufficient statistics 1 of 4  #> Computing sufficient statistics 2 of 4  #> Computing sufficient statistics 3 of 4  #> Computing sufficient statistics 4 of 4  #> Gibbs sampler iteration 1 of 10000  #> Gibbs sampler iteration 10 of 10000  #> Gibbs sampler iteration 20 of 10000  #> Gibbs sampler iteration 30 of 10000  #> Gibbs sampler iteration 40 of 10000  #> Gibbs sampler iteration 50 of 10000  #> Gibbs sampler iteration 60 of 10000  #> Gibbs sampler iteration 70 of 10000  #> Gibbs sampler iteration 80 of 10000  #> Gibbs sampler iteration 90 of 10000  #> Gibbs sampler iteration 100 of 10000  #> Gibbs sampler iteration 110 of 10000  #> Gibbs sampler iteration 120 of 10000  #> Gibbs sampler iteration 130 of 10000  #> Gibbs sampler iteration 140 of 10000  #> Gibbs sampler iteration 150 of 10000  #> Gibbs sampler iteration 160 of 10000  #> Gibbs sampler iteration 170 of 10000  #> Gibbs sampler iteration 180 of 10000  #> Gibbs sampler iteration 190 of 10000  #> Gibbs sampler iteration 200 of 10000  #> Gibbs sampler iteration 210 of 10000  #> Gibbs sampler iteration 220 of 10000  #> Gibbs sampler iteration 230 of 10000  #> Gibbs sampler iteration 240 of 10000  #> Gibbs sampler iteration 250 of 10000  #> Gibbs sampler iteration 260 of 10000  #> Gibbs sampler iteration 270 of 10000  #> Gibbs sampler iteration 280 of 10000  #> Gibbs sampler iteration 290 of 10000  #> Gibbs sampler iteration 300 of 10000  #> Gibbs sampler iteration 310 of 10000  #> Gibbs sampler iteration 320 of 10000  #> Gibbs sampler iteration 330 of 10000  #> Gibbs sampler iteration 340 of 10000  #> Gibbs sampler iteration 350 of 10000  #> Gibbs sampler iteration 360 of 10000  #> Gibbs sampler iteration 370 of 10000  #> Gibbs sampler iteration 380 of 10000  #> Gibbs sampler iteration 390 of 10000  #> Gibbs sampler iteration 400 of 10000  #> Gibbs sampler iteration 410 of 10000  #> Gibbs sampler iteration 420 of 10000  #> Gibbs sampler iteration 430 of 10000  #> Gibbs sampler iteration 440 of 10000  #> Gibbs sampler iteration 450 of 10000  #> Gibbs sampler iteration 460 of 10000  #> Gibbs sampler iteration 470 of 10000  #> Gibbs sampler iteration 480 of 10000  #> Gibbs sampler iteration 490 of 10000  #> Gibbs sampler iteration 500 of 10000  #> Gibbs sampler iteration 510 of 10000  #> Gibbs sampler iteration 520 of 10000  #> Gibbs sampler iteration 530 of 10000  #> Gibbs sampler iteration 540 of 10000  #> Gibbs sampler iteration 550 of 10000  #> Gibbs sampler iteration 560 of 10000  #> Gibbs sampler iteration 570 of 10000  #> Gibbs sampler iteration 580 of 10000  #> Gibbs sampler iteration 590 of 10000  #> Gibbs sampler iteration 600 of 10000  #> Gibbs sampler iteration 610 of 10000  #> Gibbs sampler iteration 620 of 10000  #> Gibbs sampler iteration 630 of 10000  #> Gibbs sampler iteration 640 of 10000  #> Gibbs sampler iteration 650 of 10000  #> Gibbs sampler iteration 660 of 10000  #> Gibbs sampler iteration 670 of 10000  #> Gibbs sampler iteration 680 of 10000  #> Gibbs sampler iteration 690 of 10000  #> Gibbs sampler iteration 700 of 10000  #> Gibbs sampler iteration 710 of 10000  #> Gibbs sampler iteration 720 of 10000  #> Gibbs sampler iteration 730 of 10000  #> Gibbs sampler iteration 740 of 10000  #> Gibbs sampler iteration 750 of 10000  #> Gibbs sampler iteration 760 of 10000  #> Gibbs sampler iteration 770 of 10000  #> Gibbs sampler iteration 780 of 10000  #> Gibbs sampler iteration 790 of 10000  #> Gibbs sampler iteration 800 of 10000  #> Gibbs sampler iteration 810 of 10000  #> Gibbs sampler iteration 820 of 10000  #> Gibbs sampler iteration 830 of 10000  #> Gibbs sampler iteration 840 of 10000  #> Gibbs sampler iteration 850 of 10000  #> Gibbs sampler iteration 860 of 10000  #> Gibbs sampler iteration 870 of 10000  #> Gibbs sampler iteration 880 of 10000  #> Gibbs sampler iteration 890 of 10000  #> Gibbs sampler iteration 900 of 10000  #> Gibbs sampler iteration 910 of 10000  #> Gibbs sampler iteration 920 of 10000  #> Gibbs sampler iteration 930 of 10000  #> Gibbs sampler iteration 940 of 10000  #> Gibbs sampler iteration 950 of 10000  #> Gibbs sampler iteration 960 of 10000  #> Gibbs sampler iteration 970 of 10000  #> Gibbs sampler iteration 980 of 10000  #> Gibbs sampler iteration 990 of 10000  #> Gibbs sampler iteration 1000 of 10000  #> Gibbs sampler iteration 1010 of 10000  #> Gibbs sampler iteration 1020 of 10000  #> Gibbs sampler iteration 1030 of 10000  #> Gibbs sampler iteration 1040 of 10000  #> Gibbs sampler iteration 1050 of 10000  #> Gibbs sampler iteration 1060 of 10000  #> Gibbs sampler iteration 1070 of 10000  #> Gibbs sampler iteration 1080 of 10000  #> Gibbs sampler iteration 1090 of 10000  #> Gibbs sampler iteration 1100 of 10000  #> Gibbs sampler iteration 1110 of 10000  #> Gibbs sampler iteration 1120 of 10000  #> Gibbs sampler iteration 1130 of 10000  #> Gibbs sampler iteration 1140 of 10000  #> Gibbs sampler iteration 1150 of 10000  #> Gibbs sampler iteration 1160 of 10000  #> Gibbs sampler iteration 1170 of 10000  #> Gibbs sampler iteration 1180 of 10000  #> Gibbs sampler iteration 1190 of 10000  #> Gibbs sampler iteration 1200 of 10000  #> Gibbs sampler iteration 1210 of 10000  #> Gibbs sampler iteration 1220 of 10000  #> Gibbs sampler iteration 1230 of 10000  #> Gibbs sampler iteration 1240 of 10000  #> Gibbs sampler iteration 1250 of 10000  #> Gibbs sampler iteration 1260 of 10000  #> Gibbs sampler iteration 1270 of 10000  #> Gibbs sampler iteration 1280 of 10000  #> Gibbs sampler iteration 1290 of 10000  #> Gibbs sampler iteration 1300 of 10000  #> Gibbs sampler iteration 1310 of 10000  #> Gibbs sampler iteration 1320 of 10000  #> Gibbs sampler iteration 1330 of 10000  #> Gibbs sampler iteration 1340 of 10000  #> Gibbs sampler iteration 1350 of 10000  #> Gibbs sampler iteration 1360 of 10000  #> Gibbs sampler iteration 1370 of 10000  #> Gibbs sampler iteration 1380 of 10000  #> Gibbs sampler iteration 1390 of 10000  #> Gibbs sampler iteration 1400 of 10000  #> Gibbs sampler iteration 1410 of 10000  #> Gibbs sampler iteration 1420 of 10000  #> Gibbs sampler iteration 1430 of 10000  #> Gibbs sampler iteration 1440 of 10000  #> Gibbs sampler iteration 1450 of 10000  #> Gibbs sampler iteration 1460 of 10000  #> Gibbs sampler iteration 1470 of 10000  #> Gibbs sampler iteration 1480 of 10000  #> Gibbs sampler iteration 1490 of 10000  #> Gibbs sampler iteration 1500 of 10000  #> Gibbs sampler iteration 1510 of 10000  #> Gibbs sampler iteration 1520 of 10000  #> Gibbs sampler iteration 1530 of 10000  #> Gibbs sampler iteration 1540 of 10000  #> Gibbs sampler iteration 1550 of 10000  #> Gibbs sampler iteration 1560 of 10000  #> Gibbs sampler iteration 1570 of 10000  #> Gibbs sampler iteration 1580 of 10000  #> Gibbs sampler iteration 1590 of 10000  #> Gibbs sampler iteration 1600 of 10000  #> Gibbs sampler iteration 1610 of 10000  #> Gibbs sampler iteration 1620 of 10000  #> Gibbs sampler iteration 1630 of 10000  #> Gibbs sampler iteration 1640 of 10000  #> Gibbs sampler iteration 1650 of 10000  #> Gibbs sampler iteration 1660 of 10000  #> Gibbs sampler iteration 1670 of 10000  #> Gibbs sampler iteration 1680 of 10000  #> Gibbs sampler iteration 1690 of 10000  #> Gibbs sampler iteration 1700 of 10000  #> Gibbs sampler iteration 1710 of 10000  #> Gibbs sampler iteration 1720 of 10000  #> Gibbs sampler iteration 1730 of 10000  #> Gibbs sampler iteration 1740 of 10000  #> Gibbs sampler iteration 1750 of 10000  #> Gibbs sampler iteration 1760 of 10000  #> Gibbs sampler iteration 1770 of 10000  #> Gibbs sampler iteration 1780 of 10000  #> Gibbs sampler iteration 1790 of 10000  #> Gibbs sampler iteration 1800 of 10000  #> Gibbs sampler iteration 1810 of 10000  #> Gibbs sampler iteration 1820 of 10000  #> Gibbs sampler iteration 1830 of 10000  #> Gibbs sampler iteration 1840 of 10000  #> Gibbs sampler iteration 1850 of 10000  #> Gibbs sampler iteration 1860 of 10000  #> Gibbs sampler iteration 1870 of 10000  #> Gibbs sampler iteration 1880 of 10000  #> Gibbs sampler iteration 1890 of 10000  #> Gibbs sampler iteration 1900 of 10000  #> Gibbs sampler iteration 1910 of 10000  #> Gibbs sampler iteration 1920 of 10000  #> Gibbs sampler iteration 1930 of 10000  #> Gibbs sampler iteration 1940 of 10000  #> Gibbs sampler iteration 1950 of 10000  #> Gibbs sampler iteration 1960 of 10000  #> Gibbs sampler iteration 1970 of 10000  #> Gibbs sampler iteration 1980 of 10000  #> Gibbs sampler iteration 1990 of 10000  #> Gibbs sampler iteration 2000 of 10000  #> Gibbs sampler iteration 2010 of 10000  #> Gibbs sampler iteration 2020 of 10000  #> Gibbs sampler iteration 2030 of 10000  #> Gibbs sampler iteration 2040 of 10000  #> Gibbs sampler iteration 2050 of 10000  #> Gibbs sampler iteration 2060 of 10000  #> Gibbs sampler iteration 2070 of 10000  #> Gibbs sampler iteration 2080 of 10000  #> Gibbs sampler iteration 2090 of 10000  #> Gibbs sampler iteration 2100 of 10000  #> Gibbs sampler iteration 2110 of 10000  #> Gibbs sampler iteration 2120 of 10000  #> Gibbs sampler iteration 2130 of 10000  #> Gibbs sampler iteration 2140 of 10000  #> Gibbs sampler iteration 2150 of 10000  #> Gibbs sampler iteration 2160 of 10000  #> Gibbs sampler iteration 2170 of 10000  #> Gibbs sampler iteration 2180 of 10000  #> Gibbs sampler iteration 2190 of 10000  #> Gibbs sampler iteration 2200 of 10000  #> Gibbs sampler iteration 2210 of 10000  #> Gibbs sampler iteration 2220 of 10000  #> Gibbs sampler iteration 2230 of 10000  #> Gibbs sampler iteration 2240 of 10000  #> Gibbs sampler iteration 2250 of 10000  #> Gibbs sampler iteration 2260 of 10000  #> Gibbs sampler iteration 2270 of 10000  #> Gibbs sampler iteration 2280 of 10000  #> Gibbs sampler iteration 2290 of 10000  #> Gibbs sampler iteration 2300 of 10000  #> Gibbs sampler iteration 2310 of 10000  #> Gibbs sampler iteration 2320 of 10000  #> Gibbs sampler iteration 2330 of 10000  #> Gibbs sampler iteration 2340 of 10000  #> Gibbs sampler iteration 2350 of 10000  #> Gibbs sampler iteration 2360 of 10000  #> Gibbs sampler iteration 2370 of 10000  #> Gibbs sampler iteration 2380 of 10000  #> Gibbs sampler iteration 2390 of 10000  #> Gibbs sampler iteration 2400 of 10000  #> Gibbs sampler iteration 2410 of 10000  #> Gibbs sampler iteration 2420 of 10000  #> Gibbs sampler iteration 2430 of 10000  #> Gibbs sampler iteration 2440 of 10000  #> Gibbs sampler iteration 2450 of 10000  #> Gibbs sampler iteration 2460 of 10000  #> Gibbs sampler iteration 2470 of 10000  #> Gibbs sampler iteration 2480 of 10000  #> Gibbs sampler iteration 2490 of 10000  #> Gibbs sampler iteration 2500 of 10000  #> Gibbs sampler iteration 2510 of 10000  #> Gibbs sampler iteration 2520 of 10000  #> Gibbs sampler iteration 2530 of 10000  #> Gibbs sampler iteration 2540 of 10000  #> Gibbs sampler iteration 2550 of 10000  #> Gibbs sampler iteration 2560 of 10000  #> Gibbs sampler iteration 2570 of 10000  #> Gibbs sampler iteration 2580 of 10000  #> Gibbs sampler iteration 2590 of 10000  #> Gibbs sampler iteration 2600 of 10000  #> Gibbs sampler iteration 2610 of 10000  #> Gibbs sampler iteration 2620 of 10000  #> Gibbs sampler iteration 2630 of 10000  #> Gibbs sampler iteration 2640 of 10000  #> Gibbs sampler iteration 2650 of 10000  #> Gibbs sampler iteration 2660 of 10000  #> Gibbs sampler iteration 2670 of 10000  #> Gibbs sampler iteration 2680 of 10000  #> Gibbs sampler iteration 2690 of 10000  #> Gibbs sampler iteration 2700 of 10000  #> Gibbs sampler iteration 2710 of 10000  #> Gibbs sampler iteration 2720 of 10000  #> Gibbs sampler iteration 2730 of 10000  #> Gibbs sampler iteration 2740 of 10000  #> Gibbs sampler iteration 2750 of 10000  #> Gibbs sampler iteration 2760 of 10000  #> Gibbs sampler iteration 2770 of 10000  #> Gibbs sampler iteration 2780 of 10000  #> Gibbs sampler iteration 2790 of 10000  #> Gibbs sampler iteration 2800 of 10000  #> Gibbs sampler iteration 2810 of 10000  #> Gibbs sampler iteration 2820 of 10000  #> Gibbs sampler iteration 2830 of 10000  #> Gibbs sampler iteration 2840 of 10000  #> Gibbs sampler iteration 2850 of 10000  #> Gibbs sampler iteration 2860 of 10000  #> Gibbs sampler iteration 2870 of 10000  #> Gibbs sampler iteration 2880 of 10000  #> Gibbs sampler iteration 2890 of 10000  #> Gibbs sampler iteration 2900 of 10000  #> Gibbs sampler iteration 2910 of 10000  #> Gibbs sampler iteration 2920 of 10000  #> Gibbs sampler iteration 2930 of 10000  #> Gibbs sampler iteration 2940 of 10000  #> Gibbs sampler iteration 2950 of 10000  #> Gibbs sampler iteration 2960 of 10000  #> Gibbs sampler iteration 2970 of 10000  #> Gibbs sampler iteration 2980 of 10000  #> Gibbs sampler iteration 2990 of 10000  #> Gibbs sampler iteration 3000 of 10000  #> Gibbs sampler iteration 3010 of 10000  #> Gibbs sampler iteration 3020 of 10000  #> Gibbs sampler iteration 3030 of 10000  #> Gibbs sampler iteration 3040 of 10000  #> Gibbs sampler iteration 3050 of 10000  #> Gibbs sampler iteration 3060 of 10000  #> Gibbs sampler iteration 3070 of 10000  #> Gibbs sampler iteration 3080 of 10000  #> Gibbs sampler iteration 3090 of 10000  #> Gibbs sampler iteration 3100 of 10000  #> Gibbs sampler iteration 3110 of 10000  #> Gibbs sampler iteration 3120 of 10000  #> Gibbs sampler iteration 3130 of 10000  #> Gibbs sampler iteration 3140 of 10000  #> Gibbs sampler iteration 3150 of 10000  #> Gibbs sampler iteration 3160 of 10000  #> Gibbs sampler iteration 3170 of 10000  #> Gibbs sampler iteration 3180 of 10000  #> Gibbs sampler iteration 3190 of 10000  #> Gibbs sampler iteration 3200 of 10000  #> Gibbs sampler iteration 3210 of 10000  #> Gibbs sampler iteration 3220 of 10000  #> Gibbs sampler iteration 3230 of 10000  #> Gibbs sampler iteration 3240 of 10000  #> Gibbs sampler iteration 3250 of 10000  #> Gibbs sampler iteration 3260 of 10000  #> Gibbs sampler iteration 3270 of 10000  #> Gibbs sampler iteration 3280 of 10000  #> Gibbs sampler iteration 3290 of 10000  #> Gibbs sampler iteration 3300 of 10000  #> Gibbs sampler iteration 3310 of 10000  #> Gibbs sampler iteration 3320 of 10000  #> Gibbs sampler iteration 3330 of 10000  #> Gibbs sampler iteration 3340 of 10000  #> Gibbs sampler iteration 3350 of 10000  #> Gibbs sampler iteration 3360 of 10000  #> Gibbs sampler iteration 3370 of 10000  #> Gibbs sampler iteration 3380 of 10000  #> Gibbs sampler iteration 3390 of 10000  #> Gibbs sampler iteration 3400 of 10000  #> Gibbs sampler iteration 3410 of 10000  #> Gibbs sampler iteration 3420 of 10000  #> Gibbs sampler iteration 3430 of 10000  #> Gibbs sampler iteration 3440 of 10000  #> Gibbs sampler iteration 3450 of 10000  #> Gibbs sampler iteration 3460 of 10000  #> Gibbs sampler iteration 3470 of 10000  #> Gibbs sampler iteration 3480 of 10000  #> Gibbs sampler iteration 3490 of 10000  #> Gibbs sampler iteration 3500 of 10000  #> Gibbs sampler iteration 3510 of 10000  #> Gibbs sampler iteration 3520 of 10000  #> Gibbs sampler iteration 3530 of 10000  #> Gibbs sampler iteration 3540 of 10000  #> Gibbs sampler iteration 3550 of 10000  #> Gibbs sampler iteration 3560 of 10000  #> Gibbs sampler iteration 3570 of 10000  #> Gibbs sampler iteration 3580 of 10000  #> Gibbs sampler iteration 3590 of 10000  #> Gibbs sampler iteration 3600 of 10000  #> Gibbs sampler iteration 3610 of 10000  #> Gibbs sampler iteration 3620 of 10000  #> Gibbs sampler iteration 3630 of 10000  #> Gibbs sampler iteration 3640 of 10000  #> Gibbs sampler iteration 3650 of 10000  #> Gibbs sampler iteration 3660 of 10000  #> Gibbs sampler iteration 3670 of 10000  #> Gibbs sampler iteration 3680 of 10000  #> Gibbs sampler iteration 3690 of 10000  #> Gibbs sampler iteration 3700 of 10000  #> Gibbs sampler iteration 3710 of 10000  #> Gibbs sampler iteration 3720 of 10000  #> Gibbs sampler iteration 3730 of 10000  #> Gibbs sampler iteration 3740 of 10000  #> Gibbs sampler iteration 3750 of 10000  #> Gibbs sampler iteration 3760 of 10000  #> Gibbs sampler iteration 3770 of 10000  #> Gibbs sampler iteration 3780 of 10000  #> Gibbs sampler iteration 3790 of 10000  #> Gibbs sampler iteration 3800 of 10000  #> Gibbs sampler iteration 3810 of 10000  #> Gibbs sampler iteration 3820 of 10000  #> Gibbs sampler iteration 3830 of 10000  #> Gibbs sampler iteration 3840 of 10000  #> Gibbs sampler iteration 3850 of 10000  #> Gibbs sampler iteration 3860 of 10000  #> Gibbs sampler iteration 3870 of 10000  #> Gibbs sampler iteration 3880 of 10000  #> Gibbs sampler iteration 3890 of 10000  #> Gibbs sampler iteration 3900 of 10000  #> Gibbs sampler iteration 3910 of 10000  #> Gibbs sampler iteration 3920 of 10000  #> Gibbs sampler iteration 3930 of 10000  #> Gibbs sampler iteration 3940 of 10000  #> Gibbs sampler iteration 3950 of 10000  #> Gibbs sampler iteration 3960 of 10000  #> Gibbs sampler iteration 3970 of 10000  #> Gibbs sampler iteration 3980 of 10000  #> Gibbs sampler iteration 3990 of 10000  #> Gibbs sampler iteration 4000 of 10000  #> Gibbs sampler iteration 4010 of 10000  #> Gibbs sampler iteration 4020 of 10000  #> Gibbs sampler iteration 4030 of 10000  #> Gibbs sampler iteration 4040 of 10000  #> Gibbs sampler iteration 4050 of 10000  #> Gibbs sampler iteration 4060 of 10000  #> Gibbs sampler iteration 4070 of 10000  #> Gibbs sampler iteration 4080 of 10000  #> Gibbs sampler iteration 4090 of 10000  #> Gibbs sampler iteration 4100 of 10000  #> Gibbs sampler iteration 4110 of 10000  #> Gibbs sampler iteration 4120 of 10000  #> Gibbs sampler iteration 4130 of 10000  #> Gibbs sampler iteration 4140 of 10000  #> Gibbs sampler iteration 4150 of 10000  #> Gibbs sampler iteration 4160 of 10000  #> Gibbs sampler iteration 4170 of 10000  #> Gibbs sampler iteration 4180 of 10000  #> Gibbs sampler iteration 4190 of 10000  #> Gibbs sampler iteration 4200 of 10000  #> Gibbs sampler iteration 4210 of 10000  #> Gibbs sampler iteration 4220 of 10000  #> Gibbs sampler iteration 4230 of 10000  #> Gibbs sampler iteration 4240 of 10000  #> Gibbs sampler iteration 4250 of 10000  #> Gibbs sampler iteration 4260 of 10000  #> Gibbs sampler iteration 4270 of 10000  #> Gibbs sampler iteration 4280 of 10000  #> Gibbs sampler iteration 4290 of 10000  #> Gibbs sampler iteration 4300 of 10000  #> Gibbs sampler iteration 4310 of 10000  #> Gibbs sampler iteration 4320 of 10000  #> Gibbs sampler iteration 4330 of 10000  #> Gibbs sampler iteration 4340 of 10000  #> Gibbs sampler iteration 4350 of 10000  #> Gibbs sampler iteration 4360 of 10000  #> Gibbs sampler iteration 4370 of 10000  #> Gibbs sampler iteration 4380 of 10000  #> Gibbs sampler iteration 4390 of 10000  #> Gibbs sampler iteration 4400 of 10000  #> Gibbs sampler iteration 4410 of 10000  #> Gibbs sampler iteration 4420 of 10000  #> Gibbs sampler iteration 4430 of 10000  #> Gibbs sampler iteration 4440 of 10000  #> Gibbs sampler iteration 4450 of 10000  #> Gibbs sampler iteration 4460 of 10000  #> Gibbs sampler iteration 4470 of 10000  #> Gibbs sampler iteration 4480 of 10000  #> Gibbs sampler iteration 4490 of 10000  #> Gibbs sampler iteration 4500 of 10000  #> Gibbs sampler iteration 4510 of 10000  #> Gibbs sampler iteration 4520 of 10000  #> Gibbs sampler iteration 4530 of 10000  #> Gibbs sampler iteration 4540 of 10000  #> Gibbs sampler iteration 4550 of 10000  #> Gibbs sampler iteration 4560 of 10000  #> Gibbs sampler iteration 4570 of 10000  #> Gibbs sampler iteration 4580 of 10000  #> Gibbs sampler iteration 4590 of 10000  #> Gibbs sampler iteration 4600 of 10000  #> Gibbs sampler iteration 4610 of 10000  #> Gibbs sampler iteration 4620 of 10000  #> Gibbs sampler iteration 4630 of 10000  #> Gibbs sampler iteration 4640 of 10000  #> Gibbs sampler iteration 4650 of 10000  #> Gibbs sampler iteration 4660 of 10000  #> Gibbs sampler iteration 4670 of 10000  #> Gibbs sampler iteration 4680 of 10000  #> Gibbs sampler iteration 4690 of 10000  #> Gibbs sampler iteration 4700 of 10000  #> Gibbs sampler iteration 4710 of 10000  #> Gibbs sampler iteration 4720 of 10000  #> Gibbs sampler iteration 4730 of 10000  #> Gibbs sampler iteration 4740 of 10000  #> Gibbs sampler iteration 4750 of 10000  #> Gibbs sampler iteration 4760 of 10000  #> Gibbs sampler iteration 4770 of 10000  #> Gibbs sampler iteration 4780 of 10000  #> Gibbs sampler iteration 4790 of 10000  #> Gibbs sampler iteration 4800 of 10000  #> Gibbs sampler iteration 4810 of 10000  #> Gibbs sampler iteration 4820 of 10000  #> Gibbs sampler iteration 4830 of 10000  #> Gibbs sampler iteration 4840 of 10000  #> Gibbs sampler iteration 4850 of 10000  #> Gibbs sampler iteration 4860 of 10000  #> Gibbs sampler iteration 4870 of 10000  #> Gibbs sampler iteration 4880 of 10000  #> Gibbs sampler iteration 4890 of 10000  #> Gibbs sampler iteration 4900 of 10000  #> Gibbs sampler iteration 4910 of 10000  #> Gibbs sampler iteration 4920 of 10000  #> Gibbs sampler iteration 4930 of 10000  #> Gibbs sampler iteration 4940 of 10000  #> Gibbs sampler iteration 4950 of 10000  #> Gibbs sampler iteration 4960 of 10000  #> Gibbs sampler iteration 4970 of 10000  #> Gibbs sampler iteration 4980 of 10000  #> Gibbs sampler iteration 4990 of 10000  #> Gibbs sampler iteration 5000 of 10000  #> Gibbs sampler iteration 5010 of 10000  #> Gibbs sampler iteration 5020 of 10000  #> Gibbs sampler iteration 5030 of 10000  #> Gibbs sampler iteration 5040 of 10000  #> Gibbs sampler iteration 5050 of 10000  #> Gibbs sampler iteration 5060 of 10000  #> Gibbs sampler iteration 5070 of 10000  #> Gibbs sampler iteration 5080 of 10000  #> Gibbs sampler iteration 5090 of 10000  #> Gibbs sampler iteration 5100 of 10000  #> Gibbs sampler iteration 5110 of 10000  #> Gibbs sampler iteration 5120 of 10000  #> Gibbs sampler iteration 5130 of 10000  #> Gibbs sampler iteration 5140 of 10000  #> Gibbs sampler iteration 5150 of 10000  #> Gibbs sampler iteration 5160 of 10000  #> Gibbs sampler iteration 5170 of 10000  #> Gibbs sampler iteration 5180 of 10000  #> Gibbs sampler iteration 5190 of 10000  #> Gibbs sampler iteration 5200 of 10000  #> Gibbs sampler iteration 5210 of 10000  #> Gibbs sampler iteration 5220 of 10000  #> Gibbs sampler iteration 5230 of 10000  #> Gibbs sampler iteration 5240 of 10000  #> Gibbs sampler iteration 5250 of 10000  #> Gibbs sampler iteration 5260 of 10000  #> Gibbs sampler iteration 5270 of 10000  #> Gibbs sampler iteration 5280 of 10000  #> Gibbs sampler iteration 5290 of 10000  #> Gibbs sampler iteration 5300 of 10000  #> Gibbs sampler iteration 5310 of 10000  #> Gibbs sampler iteration 5320 of 10000  #> Gibbs sampler iteration 5330 of 10000  #> Gibbs sampler iteration 5340 of 10000  #> Gibbs sampler iteration 5350 of 10000  #> Gibbs sampler iteration 5360 of 10000  #> Gibbs sampler iteration 5370 of 10000  #> Gibbs sampler iteration 5380 of 10000  #> Gibbs sampler iteration 5390 of 10000  #> Gibbs sampler iteration 5400 of 10000  #> Gibbs sampler iteration 5410 of 10000  #> Gibbs sampler iteration 5420 of 10000  #> Gibbs sampler iteration 5430 of 10000  #> Gibbs sampler iteration 5440 of 10000  #> Gibbs sampler iteration 5450 of 10000  #> Gibbs sampler iteration 5460 of 10000  #> Gibbs sampler iteration 5470 of 10000  #> Gibbs sampler iteration 5480 of 10000  #> Gibbs sampler iteration 5490 of 10000  #> Gibbs sampler iteration 5500 of 10000  #> Gibbs sampler iteration 5510 of 10000  #> Gibbs sampler iteration 5520 of 10000  #> Gibbs sampler iteration 5530 of 10000  #> Gibbs sampler iteration 5540 of 10000  #> Gibbs sampler iteration 5550 of 10000  #> Gibbs sampler iteration 5560 of 10000  #> Gibbs sampler iteration 5570 of 10000  #> Gibbs sampler iteration 5580 of 10000  #> Gibbs sampler iteration 5590 of 10000  #> Gibbs sampler iteration 5600 of 10000  #> Gibbs sampler iteration 5610 of 10000  #> Gibbs sampler iteration 5620 of 10000  #> Gibbs sampler iteration 5630 of 10000  #> Gibbs sampler iteration 5640 of 10000  #> Gibbs sampler iteration 5650 of 10000  #> Gibbs sampler iteration 5660 of 10000  #> Gibbs sampler iteration 5670 of 10000  #> Gibbs sampler iteration 5680 of 10000  #> Gibbs sampler iteration 5690 of 10000  #> Gibbs sampler iteration 5700 of 10000  #> Gibbs sampler iteration 5710 of 10000  #> Gibbs sampler iteration 5720 of 10000  #> Gibbs sampler iteration 5730 of 10000  #> Gibbs sampler iteration 5740 of 10000  #> Gibbs sampler iteration 5750 of 10000  #> Gibbs sampler iteration 5760 of 10000  #> Gibbs sampler iteration 5770 of 10000  #> Gibbs sampler iteration 5780 of 10000  #> Gibbs sampler iteration 5790 of 10000  #> Gibbs sampler iteration 5800 of 10000  #> Gibbs sampler iteration 5810 of 10000  #> Gibbs sampler iteration 5820 of 10000  #> Gibbs sampler iteration 5830 of 10000  #> Gibbs sampler iteration 5840 of 10000  #> Gibbs sampler iteration 5850 of 10000  #> Gibbs sampler iteration 5860 of 10000  #> Gibbs sampler iteration 5870 of 10000  #> Gibbs sampler iteration 5880 of 10000  #> Gibbs sampler iteration 5890 of 10000  #> Gibbs sampler iteration 5900 of 10000  #> Gibbs sampler iteration 5910 of 10000  #> Gibbs sampler iteration 5920 of 10000  #> Gibbs sampler iteration 5930 of 10000  #> Gibbs sampler iteration 5940 of 10000  #> Gibbs sampler iteration 5950 of 10000  #> Gibbs sampler iteration 5960 of 10000  #> Gibbs sampler iteration 5970 of 10000  #> Gibbs sampler iteration 5980 of 10000  #> Gibbs sampler iteration 5990 of 10000  #> Gibbs sampler iteration 6000 of 10000  #> Gibbs sampler iteration 6010 of 10000  #> Gibbs sampler iteration 6020 of 10000  #> Gibbs sampler iteration 6030 of 10000  #> Gibbs sampler iteration 6040 of 10000  #> Gibbs sampler iteration 6050 of 10000  #> Gibbs sampler iteration 6060 of 10000  #> Gibbs sampler iteration 6070 of 10000  #> Gibbs sampler iteration 6080 of 10000  #> Gibbs sampler iteration 6090 of 10000  #> Gibbs sampler iteration 6100 of 10000  #> Gibbs sampler iteration 6110 of 10000  #> Gibbs sampler iteration 6120 of 10000  #> Gibbs sampler iteration 6130 of 10000  #> Gibbs sampler iteration 6140 of 10000  #> Gibbs sampler iteration 6150 of 10000  #> Gibbs sampler iteration 6160 of 10000  #> Gibbs sampler iteration 6170 of 10000  #> Gibbs sampler iteration 6180 of 10000  #> Gibbs sampler iteration 6190 of 10000  #> Gibbs sampler iteration 6200 of 10000  #> Gibbs sampler iteration 6210 of 10000  #> Gibbs sampler iteration 6220 of 10000  #> Gibbs sampler iteration 6230 of 10000  #> Gibbs sampler iteration 6240 of 10000  #> Gibbs sampler iteration 6250 of 10000  #> Gibbs sampler iteration 6260 of 10000  #> Gibbs sampler iteration 6270 of 10000  #> Gibbs sampler iteration 6280 of 10000  #> Gibbs sampler iteration 6290 of 10000  #> Gibbs sampler iteration 6300 of 10000  #> Gibbs sampler iteration 6310 of 10000  #> Gibbs sampler iteration 6320 of 10000  #> Gibbs sampler iteration 6330 of 10000  #> Gibbs sampler iteration 6340 of 10000  #> Gibbs sampler iteration 6350 of 10000  #> Gibbs sampler iteration 6360 of 10000  #> Gibbs sampler iteration 6370 of 10000  #> Gibbs sampler iteration 6380 of 10000  #> Gibbs sampler iteration 6390 of 10000  #> Gibbs sampler iteration 6400 of 10000  #> Gibbs sampler iteration 6410 of 10000  #> Gibbs sampler iteration 6420 of 10000  #> Gibbs sampler iteration 6430 of 10000  #> Gibbs sampler iteration 6440 of 10000  #> Gibbs sampler iteration 6450 of 10000  #> Gibbs sampler iteration 6460 of 10000  #> Gibbs sampler iteration 6470 of 10000  #> Gibbs sampler iteration 6480 of 10000  #> Gibbs sampler iteration 6490 of 10000  #> Gibbs sampler iteration 6500 of 10000  #> Gibbs sampler iteration 6510 of 10000  #> Gibbs sampler iteration 6520 of 10000  #> Gibbs sampler iteration 6530 of 10000  #> Gibbs sampler iteration 6540 of 10000  #> Gibbs sampler iteration 6550 of 10000  #> Gibbs sampler iteration 6560 of 10000  #> Gibbs sampler iteration 6570 of 10000  #> Gibbs sampler iteration 6580 of 10000  #> Gibbs sampler iteration 6590 of 10000  #> Gibbs sampler iteration 6600 of 10000  #> Gibbs sampler iteration 6610 of 10000  #> Gibbs sampler iteration 6620 of 10000  #> Gibbs sampler iteration 6630 of 10000  #> Gibbs sampler iteration 6640 of 10000  #> Gibbs sampler iteration 6650 of 10000  #> Gibbs sampler iteration 6660 of 10000  #> Gibbs sampler iteration 6670 of 10000  #> Gibbs sampler iteration 6680 of 10000  #> Gibbs sampler iteration 6690 of 10000  #> Gibbs sampler iteration 6700 of 10000  #> Gibbs sampler iteration 6710 of 10000  #> Gibbs sampler iteration 6720 of 10000  #> Gibbs sampler iteration 6730 of 10000  #> Gibbs sampler iteration 6740 of 10000  #> Gibbs sampler iteration 6750 of 10000  #> Gibbs sampler iteration 6760 of 10000  #> Gibbs sampler iteration 6770 of 10000  #> Gibbs sampler iteration 6780 of 10000  #> Gibbs sampler iteration 6790 of 10000  #> Gibbs sampler iteration 6800 of 10000  #> Gibbs sampler iteration 6810 of 10000  #> Gibbs sampler iteration 6820 of 10000  #> Gibbs sampler iteration 6830 of 10000  #> Gibbs sampler iteration 6840 of 10000  #> Gibbs sampler iteration 6850 of 10000  #> Gibbs sampler iteration 6860 of 10000  #> Gibbs sampler iteration 6870 of 10000  #> Gibbs sampler iteration 6880 of 10000  #> Gibbs sampler iteration 6890 of 10000  #> Gibbs sampler iteration 6900 of 10000  #> Gibbs sampler iteration 6910 of 10000  #> Gibbs sampler iteration 6920 of 10000  #> Gibbs sampler iteration 6930 of 10000  #> Gibbs sampler iteration 6940 of 10000  #> Gibbs sampler iteration 6950 of 10000  #> Gibbs sampler iteration 6960 of 10000  #> Gibbs sampler iteration 6970 of 10000  #> Gibbs sampler iteration 6980 of 10000  #> Gibbs sampler iteration 6990 of 10000  #> Gibbs sampler iteration 7000 of 10000  #> Gibbs sampler iteration 7010 of 10000  #> Gibbs sampler iteration 7020 of 10000  #> Gibbs sampler iteration 7030 of 10000  #> Gibbs sampler iteration 7040 of 10000  #> Gibbs sampler iteration 7050 of 10000  #> Gibbs sampler iteration 7060 of 10000  #> Gibbs sampler iteration 7070 of 10000  #> Gibbs sampler iteration 7080 of 10000  #> Gibbs sampler iteration 7090 of 10000  #> Gibbs sampler iteration 7100 of 10000  #> Gibbs sampler iteration 7110 of 10000  #> Gibbs sampler iteration 7120 of 10000  #> Gibbs sampler iteration 7130 of 10000  #> Gibbs sampler iteration 7140 of 10000  #> Gibbs sampler iteration 7150 of 10000  #> Gibbs sampler iteration 7160 of 10000  #> Gibbs sampler iteration 7170 of 10000  #> Gibbs sampler iteration 7180 of 10000  #> Gibbs sampler iteration 7190 of 10000  #> Gibbs sampler iteration 7200 of 10000  #> Gibbs sampler iteration 7210 of 10000  #> Gibbs sampler iteration 7220 of 10000  #> Gibbs sampler iteration 7230 of 10000  #> Gibbs sampler iteration 7240 of 10000  #> Gibbs sampler iteration 7250 of 10000  #> Gibbs sampler iteration 7260 of 10000  #> Gibbs sampler iteration 7270 of 10000  #> Gibbs sampler iteration 7280 of 10000  #> Gibbs sampler iteration 7290 of 10000  #> Gibbs sampler iteration 7300 of 10000  #> Gibbs sampler iteration 7310 of 10000  #> Gibbs sampler iteration 7320 of 10000  #> Gibbs sampler iteration 7330 of 10000  #> Gibbs sampler iteration 7340 of 10000  #> Gibbs sampler iteration 7350 of 10000  #> Gibbs sampler iteration 7360 of 10000  #> Gibbs sampler iteration 7370 of 10000  #> Gibbs sampler iteration 7380 of 10000  #> Gibbs sampler iteration 7390 of 10000  #> Gibbs sampler iteration 7400 of 10000  #> Gibbs sampler iteration 7410 of 10000  #> Gibbs sampler iteration 7420 of 10000  #> Gibbs sampler iteration 7430 of 10000  #> Gibbs sampler iteration 7440 of 10000  #> Gibbs sampler iteration 7450 of 10000  #> Gibbs sampler iteration 7460 of 10000  #> Gibbs sampler iteration 7470 of 10000  #> Gibbs sampler iteration 7480 of 10000  #> Gibbs sampler iteration 7490 of 10000  #> Gibbs sampler iteration 7500 of 10000  #> Gibbs sampler iteration 7510 of 10000  #> Gibbs sampler iteration 7520 of 10000  #> Gibbs sampler iteration 7530 of 10000  #> Gibbs sampler iteration 7540 of 10000  #> Gibbs sampler iteration 7550 of 10000  #> Gibbs sampler iteration 7560 of 10000  #> Gibbs sampler iteration 7570 of 10000  #> Gibbs sampler iteration 7580 of 10000  #> Gibbs sampler iteration 7590 of 10000  #> Gibbs sampler iteration 7600 of 10000  #> Gibbs sampler iteration 7610 of 10000  #> Gibbs sampler iteration 7620 of 10000  #> Gibbs sampler iteration 7630 of 10000  #> Gibbs sampler iteration 7640 of 10000  #> Gibbs sampler iteration 7650 of 10000  #> Gibbs sampler iteration 7660 of 10000  #> Gibbs sampler iteration 7670 of 10000  #> Gibbs sampler iteration 7680 of 10000  #> Gibbs sampler iteration 7690 of 10000  #> Gibbs sampler iteration 7700 of 10000  #> Gibbs sampler iteration 7710 of 10000  #> Gibbs sampler iteration 7720 of 10000  #> Gibbs sampler iteration 7730 of 10000  #> Gibbs sampler iteration 7740 of 10000  #> Gibbs sampler iteration 7750 of 10000  #> Gibbs sampler iteration 7760 of 10000  #> Gibbs sampler iteration 7770 of 10000  #> Gibbs sampler iteration 7780 of 10000  #> Gibbs sampler iteration 7790 of 10000  #> Gibbs sampler iteration 7800 of 10000  #> Gibbs sampler iteration 7810 of 10000  #> Gibbs sampler iteration 7820 of 10000  #> Gibbs sampler iteration 7830 of 10000  #> Gibbs sampler iteration 7840 of 10000  #> Gibbs sampler iteration 7850 of 10000  #> Gibbs sampler iteration 7860 of 10000  #> Gibbs sampler iteration 7870 of 10000  #> Gibbs sampler iteration 7880 of 10000  #> Gibbs sampler iteration 7890 of 10000  #> Gibbs sampler iteration 7900 of 10000  #> Gibbs sampler iteration 7910 of 10000  #> Gibbs sampler iteration 7920 of 10000  #> Gibbs sampler iteration 7930 of 10000  #> Gibbs sampler iteration 7940 of 10000  #> Gibbs sampler iteration 7950 of 10000  #> Gibbs sampler iteration 7960 of 10000  #> Gibbs sampler iteration 7970 of 10000  #> Gibbs sampler iteration 7980 of 10000  #> Gibbs sampler iteration 7990 of 10000  #> Gibbs sampler iteration 8000 of 10000  #> Gibbs sampler iteration 8010 of 10000  #> Gibbs sampler iteration 8020 of 10000  #> Gibbs sampler iteration 8030 of 10000  #> Gibbs sampler iteration 8040 of 10000  #> Gibbs sampler iteration 8050 of 10000  #> Gibbs sampler iteration 8060 of 10000  #> Gibbs sampler iteration 8070 of 10000  #> Gibbs sampler iteration 8080 of 10000  #> Gibbs sampler iteration 8090 of 10000  #> Gibbs sampler iteration 8100 of 10000  #> Gibbs sampler iteration 8110 of 10000  #> Gibbs sampler iteration 8120 of 10000  #> Gibbs sampler iteration 8130 of 10000  #> Gibbs sampler iteration 8140 of 10000  #> Gibbs sampler iteration 8150 of 10000  #> Gibbs sampler iteration 8160 of 10000  #> Gibbs sampler iteration 8170 of 10000  #> Gibbs sampler iteration 8180 of 10000  #> Gibbs sampler iteration 8190 of 10000  #> Gibbs sampler iteration 8200 of 10000  #> Gibbs sampler iteration 8210 of 10000  #> Gibbs sampler iteration 8220 of 10000  #> Gibbs sampler iteration 8230 of 10000  #> Gibbs sampler iteration 8240 of 10000  #> Gibbs sampler iteration 8250 of 10000  #> Gibbs sampler iteration 8260 of 10000  #> Gibbs sampler iteration 8270 of 10000  #> Gibbs sampler iteration 8280 of 10000  #> Gibbs sampler iteration 8290 of 10000  #> Gibbs sampler iteration 8300 of 10000  #> Gibbs sampler iteration 8310 of 10000  #> Gibbs sampler iteration 8320 of 10000  #> Gibbs sampler iteration 8330 of 10000  #> Gibbs sampler iteration 8340 of 10000  #> Gibbs sampler iteration 8350 of 10000  #> Gibbs sampler iteration 8360 of 10000  #> Gibbs sampler iteration 8370 of 10000  #> Gibbs sampler iteration 8380 of 10000  #> Gibbs sampler iteration 8390 of 10000  #> Gibbs sampler iteration 8400 of 10000  #> Gibbs sampler iteration 8410 of 10000  #> Gibbs sampler iteration 8420 of 10000  #> Gibbs sampler iteration 8430 of 10000  #> Gibbs sampler iteration 8440 of 10000  #> Gibbs sampler iteration 8450 of 10000  #> Gibbs sampler iteration 8460 of 10000  #> Gibbs sampler iteration 8470 of 10000  #> Gibbs sampler iteration 8480 of 10000  #> Gibbs sampler iteration 8490 of 10000  #> Gibbs sampler iteration 8500 of 10000  #> Gibbs sampler iteration 8510 of 10000  #> Gibbs sampler iteration 8520 of 10000  #> Gibbs sampler iteration 8530 of 10000  #> Gibbs sampler iteration 8540 of 10000  #> Gibbs sampler iteration 8550 of 10000  #> Gibbs sampler iteration 8560 of 10000  #> Gibbs sampler iteration 8570 of 10000  #> Gibbs sampler iteration 8580 of 10000  #> Gibbs sampler iteration 8590 of 10000  #> Gibbs sampler iteration 8600 of 10000  #> Gibbs sampler iteration 8610 of 10000  #> Gibbs sampler iteration 8620 of 10000  #> Gibbs sampler iteration 8630 of 10000  #> Gibbs sampler iteration 8640 of 10000  #> Gibbs sampler iteration 8650 of 10000  #> Gibbs sampler iteration 8660 of 10000  #> Gibbs sampler iteration 8670 of 10000  #> Gibbs sampler iteration 8680 of 10000  #> Gibbs sampler iteration 8690 of 10000  #> Gibbs sampler iteration 8700 of 10000  #> Gibbs sampler iteration 8710 of 10000  #> Gibbs sampler iteration 8720 of 10000  #> Gibbs sampler iteration 8730 of 10000  #> Gibbs sampler iteration 8740 of 10000  #> Gibbs sampler iteration 8750 of 10000  #> Gibbs sampler iteration 8760 of 10000  #> Gibbs sampler iteration 8770 of 10000  #> Gibbs sampler iteration 8780 of 10000  #> Gibbs sampler iteration 8790 of 10000  #> Gibbs sampler iteration 8800 of 10000  #> Gibbs sampler iteration 8810 of 10000  #> Gibbs sampler iteration 8820 of 10000  #> Gibbs sampler iteration 8830 of 10000  #> Gibbs sampler iteration 8840 of 10000  #> Gibbs sampler iteration 8850 of 10000  #> Gibbs sampler iteration 8860 of 10000  #> Gibbs sampler iteration 8870 of 10000  #> Gibbs sampler iteration 8880 of 10000  #> Gibbs sampler iteration 8890 of 10000  #> Gibbs sampler iteration 8900 of 10000  #> Gibbs sampler iteration 8910 of 10000  #> Gibbs sampler iteration 8920 of 10000  #> Gibbs sampler iteration 8930 of 10000  #> Gibbs sampler iteration 8940 of 10000  #> Gibbs sampler iteration 8950 of 10000  #> Gibbs sampler iteration 8960 of 10000  #> Gibbs sampler iteration 8970 of 10000  #> Gibbs sampler iteration 8980 of 10000  #> Gibbs sampler iteration 8990 of 10000  #> Gibbs sampler iteration 9000 of 10000  #> Gibbs sampler iteration 9010 of 10000  #> Gibbs sampler iteration 9020 of 10000  #> Gibbs sampler iteration 9030 of 10000  #> Gibbs sampler iteration 9040 of 10000  #> Gibbs sampler iteration 9050 of 10000  #> Gibbs sampler iteration 9060 of 10000  #> Gibbs sampler iteration 9070 of 10000  #> Gibbs sampler iteration 9080 of 10000  #> Gibbs sampler iteration 9090 of 10000  #> Gibbs sampler iteration 9100 of 10000  #> Gibbs sampler iteration 9110 of 10000  #> Gibbs sampler iteration 9120 of 10000  #> Gibbs sampler iteration 9130 of 10000  #> Gibbs sampler iteration 9140 of 10000  #> Gibbs sampler iteration 9150 of 10000  #> Gibbs sampler iteration 9160 of 10000  #> Gibbs sampler iteration 9170 of 10000  #> Gibbs sampler iteration 9180 of 10000  #> Gibbs sampler iteration 9190 of 10000  #> Gibbs sampler iteration 9200 of 10000  #> Gibbs sampler iteration 9210 of 10000  #> Gibbs sampler iteration 9220 of 10000  #> Gibbs sampler iteration 9230 of 10000  #> Gibbs sampler iteration 9240 of 10000  #> Gibbs sampler iteration 9250 of 10000  #> Gibbs sampler iteration 9260 of 10000  #> Gibbs sampler iteration 9270 of 10000  #> Gibbs sampler iteration 9280 of 10000  #> Gibbs sampler iteration 9290 of 10000  #> Gibbs sampler iteration 9300 of 10000  #> Gibbs sampler iteration 9310 of 10000  #> Gibbs sampler iteration 9320 of 10000  #> Gibbs sampler iteration 9330 of 10000  #> Gibbs sampler iteration 9340 of 10000  #> Gibbs sampler iteration 9350 of 10000  #> Gibbs sampler iteration 9360 of 10000  #> Gibbs sampler iteration 9370 of 10000  #> Gibbs sampler iteration 9380 of 10000  #> Gibbs sampler iteration 9390 of 10000  #> Gibbs sampler iteration 9400 of 10000  #> Gibbs sampler iteration 9410 of 10000  #> Gibbs sampler iteration 9420 of 10000  #> Gibbs sampler iteration 9430 of 10000  #> Gibbs sampler iteration 9440 of 10000  #> Gibbs sampler iteration 9450 of 10000  #> Gibbs sampler iteration 9460 of 10000  #> Gibbs sampler iteration 9470 of 10000  #> Gibbs sampler iteration 9480 of 10000  #> Gibbs sampler iteration 9490 of 10000  #> Gibbs sampler iteration 9500 of 10000  #> Gibbs sampler iteration 9510 of 10000  #> Gibbs sampler iteration 9520 of 10000  #> Gibbs sampler iteration 9530 of 10000  #> Gibbs sampler iteration 9540 of 10000  #> Gibbs sampler iteration 9550 of 10000  #> Gibbs sampler iteration 9560 of 10000  #> Gibbs sampler iteration 9570 of 10000  #> Gibbs sampler iteration 9580 of 10000  #> Gibbs sampler iteration 9590 of 10000  #> Gibbs sampler iteration 9600 of 10000  #> Gibbs sampler iteration 9610 of 10000  #> Gibbs sampler iteration 9620 of 10000  #> Gibbs sampler iteration 9630 of 10000  #> Gibbs sampler iteration 9640 of 10000  #> Gibbs sampler iteration 9650 of 10000  #> Gibbs sampler iteration 9660 of 10000  #> Gibbs sampler iteration 9670 of 10000  #> Gibbs sampler iteration 9680 of 10000  #> Gibbs sampler iteration 9690 of 10000  #> Gibbs sampler iteration 9700 of 10000  #> Gibbs sampler iteration 9710 of 10000  #> Gibbs sampler iteration 9720 of 10000  #> Gibbs sampler iteration 9730 of 10000  #> Gibbs sampler iteration 9740 of 10000  #> Gibbs sampler iteration 9750 of 10000  #> Gibbs sampler iteration 9760 of 10000  #> Gibbs sampler iteration 9770 of 10000  #> Gibbs sampler iteration 9780 of 10000  #> Gibbs sampler iteration 9790 of 10000  #> Gibbs sampler iteration 9800 of 10000  #> Gibbs sampler iteration 9810 of 10000  #> Gibbs sampler iteration 9820 of 10000  #> Gibbs sampler iteration 9830 of 10000  #> Gibbs sampler iteration 9840 of 10000  #> Gibbs sampler iteration 9850 of 10000  #> Gibbs sampler iteration 9860 of 10000  #> Gibbs sampler iteration 9870 of 10000  #> Gibbs sampler iteration 9880 of 10000  #> Gibbs sampler iteration 9890 of 10000  #> Gibbs sampler iteration 9900 of 10000  #> Gibbs sampler iteration 9910 of 10000  #> Gibbs sampler iteration 9920 of 10000  #> Gibbs sampler iteration 9930 of 10000  #> Gibbs sampler iteration 9940 of 10000  #> Gibbs sampler iteration 9950 of 10000  #> Gibbs sampler iteration 9960 of 10000  #> Gibbs sampler iteration 9970 of 10000  #> Gibbs sampler iteration 9980 of 10000  #> Gibbs sampler iteration 9990 of 10000  #> Gibbs sampler iteration 10000 of 10000  coef(model) #>          Estimate   (sd) #> 1   cov     -2.99 (1.15) #> 2 ASC_A     -0.93 (0.39) predict(model) #>     predicted #> true  A  B #>    A 11  2 #>    B  0 37 predict(model, overview = FALSE) #>    id idc            A            B true predicted correct #> 1   1   1 1.807350e-01 8.192650e-01    B         B    TRUE #> 2   1   2 8.785305e-02 9.121469e-01    B         B    TRUE #> 3   1   3 1.238442e-01 8.761558e-01    B         B    TRUE #> 4   1   4 8.488609e-08 9.999999e-01    B         B    TRUE #> 5   1   5 5.347099e-05 9.999465e-01    B         B    TRUE #> 6   1   6 1.000000e+00 4.899764e-12    A         A    TRUE #> 7   1   7 4.032345e-01 5.967655e-01    B         B    TRUE #> 8   1   8 3.384275e-01 6.615725e-01    B         B    TRUE #> 9   1   9 6.630947e-02 9.336905e-01    B         B    TRUE #> 10  1  10 9.999997e-01 2.662796e-07    A         A    TRUE #> 11  2   1 9.605318e-14 1.000000e+00    B         B    TRUE #> 12  2   2 2.529192e-04 9.997471e-01    B         B    TRUE #> 13  2   3 9.999999e-01 9.366899e-08    A         A    TRUE #> 14  2   4 9.999106e-01 8.937791e-05    A         A    TRUE #> 15  2   5 4.420377e-07 9.999996e-01    B         B    TRUE #> 16  2   6 2.434504e-02 9.756550e-01    B         B    TRUE #> 17  2   7 3.292280e-02 9.670772e-01    B         B    TRUE #> 18  2   8 2.229885e-06 9.999978e-01    B         B    TRUE #> 19  2   9 2.813382e-02 9.718662e-01    B         B    TRUE #> 20  2  10 6.027138e-04 9.993973e-01    B         B    TRUE #> 21  3   1 1.035370e-07 9.999999e-01    B         B    TRUE #> 22  3   2 7.724475e-01 2.275525e-01    A         A    TRUE #> 23  3   3 3.627674e-02 9.637233e-01    B         B    TRUE #> 24  3   4 9.999963e-01 3.689158e-06    A         A    TRUE #> 25  3   5 1.019804e-03 9.989802e-01    B         B    TRUE #> 26  3   6 9.144621e-01 8.553794e-02    A         A    TRUE #> 27  3   7 2.467305e-01 7.532695e-01    B         B    TRUE #> 28  3   8 9.996043e-01 3.956781e-04    A         A    TRUE #> 29  3   9 6.184222e-02 9.381578e-01    B         B    TRUE #> 30  3  10 8.141432e-04 9.991859e-01    B         B    TRUE #> 31  4   1 7.369115e-07 9.999993e-01    B         B    TRUE #> 32  4   2 8.545130e-03 9.914549e-01    B         B    TRUE #> 33  4   3 3.083322e-01 6.916678e-01    A         B   FALSE #> 34  4   4 5.436878e-08 9.999999e-01    B         B    TRUE #> 35  4   5 9.999798e-01 2.021011e-05    A         A    TRUE #> 36  4   6 9.131205e-06 9.999909e-01    B         B    TRUE #> 37  4   7 2.573086e-01 7.426914e-01    B         B    TRUE #> 38  4   8 9.811163e-03 9.901888e-01    B         B    TRUE #> 39  4   9 3.382023e-10 1.000000e+00    B         B    TRUE #> 40  4  10 3.576074e-04 9.996424e-01    B         B    TRUE #> 41  5   1 3.531328e-10 1.000000e+00    B         B    TRUE #> 42  5   2 9.995905e-01 4.094751e-04    A         A    TRUE #> 43  5   3 6.451406e-16 1.000000e+00    B         B    TRUE #> 44  5   4 3.413180e-05 9.999659e-01    B         B    TRUE #> 45  5   5 1.360616e-02 9.863938e-01    B         B    TRUE #> 46  5   6 1.444905e-01 8.555095e-01    A         B   FALSE #> 47  5   7 9.999879e-01 1.212772e-05    A         A    TRUE #> 48  5   8 7.427831e-04 9.992572e-01    B         B    TRUE #> 49  5   9 4.452827e-06 9.999955e-01    B         B    TRUE #> 50  5  10 1.191380e-17 1.000000e+00    B         B    TRUE predict(model, data = data$test) #>     predicted #> true  A  B #>    A 15  3 #>    B  6 26 predict(model, data = data.frame(\"cov_A\" = c(1,1,NA,NA), \"cov_B\" = c(1,NA,1,NA)),         overview = FALSE) #>   id idc            A          B prediction #> 1  1   1 1.759851e-01 0.82401491          B #> 2  2   1 4.393451e-05 0.99995607          B #> 3  3   1 9.803155e-01 0.01968455          A #> 4  4   1 1.759851e-01 0.82401491          B"},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_classification.html","id":null,"dir":"Reference","previous_headings":"","what":"Classify deciders based on their preferences — preference_classification","title":"Classify deciders based on their preferences — preference_classification","text":"function classifies deciders based allocation components mixing distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_classification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Classify deciders based on their preferences — preference_classification","text":"","code":"preference_classification(x, add_true = FALSE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_classification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Classify deciders based on their preferences — preference_classification","text":"x object class RprobitB_fit. add_true Set TRUE add true class memberships output (available).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_classification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Classify deciders based on their preferences — preference_classification","text":"data frame. row names decider ids. first C columns contain relative frequencies deciders allocated C classes. Next, column est contains estimated class decider based highest allocation frequency. add_true, next column true contains true class memberships.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_classification.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Classify deciders based on their preferences — preference_classification","text":"function can used model least one random effect (.e. P_r >= 1) least two latent classes (.e. C >= 2). case, let \\(z_1,\\dots,z_N\\) denote class allocations \\(N\\) deciders based estimated mixed coefficients \\(\\beta = (\\beta_1,\\dots,\\beta_N)\\). Independently decider \\(n\\), conditional probability \\(\\Pr(z_n = c \\mid s,\\beta_n,b,\\Omega)\\) \\(\\beta_n\\) allocated class \\(c\\) \\(c=1,\\dots,C\\) depends class allocation vector \\(s\\), class means \\(b=(b_c)_c\\) class covariance matrices \\(Omega=(Omega_c)_c\\) proportional $$s_c \\phi(\\beta_n \\mid b_c,Omega_c).$$ function displays relative frequencies decider allocated classes Gibbs sampling. thinned samples burn-period considered.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_flip.html","id":null,"dir":"Reference","previous_headings":"","what":"Check for flip in preferences after change in model scale. — preference_flip","title":"Check for flip in preferences after change in model scale. — preference_flip","text":"function checks change model scale flipped preferences.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_flip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check for flip in preferences after change in model scale. — preference_flip","text":"","code":"preference_flip(model_old, model_new)"},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_flip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check for flip in preferences after change in model scale. — preference_flip","text":"model_old object class RprobitB_fit, model scale change. model_new object class RprobitB_fit, model scale change.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/preference_flip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check for flip in preferences after change in model scale. — preference_flip","text":"return value, called side-effects.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/prepare_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare empirical choice data for estimation — prepare_data","title":"Prepare empirical choice data for estimation — prepare_data","text":"function prepares empirical choice data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/prepare_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare empirical choice data for estimation — prepare_data","text":"","code":"prepare_data(   form,   choice_data,   re = NULL,   alternatives = NULL,   id = \"id\",   idc = NULL,   standardize = NULL,   impute = \"complete_cases\" )"},{"path":"https://loelschlaeger.de/RprobitB/reference/prepare_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare empirical choice data for estimation — prepare_data","text":"form formula object used specify probit model. structure choice ~ | B | C, names alternative choice situation specific covariates generic coefficient, B names choice situation specific covariates alternative specific coefficients, C names alternative choice situation specific covariates alternative specific coefficients. Separate multiple covariates one type + sign. default, alternative specific constants (ASCs) added model (except last alternative due identifiability). can removed adding +0 second spot. See vignette choice data details. choice_data data frame choice data wide format, .e. row represents one choice occasion. re character (vector) covariates form random effects. re = NULL (default), random effects. random effects alternative specific constants, include \"ASC\" re. alternatives character vector names choice alternatives. specified, choice set defined observed choices. id character, name column choice_data contains unique identifier decision maker. default \"id\". idc character, name column choice_data contains unique identifier choice situation decision maker. default NULL, case identifier generated automatically. standardize character vector names covariates get standardized. Covariates type 1 3 addressed <covariate>_<alternative>. standardize = \"\", covariates get standardized. impute character specifies handle missing entries (elements ) as_missing) choice_data, one : \"complete_cases\", removes rows containing missing entries (default), \"zero_out\", replaces missing entries zero (numeric columns), \"mean\", imputes missing entries covariate mean (numeric columns).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/prepare_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare empirical choice data for estimation — prepare_data","text":"object class RprobitB_data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/prepare_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Prepare empirical choice data for estimation — prepare_data","text":"Requirements choice_data: must contain column named id contains unique identifier decision maker. can contain column named idc contains unique identifier choice situation decision maker. information missing, identifier generated automatically appearance choices data set. can contain column named choice observed choices, choice must match name dependent variable form. column required model fitting prediction. must contain numeric column named p_j alternative specific covariate p form choice alternative j alternatives. must contain numeric column named q covariate q form constant across alternatives. See vignette choice data details.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/reference/prepare_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare empirical choice data for estimation — prepare_data","text":"","code":"data(\"Train\", package = \"mlogit\") data <- prepare_data(   form = choice ~ price + time + comfort + change | 0,   choice_data = Train,   re = c(\"price\", \"time\"),   id = \"id\",   idc = \"choiceid\",   standardize = c(\"price\", \"time\") )"},{"path":"https://loelschlaeger.de/RprobitB/reference/rdirichlet.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw from Dirichlet distribution — rdirichlet","title":"Draw from Dirichlet distribution — rdirichlet","text":"Function draw Dirichlet distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rdirichlet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw from Dirichlet distribution — rdirichlet","text":"","code":"rdirichlet(delta)"},{"path":"https://loelschlaeger.de/RprobitB/reference/rdirichlet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw from Dirichlet distribution — rdirichlet","text":"delta vector, concentration parameter.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rdirichlet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw from Dirichlet distribution — rdirichlet","text":"vector, sample Dirichlet distribution length delta.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rdirichlet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Draw from Dirichlet distribution — rdirichlet","text":"","code":"rdirichlet(delta = 1:3) #>            [,1] #> [1,] 0.09103113 #> [2,] 0.11062157 #> [3,] 0.79834729"},{"path":"https://loelschlaeger.de/RprobitB/reference/rmvnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw from multivariate normal distribution — rmvnorm","title":"Draw from multivariate normal distribution — rmvnorm","text":"function draws multivariate normal distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rmvnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw from multivariate normal distribution — rmvnorm","text":"","code":"rmvnorm(mu, Sigma)"},{"path":"https://loelschlaeger.de/RprobitB/reference/rmvnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw from multivariate normal distribution — rmvnorm","text":"mu mean vector length n. Sigma covariance matrix dimension n x n.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rmvnorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw from multivariate normal distribution — rmvnorm","text":"numeric vector length n.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rmvnorm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Draw from multivariate normal distribution — rmvnorm","text":"function builds upon following fact: \\(\\epsilon = (\\epsilon_1,\\dots,\\epsilon_n)\\), \\(\\epsilon_i\\) drawn independently standard normal distribution, \\(\\mu+L\\epsilon\\) draw multivariate normal distribution \\(N(\\mu,\\Sigma)\\), \\(L\\) lower triangular factor Choleski decomposition \\(\\Sigma\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rmvnorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Draw from multivariate normal distribution — rmvnorm","text":"","code":"mu <- c(0,0) Sigma <- diag(2) rmvnorm(mu = mu, Sigma = Sigma) #>           [,1] #> [1,] 0.5455607 #> [2,] 0.1192982"},{"path":"https://loelschlaeger.de/RprobitB/reference/rtnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw from truncated normal — rtnorm","title":"Draw from truncated normal — rtnorm","text":"function draws truncated univariate normal distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rtnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw from truncated normal — rtnorm","text":"","code":"rtnorm(mu, sig, trunpt, above)"},{"path":"https://loelschlaeger.de/RprobitB/reference/rtnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw from truncated normal — rtnorm","text":"mu mean. sig standard deviation. trunpt truncation point. boolean, TRUE truncate , otherwise .","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rtnorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw from truncated normal — rtnorm","text":"numeric value.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rtnorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Draw from truncated normal — rtnorm","text":"","code":"### draw R samples from a standard normal truncated at 1 from above R <- 1e4 draws <- replicate(R, rtnorm(1,1,1,TRUE)) ### draw the density plot(density(draws))"},{"path":"https://loelschlaeger.de/RprobitB/reference/rwishart.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw from Wishart distribution — rwishart","title":"Draw from Wishart distribution — rwishart","text":"function draws Wishart inverted Wishart distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rwishart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw from Wishart distribution — rwishart","text":"","code":"rwishart(nu, V)"},{"path":"https://loelschlaeger.de/RprobitB/reference/rwishart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw from Wishart distribution — rwishart","text":"nu numeric, degrees freedom. Must least number dimensions. V matrix, scale matrix.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rwishart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw from Wishart distribution — rwishart","text":"list, draws Wishart (W), inverted Wishart (IW), corresponding Choleski decomposition (C CI).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rwishart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Draw from Wishart distribution — rwishart","text":"Wishart distribution generalization multiple dimensions gamma distributions draws space covariance matrices. expectation nu*V variance increases nu values V. Wishart distribution conjugate prior precision matrix multivariate normal distribution proper nu greater number dimensions.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/rwishart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Draw from Wishart distribution — rwishart","text":"","code":"rwishart(nu = 2, V = diag(2)) #> $W #>          [,1]     [,2] #> [1,] 4.573638 2.976911 #> [2,] 2.976911 1.957824 #>  #> $IW #>           [,1]      [,2] #> [1,]  21.19319 -32.22468 #> [2,] -32.22468  49.50904 #>  #> $C #>          [,1]      [,2] #> [1,] 2.138607 1.3919862 #> [2,] 0.000000 0.1421208 #>  #> $CI #>           [,1]      [,2] #> [1,] 0.4675942 -4.579798 #> [2,] 0.0000000  7.036266 #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/set_initial_gibbs_values.html","id":null,"dir":"Reference","previous_headings":"","what":"Set initial values for the Gibbs sampler — set_initial_gibbs_values","title":"Set initial values for the Gibbs sampler — set_initial_gibbs_values","text":"function sets initial values Gibbs sampler.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/set_initial_gibbs_values.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set initial values for the Gibbs sampler — set_initial_gibbs_values","text":"","code":"set_initial_gibbs_values(N, T, J, P_f, P_r, C)"},{"path":"https://loelschlaeger.de/RprobitB/reference/set_initial_gibbs_values.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set initial values for the Gibbs sampler — set_initial_gibbs_values","text":"N number (greater equal 1) decision makers. T number (greater equal 1) choice occasions vector choice occasions length N (.e. decision maker specific number). J number (greater equal 2) choice alternatives. P_f number covariates connected fixed coefficient (can 0). P_r number covariates connected random coefficient (can 0). C number (greater equal 1) latent classes.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/set_initial_gibbs_values.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set initial values for the Gibbs sampler — set_initial_gibbs_values","text":"list initial values Gibbs sampler.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/set_initial_gibbs_values.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set initial values for the Gibbs sampler — set_initial_gibbs_values","text":"","code":"RprobitB:::set_initial_gibbs_values(   N = 2, T = 3, J = 3, P_f = 1, P_r = 2, C = 2 ) #> $alpha0 #> [1] 0 #>  #> $z0 #> [1] 1 1 #>  #> $m0 #> [1] 1 1 #>  #> $b0 #>      [,1] [,2] #> [1,]    0    0 #> [2,]    0    0 #>  #> $Omega0 #>      [,1] [,2] #> [1,]    1    1 #> [2,]    0    0 #> [3,]    0    0 #> [4,]    1    1 #>  #> $beta0 #>      [,1] [,2] #> [1,]    0    0 #> [2,]    0    0 #>  #> $U0 #>      [,1] [,2] [,3] [,4] [,5] [,6] #> [1,]    0    0    0    0    0    0 #> [2,]    0    0    0    0    0    0 #>  #> $Sigma0 #>      [,1] [,2] #> [1,]    1    0 #> [2,]    0    1 #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/simulate_choices.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate choice data — simulate_choices","title":"Simulate choice data — simulate_choices","text":"function simulates choice data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/simulate_choices.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate choice data — simulate_choices","text":"","code":"simulate_choices(   form,   N,   T,   J,   re = NULL,   alternatives = NULL,   covariates = NULL,   seed = NULL,   ... )"},{"path":"https://loelschlaeger.de/RprobitB/reference/simulate_choices.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate choice data — simulate_choices","text":"form formula object used specify probit model. structure choice ~ | B | C, names alternative choice situation specific covariates generic coefficient, B names choice situation specific covariates alternative specific coefficients, C names alternative choice situation specific covariates alternative specific coefficients. Separate multiple covariates one type + sign. default, alternative specific constants (ASCs) added model (except last alternative due identifiability). can removed adding +0 second spot. See vignette choice data details. N number (greater equal 1) decision makers. T number (greater equal 1) choice occasions vector choice occasions length N (.e. decision maker specific number). J number (greater equal 2) choice alternatives. re character (vector) covariates form random effects. re = NULL (default), random effects. random effects alternative specific constants, include \"ASC\" re. alternatives character vector names choice alternatives. specified, choice set defined observed choices. covariates named list covariate values. element must vector length equal number choice occasions named according covariate. Covariates values supplied drawn standard normal distribution. seed Set seed simulation. ... Optionally specify alpha, C, s, b, Omega, Sigma, Sigma_full, beta, z simulation.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/simulate_choices.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate choice data — simulate_choices","text":"object class RprobitB_data.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/simulate_choices.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate choice data — simulate_choices","text":"See vignette choice data details.","code":""},{"path":[]},{"path":"https://loelschlaeger.de/RprobitB/reference/simulate_choices.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate choice data — simulate_choices","text":"","code":"data <- simulate_choices(   form = choice ~ cost | income | time,   N = 100,   T = 10,   J = 2,   re = c(\"cost\", \"time\"),   alternatives = c(\"car\", \"bus\"),   seed = 1,   alpha = c(-1, 1),   b = matrix(c(-1, -1, -0.5, -1.5, 0, -1), ncol = 2),   C = 2 )"},{"path":"https://loelschlaeger.de/RprobitB/reference/sufficient_statistics.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute sufficient statistics — sufficient_statistics","title":"Compute sufficient statistics — sufficient_statistics","text":"function computes sufficient statistics RprobitB_data object Gibbs sampler save computation time.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/sufficient_statistics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute sufficient statistics — sufficient_statistics","text":"","code":"sufficient_statistics(data, normalization)"},{"path":"https://loelschlaeger.de/RprobitB/reference/sufficient_statistics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute sufficient statistics — sufficient_statistics","text":"data object class RprobitB_data. normalization object class RprobitB_normalization, can created via RprobitB_normalization.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/sufficient_statistics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute sufficient statistics — sufficient_statistics","text":"list sufficient statistics data Gibbs sampling, containing elements N, T, J, P_f P_r data, Tvec, vector choice occasions decider length N, csTvec, vector length N cumulated sums Tvec starting 0, W, list design matrices differenced respect alternative number normalization$level decider choice occasion covariates linked fixed coefficient (NA P_f = 0), X, list design matrices differenced respect alternative number normalization$level decider choice occasion covariates linked random coefficient (NA P_r = 0), y, matrix dimension N x max(Tvec) observed choices deciders rows choice occasions columns, decoded numeric values respect appearance data$alternatives, rows filled NA case unbalanced panel, WkW, matrix dimension P_f^2 x (J-1)^2, sum Kronecker products transposed element W , XkX, list length N, element constructed way WkW elements X separately decider.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/sufficient_statistics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute sufficient statistics — sufficient_statistics","text":"","code":"RprobitB:::sufficient_statistics(   data = simulate_choices(choice ~ v1 | v2, N = 2, T = 1:2, J = 3, re = \"v2\"),   normalization = RprobitB:::RprobitB_normalization(J = 3, P_f = 3) ) #> Computing sufficient statistics 0 of 4  #> Computing sufficient statistics 1 of 4  #> Computing sufficient statistics 2 of 4  #> Computing sufficient statistics 3 of 4  #> Computing sufficient statistics 4 of 4  #> $N #> [1] 2 #>  #> $T #> [1] 1 2 #>  #> $J #> [1] 3 #>  #> $P_f #> [1] 3 #>  #> $P_r #> [1] 2 #>  #> $Tvec #> [1] 1 2 #>  #> $csTvec #> [1] 0 1 #>  #> $W #> $W[[1]] #>              v1 ASC_A ASC_B #> [1,] -0.1134949     1     0 #> [2,]  0.8146276     0     1 #>  #> $W[[2]] #>             v1 ASC_A ASC_B #> [1,] 0.4088111     1     0 #> [2,] 1.2051665     0     1 #>  #> $W[[3]] #>              v1 ASC_A ASC_B #> [1,]  1.5144790     1     0 #> [2,] -0.2588766     0     1 #>  #>  #> $X #> $X[[1]] #>            v2_A       v2_B #> [1,] 0.08900887 0.00000000 #> [2,] 0.00000000 0.08900887 #>  #> $X[[2]] #>             v2_A        v2_B #> [1,] -0.09891759  0.00000000 #> [2,]  0.00000000 -0.09891759 #>  #> $X[[3]] #>           v2_A      v2_B #> [1,] -1.315725  0.000000 #> [2,]  0.000000 -1.315725 #>  #>  #> $y #>      [,1] [,2] #> [1,]    2   NA #> [2,]    2    2 #>  #> $WkW #>           [,1]        [,2]        [,3]     [,4] #>  [1,] 2.473654 0.008166166 0.008166166 2.183061 #>  [2,] 1.809795 0.000000000 1.760917453 0.000000 #>  [3,] 0.000000 1.809795217 0.000000000 1.760917 #>  [4,] 1.809795 1.760917453 0.000000000 0.000000 #>  [5,] 3.000000 0.000000000 0.000000000 0.000000 #>  [6,] 0.000000 3.000000000 0.000000000 0.000000 #>  [7,] 0.000000 0.000000000 1.809795217 1.760917 #>  [8,] 0.000000 0.000000000 3.000000000 0.000000 #>  [9,] 0.000000 0.000000000 0.000000000 3.000000 #>  #> $XkX #> $XkX[[1]] #>             [,1]        [,2]        [,3]        [,4] #> [1,] 0.007922579 0.000000000 0.000000000 0.000000000 #> [2,] 0.000000000 0.007922579 0.000000000 0.000000000 #> [3,] 0.000000000 0.000000000 0.007922579 0.000000000 #> [4,] 0.000000000 0.000000000 0.000000000 0.007922579 #>  #> $XkX[[2]] #>          [,1]     [,2]     [,3]     [,4] #> [1,] 1.740916 0.000000 0.000000 0.000000 #> [2,] 0.000000 1.740916 0.000000 0.000000 #> [3,] 0.000000 0.000000 1.740916 0.000000 #> [4,] 0.000000 0.000000 0.000000 1.740916 #>  #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/train_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Split choice data set in two parts — train_test","title":"Split choice data set in two parts — train_test","text":"function splits choice data train test part.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/train_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split choice data set in two parts — train_test","text":"","code":"train_test(   x,   test_proportion = NULL,   test_number = NULL,   by = \"N\",   random = FALSE,   seed = NULL )"},{"path":"https://loelschlaeger.de/RprobitB/reference/train_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split choice data set in two parts — train_test","text":"x object class RprobitB_data. test_proportion number 0 1, proportion test subsample. test_number positive integer, number observations test subsample. One \"N\" (split deciders) \"T\" (split choice occasions). random TRUE, subsamples build randomly. seed Set seed building subsamples randomly.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/train_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split choice data set in two parts — train_test","text":"list two objects class RprobitB_data, named \"train\"\"test\".","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/train_test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split choice data set in two parts — train_test","text":"See vignette choice data details.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/train_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split choice data set in two parts — train_test","text":"","code":"### simulate choices for demonstration x <- simulate_choices(form = choice ~ covariate, N = 10, T = 10, J = 2)  ### 70% of deciders in the train subsample, ### 30% of deciders in the test subsample train_test(x, test_proportion = 0.3, by = \"N\") #> $train #> Simulated data of 70 choices. #>  #> $test #> Simulated data of 30 choices. #>   ### 2 randomly chosen choice occasions per decider in the test subsample, ### the rest in the train subsample train_test(x, test_number = 2, by = \"T\", random = TRUE, seed = 1) #> $train #> Simulated data of 80 choices. #>  #> $test #> Simulated data of 20 choices. #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform an RprobitB_fit object — transform.RprobitB_fit","title":"Transform an RprobitB_fit object — transform.RprobitB_fit","text":"Given object class RprobitB_fit, function can: change length B burn-period, change thinning factor Q Gibbs samples, change model normalization scale.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform an RprobitB_fit object — transform.RprobitB_fit","text":"","code":"# S3 method for RprobitB_fit transform(   `_data`,   B = NULL,   Q = NULL,   scale = NULL,   check_preference_flip = TRUE,   ... )"},{"path":"https://loelschlaeger.de/RprobitB/reference/transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform an RprobitB_fit object — transform.RprobitB_fit","text":"_data object class RprobitB_fit. B length burn-period, .e. non-negative number samples discarded. Q thinning factor Gibbs samples, .e. every Qth sample kept. scale named list three elements, determining parameter normalization respect utility scale: parameter: Either \"\" (linear coefficient \"alpha\") \"s\" (variance error-term covariance matrix \"Sigma\"). index: index parameter gets fixed. value: value fixed parameter. check_preference_flip Set TRUE check flip preferences new scale. ... Ignored.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform an RprobitB_fit object — transform.RprobitB_fit","text":"transformed RprobitB_fit object.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transform an RprobitB_fit object — transform.RprobitB_fit","text":"See vignette \"Model fitting\" details: vignette(\"v03_model_fitting\", package = \"RprobitB\").","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform an RprobitB_fit object — transform.RprobitB_fit","text":"","code":"data(model_train)  ### change the length B of the burn-in period transform(model_train, B = 1) #> Probit model 'choice ~ price + time + change + comfort | 0'.  ### change the thinning factor Q transform(model_train, Q = 1) #> Probit model 'choice ~ price + time + change + comfort | 0'.  ### change the scale new_scale <- list(parameter = \"s\", index = 1, value = 1) transform(model_train, scale = new_scale) #> Probit model 'choice ~ price + time + change + comfort | 0'."},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_gibbs_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Transformation of Gibbs samples — transform_gibbs_samples","title":"Transformation of Gibbs samples — transform_gibbs_samples","text":"function normalizes, burns thins Gibbs samples.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_gibbs_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transformation of Gibbs samples — transform_gibbs_samples","text":"","code":"transform_gibbs_samples(gibbs_samples, R, B, Q, normalization)"},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_gibbs_samples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transformation of Gibbs samples — transform_gibbs_samples","text":"gibbs_samples output gibbs_sampling, .e. list Gibbs samples Sigma, alpha (P_f>0), s, z, b, Omega (P_r>0). R number iterations Gibbs sampler. B length burn-period, .e. non-negative number samples discarded. Q thinning factor Gibbs samples, .e. every Qth sample kept. normalization object class RprobitB_normalization, can created via RprobitB_normalization.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_gibbs_samples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transformation of Gibbs samples — transform_gibbs_samples","text":"list, first element gibbs_sampes_raw input gibbs_samples, second element normalized, burned, thinned version gibbs_samples called gibbs_samples_nbt. list gets class RprobitB_gibbs_samples.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_parameter.html","id":null,"dir":"Reference","previous_headings":"","what":"Transformation of parameter values — transform_parameter","title":"Transformation of parameter values — transform_parameter","text":"function transforms parameter values based normalization.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_parameter.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transformation of parameter values — transform_parameter","text":"","code":"transform_parameter(parameter, normalization)"},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_parameter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transformation of parameter values — transform_parameter","text":"parameter object class RprobitB_parameter. normalization object class RprobitB_normalization.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/transform_parameter.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transformation of parameter values — transform_parameter","text":"object class RprobitB_parameter.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/undiff_Sigma.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform differenced to non-differenced error term covariance matrix — undiff_Sigma","title":"Transform differenced to non-differenced error term covariance matrix — undiff_Sigma","text":"function transforms differenced error term covariance matrix Sigma back non-differenced error term covariance matrix.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/undiff_Sigma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform differenced to non-differenced error term covariance matrix — undiff_Sigma","text":"","code":"undiff_Sigma(Sigma, i, checks = TRUE, pos = TRUE, labels = TRUE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/undiff_Sigma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform differenced to non-differenced error term covariance matrix — undiff_Sigma","text":"Sigma error term covariance matrix dimension J-1 x J-1 differenced respect alternative . integer, alternative number respect Sigma differenced. checks TRUE function runs additional input transformation checks. pos TRUE function returns positive matrix. labels TRUE function adds labels output matrix.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/undiff_Sigma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform differenced to non-differenced error term covariance matrix — undiff_Sigma","text":"covariance matrix dimension J x J. covariance matrix gets differenced respect alternative , results Sigma.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/undiff_Sigma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform differenced to non-differenced error term covariance matrix — undiff_Sigma","text":"","code":"J <- 3 i <- 2 Sigma_full <- rwishart(3, diag(3))$W Sigma <- delta(J, 2) %*% Sigma_full %*% t(delta(J, 2)) Sigma_back <- RprobitB:::undiff_Sigma(Sigma = Sigma, i = 2)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Omega.html","id":null,"dir":"Reference","previous_headings":"","what":"Update class covariances — update_Omega","title":"Update class covariances — update_Omega","text":"function updates class covariances (independent classes).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Omega.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update class covariances — update_Omega","text":"","code":"update_Omega(beta, b, z, m, nu, Theta)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Omega.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update class covariances — update_Omega","text":"beta matrix decision-maker specific coefficient vectors dimension P_r x N. Set NA P_r = 0. b matrix class means columns dimension P_r x C. Set NA P_r = 0. z vector allocation variables length N. Set NA P_r = 0. m vector class sizes length C. nu degrees freedom (natural number greater P_r) Inverse Wishart prior Omega_c. Theta scale matrix dimension P_r x P_r Inverse Wishart prior Omega_c.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Omega.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update class covariances — update_Omega","text":"matrix updated covariance matrices class columns.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Omega.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update class covariances — update_Omega","text":"following holds independently class \\(c\\). Let \\(\\Omega_c\\) covariance matrix class number c. priori, assume \\(\\Omega_c\\) inverse Wishart distributed \\(\\nu\\) degrees freedom scale matrix \\(\\Theta\\). Let \\((\\beta_n)_{z_n=c}\\) collection \\(\\beta_n\\) currently allocated class \\(c\\), \\(m_c\\) size class \\(c\\), \\(b_c\\) class mean vector. Due conjugacy prior, posterior \\(\\Pr(\\Omega_c \\mid (\\beta_n)_{z_n=c})\\) follows inverted Wishart distribution \\(\\nu + m_c\\) degrees freedom scale matrix \\(\\Theta^{-1} + \\sum_n (\\beta_n - b_c)(\\beta_n - b_c)'\\), product values \\(n\\) \\(z_n=c\\) holds.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Omega.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update class covariances — update_Omega","text":"","code":"### N = 100 decider, P_r = 2 random coefficients, and C = 2 latent classes N <- 100 b <- cbind(c(0,0),c(1,1)) (Omega_true <- matrix(c(1,0.3,0.3,0.5,1,-0.3,-0.3,0.8), ncol=2)) #>      [,1] [,2] #> [1,]  1.0  1.0 #> [2,]  0.3 -0.3 #> [3,]  0.3 -0.3 #> [4,]  0.5  0.8 z <- c(rep(1,N/2),rep(2,N/2)) m <- as.numeric(table(z)) beta <- sapply(z, function(z) rmvnorm(b[,z], matrix(Omega_true[,z],2,2))) ### degrees of freedom and scale matrix for the Wishart prior nu <- 1 Theta <- diag(2) ### updated class covariance matrices (in columns) update_Omega(beta = beta, b = b, z = z, m = m, nu = nu, Theta = Theta) #>           [,1]       [,2] #> [1,] 0.5829456  1.2182977 #> [2,] 0.1225124 -0.4149462 #> [3,] 0.1225124 -0.4149462 #> [4,] 0.5512615  0.9813529"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Sigma.html","id":null,"dir":"Reference","previous_headings":"","what":"Update error term covariance matrix of multiple linear regression — update_Sigma","title":"Update error term covariance matrix of multiple linear regression — update_Sigma","text":"function updates error term covariance matrix multiple linear regression.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Sigma.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update error term covariance matrix of multiple linear regression — update_Sigma","text":"","code":"update_Sigma(kappa, E, N, S)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Sigma.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update error term covariance matrix of multiple linear regression — update_Sigma","text":"kappa degrees freedom (natural number greater J-1) Inverse Wishart prior Sigma. E scale matrix dimension J-1 x J-1 Inverse Wishart prior Sigma. N draw size. S matrix, sum outer products residuals \\((\\epsilon_n)_{n=1,\\dots,N}\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Sigma.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update error term covariance matrix of multiple linear regression — update_Sigma","text":"matrix, draw Inverse Wishart posterior distribution error term covariance matrix multiple linear regression.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Sigma.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update error term covariance matrix of multiple linear regression — update_Sigma","text":"function draws posterior distribution covariance matrix \\(\\Sigma\\) linear utility equation $$U_n = X_n\\beta + \\epsilon_n,$$ \\(U_n\\) (latent, assumed known) utility vector decider \\(n = 1,\\dots,N\\), \\(X_n\\) design matrix build choice characteristics faced \\(n\\), \\(\\beta\\) coefficient vector, \\(\\epsilon_n\\) error term assumed normally distributed mean \\(0\\) unknown covariance matrix \\(\\Sigma\\). priori assume (conjugate) Inverse Wishart distribution $$\\Sigma \\sim W(\\kappa,E)$$ \\(\\kappa\\) degrees freedom scale matrix \\(E\\). posterior \\(\\Sigma\\) Inverted Wishart distribution \\(\\kappa + N\\) degrees freedom scale matrix \\(E^{-1}+S\\), \\(S = \\sum_{n=1}^{N} \\epsilon_n \\epsilon_n'\\) sum outer products residuals \\((\\epsilon_n = U_n - X_n\\beta)_n\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_Sigma.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update error term covariance matrix of multiple linear regression — update_Sigma","text":"","code":"### true error term covariance matrix (Sigma_true <- matrix(c(1,0.5,0.2,0.5,1,0.2,0.2,0.2,2), ncol=3)) #>      [,1] [,2] [,3] #> [1,]  1.0  0.5  0.2 #> [2,]  0.5  1.0  0.2 #> [3,]  0.2  0.2  2.0 ### coefficient vector beta <- matrix(c(-1,1), ncol=1) ### draw data N <- 100 X <- replicate(N, matrix(rnorm(6), ncol=2), simplify = FALSE) eps <- replicate(N, rmvnorm(mu = c(0,0,0), Sigma = Sigma_true), simplify = FALSE) U <- mapply(function(X, eps) X %*% beta + eps, X, eps, SIMPLIFY = FALSE) ### prior parameters for covariance matrix kappa <- 4 E <- diag(3) ### draw from posterior of coefficient vector outer_prod <- function(X, U) (U - X %*% beta) %*% t(U - X %*% beta) S <- Reduce(`+`, mapply(outer_prod, X, U, SIMPLIFY = FALSE)) Sigma_draws <- replicate(100, update_Sigma(kappa, E, N, S)) apply(Sigma_draws, 1:2, mean) #>           [,1]        [,2]        [,3] #> [1,] 1.2433933 0.599333881 0.303754609 #> [2,] 0.5993339 1.126533916 0.002703635 #> [3,] 0.3037546 0.002703635 2.054583290 apply(Sigma_draws, 1:2, stats::sd) #>           [,1]      [,2]      [,3] #> [1,] 0.1910950 0.1469450 0.1866532 #> [2,] 0.1469450 0.1481329 0.1588351 #> [3,] 0.1866532 0.1588351 0.3048582"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_U.html","id":null,"dir":"Reference","previous_headings":"","what":"Update latent utility vector — update_U","title":"Update latent utility vector — update_U","text":"function updates latent utility vector, (independent across deciders choice occasions) utility alternative updated conditional utilities.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_U.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update latent utility vector — update_U","text":"","code":"update_U(U, y, sys, Sigmainv)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_U.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update latent utility vector — update_U","text":"U current utility vector length J-1. y integer 1 J, index chosen alternative. sys vector length J-1, systematic utility part. Sigmainv inverted error term covariance matrix dimension J-1 x J-1.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_U.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update latent utility vector — update_U","text":"updated utility vector length J-1.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_U.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update latent utility vector — update_U","text":"key ingredient Gibbs sampling probit models considering latent utilities parameters can updated (data augmentation). Independently deciders \\(n=1,\\dots,N\\) choice occasions \\(t=1,...,T_n\\), utility vectors \\((U_{nt})_{n,t}\\) linear utility equation \\(U_{nt} = X_{nt} \\beta + \\epsilon_{nt}\\) follow \\(J-1\\)-dimensional truncated normal distribution, \\(J\\) number alternatives, \\(X_{nt} \\beta\\) systematic (.e. non-random) part utility \\(\\epsilon_{nt} \\sim N(0,\\Sigma)\\) error term. truncation points determined choices \\(y_{nt}\\). draw truncated multivariate normal distribution, function implemented approach Geweke (1998) conditionally draw component separately univariate truncated normal distribution. See Oelschläger (2020) concrete formulas.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_U.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Update latent utility vector — update_U","text":"See Geweke (1998) Efficient Simulation Multivariate Normal Student-t Distributions Subject Linear Constraints Evaluation Constraint Probabilities Gibbs sampling truncated multivariate normal distribution. See Oelschläger Bauer (2020) Bayes Estimation Latent Class Mixed Multinomial Probit Models application probit utilities.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_U.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update latent utility vector — update_U","text":"","code":"U <- c(0,0,0) y <- 3 sys <- c(0,0,0) Sigmainv <- solve(diag(3)) update_U(U, y, sys, Sigmainv) #>            [,1] #> [1,] -1.2428306 #> [2,] -1.6710771 #> [3,]  0.8413893"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_b.html","id":null,"dir":"Reference","previous_headings":"","what":"Update class means — update_b","title":"Update class means — update_b","text":"function updates class means (independent classes).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_b.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update class means — update_b","text":"","code":"update_b(beta, Omega, z, m, xi, Dinv)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_b.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update class means — update_b","text":"beta matrix decision-maker specific coefficient vectors dimension P_r x N. Set NA P_r = 0. Omega matrix class covariance matrices columns dimension P_r*P_r x C. Set NA P_r = 0. z vector allocation variables length N. Set NA P_r = 0. m vector class sizes length C. xi mean vector length P_r normal prior b_c. Dinv precision matrix (.e. inverse covariance matrix) dimension P_r x P_r normal prior b_c.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_b.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update class means — update_b","text":"matrix updated means class columns.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_b.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update class means — update_b","text":"following holds independently class \\(c\\). Let \\(b_c\\) mean class number \\(c\\). priori, assume \\(b_c\\) normally distributed mean vector \\(\\xi\\) covariance matrix \\(D\\). Let \\((\\beta_n)_{z_n=c}\\) collection \\(\\beta_n\\) currently allocated class \\(c\\), \\(m_c\\) class size, \\(\\bar{b}_c\\) arithmetic mean. Assuming independence across draws, \\((\\beta_n)_{z_n=c}\\) normal likelihood $$\\prod_n \\phi(\\beta_n \\mid b_c,\\Omega_c),$$ product values \\(n\\) \\(z_n=c\\) holds. Due conjugacy prior, posterior \\(\\Pr(b_c \\mid (\\beta_n)_{z_n=c})\\) follows normal distribution mean $$(D^{-1} + m_c\\Omega_c^{-1})^{-1}(D^{-1}\\xi + m_c\\Omega_c^{-1}\\bar{b}_c)$$ covariance matrix $$(D^{-1} + m_c \\Omega_c^{-1})^{-1}.$$","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_b.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update class means — update_b","text":"","code":"### N = 100 decider, P_r = 2 random coefficients, and C = 2 latent classes N <- 100 (b_true <- cbind(c(0,0),c(1,1))) #>      [,1] [,2] #> [1,]    0    1 #> [2,]    0    1 Omega <- matrix(c(1,0.3,0.3,0.5,1,-0.3,-0.3,0.8), ncol=2) z <- c(rep(1,N/2),rep(2,N/2)) m <- as.numeric(table(z)) beta <- sapply(z, function(z) rmvnorm(b_true[,z], matrix(Omega[,z],2,2))) ### prior mean vector and precision matrix (inverse of covariance matrix) xi <- c(0,0) Dinv <- diag(2) ### updated class means (in columns) update_b(beta = beta, Omega = Omega, z = z, m = m, xi = xi, Dinv = Dinv) #>             [,1]      [,2] #> [1,] -0.29535190 0.7557444 #> [2,] -0.02071499 0.9725114"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_dp.html","id":null,"dir":"Reference","previous_headings":"","what":"Dirichlet process-based update of latent classes — update_classes_dp","title":"Dirichlet process-based update of latent classes — update_classes_dp","text":"function updates latent classes based Dirichlet process.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_dp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dirichlet process-based update of latent classes — update_classes_dp","text":"","code":"update_classes_dp(   Cmax,   beta,   z,   b,   Omega,   delta,   xi,   D,   nu,   Theta,   s_desc = TRUE )"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_dp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dirichlet process-based update of latent classes — update_classes_dp","text":"Cmax maximum number classes. beta matrix decision-maker specific coefficient vectors dimension P_r x N. Set NA P_r = 0. z vector allocation variables length N. Set NA P_r = 0. b matrix class means columns dimension P_r x C. Set NA P_r = 0. Omega matrix class covariance matrices columns dimension P_r*P_r x C. Set NA P_r = 0. delta numeric concentration parameter vector rep(delta,C) Dirichlet prior s. xi mean vector length P_r normal prior b_c. D covariance matrix dimension P_r x P_r normal prior b_c. nu degrees freedom (natural number greater P_r) Inverse Wishart prior Omega_c. Theta scale matrix dimension P_r x P_r Inverse Wishart prior Omega_c. s_desc TRUE, sort classes descending class weight.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_dp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dirichlet process-based update of latent classes — update_classes_dp","text":"list updated values z, b, Omega, s, C.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_dp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Dirichlet process-based update of latent classes — update_classes_dp","text":"added.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_dp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dirichlet process-based update of latent classes — update_classes_dp","text":"","code":"set.seed(1) z <- c(rep(1,20),rep(2,30)) b <- matrix(c(1,1,1,-1), ncol=2) Omega <- matrix(c(1,0.3,0.3,0.5,1,-0.3,-0.3,0.8), ncol=2) beta <- sapply(z, function(z) rmvnorm(b[,z], matrix(Omega[,z],2,2))) delta <- 1 xi <- numeric(2) D <- diag(2) nu <- 4 Theta <- diag(2) RprobitB:::update_classes_dp(   Cmax = 10, beta = beta, z = z, b = b, Omega = Omega,   delta = delta, xi = xi, D = D, nu = nu, Theta = Theta ) #> $z #>       [,1] #>  [1,]    2 #>  [2,]    2 #>  [3,]    2 #>  [4,]    2 #>  [5,]    2 #>  [6,]    2 #>  [7,]    1 #>  [8,]    2 #>  [9,]    2 #> [10,]    2 #> [11,]    2 #> [12,]    1 #> [13,]    2 #> [14,]    2 #> [15,]    2 #> [16,]    2 #> [17,]    2 #> [18,]    3 #> [19,]    1 #> [20,]    2 #> [21,]    1 #> [22,]    1 #> [23,]    1 #> [24,]    1 #> [25,]    1 #> [26,]    1 #> [27,]    1 #> [28,]    1 #> [29,]    1 #> [30,]    1 #> [31,]    1 #> [32,]    1 #> [33,]    1 #> [34,]    3 #> [35,]    2 #> [36,]    1 #> [37,]    1 #> [38,]    1 #> [39,]    1 #> [40,]    1 #> [41,]    1 #> [42,]    1 #> [43,]    1 #> [44,]    1 #> [45,]    1 #> [46,]    1 #> [47,]    1 #> [48,]    1 #> [49,]    1 #> [50,]    3 #>  #> $b #>            [,1]     [,2]       [,3] #> [1,]  1.1955613 1.381324 -0.4912123 #> [2,] -0.9791527 1.218404  0.4543349 #>  #> $Omega #>            [,1]      [,2]        [,3] #> [1,]  0.7161232 0.4793177  0.37048920 #> [2,] -0.1869330 0.1085593 -0.03874837 #> [3,] -0.1869330 0.1085593 -0.03874837 #> [4,]  0.5616673 0.2745555  0.37381786 #>  #> $s #> [1] 0.60 0.34 0.06 #>  #> $C #> [1] 3 #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_wb.html","id":null,"dir":"Reference","previous_headings":"","what":"Weight-based update of latent classes — update_classes_wb","title":"Weight-based update of latent classes — update_classes_wb","text":"function updates latent classes based class weights.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_wb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weight-based update of latent classes — update_classes_wb","text":"","code":"update_classes_wb(Cmax, epsmin, epsmax, distmin, s, b, Omega)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_wb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weight-based update of latent classes — update_classes_wb","text":"Cmax maximum number classes. epsmin threshold weight (0 1) removing class. epsmax threshold weight (0 1) splitting class. distmin (non-negative) threshold difference class means joining two classes. s vector class weights length C. Set NA P_r = 0. identifiability, vector must non-ascending. b matrix class means columns dimension P_r x C. Set NA P_r = 0. Omega matrix class covariance matrices columns dimension P_r*P_r x C. Set NA P_r = 0.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_wb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Weight-based update of latent classes — update_classes_wb","text":"list updated values s, b, Omega.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_wb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Weight-based update of latent classes — update_classes_wb","text":"updating scheme bases following rules: remove class \\(c\\), \\(s_c<\\epsilon_{\\text{min}}\\), .e. class weight \\(s_c\\) drops threshold \\(\\epsilon_{\\text{min}}\\). case indicates class \\(c\\) negligible impact mixing distribution. split class \\(c\\) two classes \\(c_1\\) \\(c_2\\), \\(s_c>\\epsilon_\\text{max}\\). case indicates class \\(c\\) high influence mixing distribution whose approximation can potentially improved increasing resolution directions high variance. Therefore, class means \\(b_{c_1}\\) \\(b_{c_2}\\) new classes \\(c_1\\) \\(c_2\\) shifted opposite directions class mean \\(b_c\\) old class \\(c\\) direction highest variance. join two classes \\(c_1\\) \\(c_2\\) one class \\(c\\), \\(\\lVert b_{c_1} - b_{c_2} \\rVert<\\epsilon_{\\text{distmin}}\\), .e. euclidean distance class means \\(b_{c_1}\\) \\(b_{c_2}\\) drops threshold \\(\\epsilon_{\\text{distmin}}\\). case indicates location redundancy repealed. parameters \\(c\\) assigned adding values \\(s\\) \\(c_1\\) \\(c_2\\) averaging values \\(b\\) \\(\\Omega\\). rules executed order, one rule per iteration Cmax exceeded.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_classes_wb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weight-based update of latent classes — update_classes_wb","text":"","code":"### parameter settings s <- c(0.8,0.2) b <- matrix(c(1,1,1,-1), ncol=2) Omega <- matrix(c(0.5,0.3,0.3,0.5,1,-0.1,-0.1,0.8), ncol=2)  ### Remove class 2 RprobitB:::update_classes_wb(Cmax = 10, epsmin = 0.3, epsmax = 0.9, distmin = 1,                              s = s, b = b, Omega = Omega) #> $s #>      [,1] #> [1,]    1 #>  #> $b #>      [,1] #> [1,]    1 #> [2,]    1 #>  #> $Omega #>      [,1] #> [1,]  0.5 #> [2,]  0.3 #> [3,]  0.3 #> [4,]  0.5 #>   ### Split class 1 RprobitB:::update_classes_wb(Cmax = 10, epsmin = 0.1, epsmax = 0.7, distmin = 1,                              s = s, b = b, Omega = Omega) #> $s #>      [,1] [,2] [,3] #> [1,]  0.4  0.4  0.2 #>  #> $b #>      [,1] [,2] [,3] #> [1,] 0.75 1.25    1 #> [2,] 1.00 1.00   -1 #>  #> $Omega #>      [,1] [,2] [,3] #> [1,] 0.25 0.25  1.0 #> [2,] 0.15 0.15 -0.1 #> [3,] 0.15 0.15 -0.1 #> [4,] 0.25 0.25  0.8 #>   ### Join classes RprobitB:::update_classes_wb(Cmax = 10, epsmin = 0.1, epsmax = 0.9, distmin = 3,                              s = s, b = b, Omega = Omega) #> $s #>      [,1] #> [1,]    1 #>  #> $b #>      [,1] #> [1,]    1 #> [2,]    0 #>  #> $Omega #>      [,1] #> [1,] 0.75 #> [2,] 0.10 #> [3,] 0.10 #> [4,] 0.65 #>"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_m.html","id":null,"dir":"Reference","previous_headings":"","what":"Update class sizes — update_m","title":"Update class sizes — update_m","text":"function updates class size vector.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_m.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update class sizes — update_m","text":"","code":"update_m(C, z, nozero = FALSE)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_m.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update class sizes — update_m","text":"C number (greater equal 1) latent classes decision makers. Set NA P_r = 0. Otherwise, C = 1 per default. z vector allocation variables length N. Set NA P_r = 0. nozero TRUE, element output vector m least one (numerical stability).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_m.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update class sizes — update_m","text":"updated class size vector.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_m.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update class sizes — update_m","text":"","code":"update_m(C = 3, z = c(1,1,1,2,2,3)) #>      [,1] #> [1,]    3 #> [2,]    2 #> [3,]    1"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"Update coefficient vector of multiple linear regression — update_reg","title":"Update coefficient vector of multiple linear regression — update_reg","text":"function updates coefficient vector multiple linear regression.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update coefficient vector of multiple linear regression — update_reg","text":"","code":"update_reg(mu0, Tau0, XSigX, XSigU)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update coefficient vector of multiple linear regression — update_reg","text":"mu0 mean vector normal prior distribution coefficient vector. Tau0 precision matrix (.e. inverted covariance matrix) normal prior distribution coefficient vector. XSigX matrix \\(\\sum_{n=1}^N X_n'\\Sigma^{-1}X_n\\). See details. XSigU vector \\(\\sum_{n=1}^N X_n'\\Sigma^{-1}U_n\\). See details.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_reg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update coefficient vector of multiple linear regression — update_reg","text":"vector, draw normal posterior distribution coefficient vector multiple linear regression.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update coefficient vector of multiple linear regression — update_reg","text":"function draws posterior distribution \\(\\beta\\) linear utility equation $$U_n = X_n\\beta + \\epsilon_n,$$ \\(U_n\\) (latent, assumed known) utility vector decider \\(n = 1,\\dots,N\\), \\(X_n\\) design matrix build choice characteristics faced \\(n\\), \\(\\beta\\) unknown coefficient vector (can either fixed coefficient vector \\(\\alpha\\) decider-specific coefficient vector \\(\\beta_n\\)), \\(\\epsilon_n\\) error term assumed normally distributed mean \\(0\\) (known) covariance matrix \\(\\Sigma\\). priori assume (conjugate) normal prior distribution $$\\beta \\sim N(\\mu_0,\\Tau_0)$$ mean vector \\(\\mu_0\\) precision matrix (.e. inverted covariance matrix) \\(\\Tau_0\\). posterior distribution \\(\\beta\\) normal covariance matrix $$\\Sigma_1 = (\\Tau_0 + \\sum_{n=1}^N X_n'\\Sigma^{-1}X_n)^{-1}$$ mean vector $$\\mu_1 = \\Sigma_1(\\Tau_0\\mu_0 + \\sum_{n=1}^N X_n'\\Sigma^{-1}U_n)$$. Note analogy \\(\\mu_1\\) generalized least squares estimator $$\\hat{\\beta}_\\text{GLS} = (\\sum_{n=1}^N X_n'\\Sigma^{-1}X_n)^{-1} \\sum_{n=1}^N X_n'\\Sigma^{-1}U_n$$ becomes weighted prior parameters \\(\\mu_0\\) \\(\\Tau_0\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update coefficient vector of multiple linear regression — update_reg","text":"","code":"### true coefficient vector beta_true <- matrix(c(-1,1), ncol=1) ### error term covariance matrix Sigma <- matrix(c(1,0.5,0.2,0.5,1,0.2,0.2,0.2,2), ncol=3) ### draw data N <- 100 X <- replicate(N, matrix(rnorm(6), ncol=2), simplify = FALSE) eps <- replicate(N, rmvnorm(mu = c(0,0,0), Sigma = Sigma), simplify = FALSE) U <- mapply(function(X, eps) X %*% beta_true + eps, X, eps, SIMPLIFY = FALSE) ### prior parameters for coefficient vector mu0 <- c(0,0) Tau0 <- diag(2) ### draw from posterior of coefficient vector XSigX <- Reduce(`+`, lapply(X, function(X) t(X) %*% solve(Sigma) %*% X)) XSigU <- Reduce(`+`, mapply(function(X, U) t(X) %*% solve(Sigma) %*% U, X, U, SIMPLIFY = FALSE)) beta_draws <- replicate(100, update_reg(mu0, Tau0, XSigX, XSigU), simplify = TRUE) rowMeans(beta_draws) #> [1] -1.071996  0.986084"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_s.html","id":null,"dir":"Reference","previous_headings":"","what":"Update class weight vector — update_s","title":"Update class weight vector — update_s","text":"function updates class weight vector drawing posterior distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_s.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update class weight vector — update_s","text":"","code":"update_s(delta, m)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_s.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update class weight vector — update_s","text":"delta numeric concentration parameter vector rep(delta,C) Dirichlet prior s. m vector current class frequencies.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_s.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update class weight vector — update_s","text":"vector, draw Dirichlet posterior distribution s.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_s.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update class weight vector — update_s","text":"Let \\(m=(m_1,\\dots,m_C)\\) frequencies \\(C\\) classes. Given class weight (probability) vector \\(s=(s_1,\\dots,s_C)\\), distribution \\(m\\) multinomial likelihood $$L(m\\mid s) \\propto \\prod_{=1}^C s_i^{m_i}.$$ conjugate prior \\(p(s)\\) \\(s\\) Dirichlet distribution, density function proportional $$\\prod_{=1}^C s_i^{\\delta_i-1},$$ \\(\\delta = (\\delta_1,\\dots,\\delta_C)\\) concentration parameter vector. Note RprobitB, \\(\\delta_1=\\dots=\\delta_C\\). restriction necessary class number \\(C\\) can change. posterior distribution \\(s\\) proportional $$p(s) L(m\\mid s) \\propto \\prod_{=1}^C s_i^{\\delta_i + m_i - 1},$$ turn proportional Dirichlet distribution parameters \\(\\delta+m\\).","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_s.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update class weight vector — update_s","text":"","code":"### number of classes C <- 4 ### current class sizes m <- sample.int(C) ### concentration parameter for Dirichlet prior (single-valued) delta <- 1 ### updated class weight vector update_s(delta = 1, m = m) #>           [,1] #> [1,] 0.1839418 #> [2,] 0.1893315 #> [3,] 0.2454951 #> [4,] 0.3812317"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_z.html","id":null,"dir":"Reference","previous_headings":"","what":"Update class allocation vector — update_z","title":"Update class allocation vector — update_z","text":"function updates class allocation vector (independently observations) drawing conditional distribution.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_z.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update class allocation vector — update_z","text":"","code":"update_z(s, beta, b, Omega)"},{"path":"https://loelschlaeger.de/RprobitB/reference/update_z.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update class allocation vector — update_z","text":"s vector class weights length C. Set NA P_r = 0. identifiability, vector must non-ascending. beta matrix decision-maker specific coefficient vectors dimension P_r x N. Set NA P_r = 0. b matrix class means columns dimension P_r x C. Set NA P_r = 0. Omega matrix class covariance matrices columns dimension P_r*P_r x C. Set NA P_r = 0.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_z.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update class allocation vector — update_z","text":"updated class allocation vector.","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_z.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update class allocation vector — update_z","text":"Let \\(z = (z_1,\\dots,z_N)\\) denote class allocation vector observations (mixed coefficients) \\(\\beta = (\\beta_1,\\dots,\\beta_N)\\). Independently \\(n\\), conditional probability \\(\\Pr(z_n = c \\mid s,\\beta_n,b,\\Omega)\\) \\(\\beta_n\\) allocated class \\(c\\) \\(c=1,\\dots,C\\) depends class allocation vector \\(s\\), class means \\(b=(b_c)_c\\) class covariance matrices \\(Omega=(Omega_c)_c\\) proportional $$s_c \\phi(\\beta_n \\mid b_c,Omega_c).$$","code":""},{"path":"https://loelschlaeger.de/RprobitB/reference/update_z.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update class allocation vector — update_z","text":"","code":"### class weights for C = 2 classes s <- rdirichlet(c(1,1)) ### coefficient vector for N = 1 decider and P_r = 2 random coefficients beta <- matrix(c(1,1), ncol = 1) ### class means and covariances b <- cbind(c(0,0),c(1,1)) Omega <- cbind(c(1,0,0,1),c(1,0,0,1)) ### updated class allocation vector update_z(s = s, beta = beta, b = b, Omega = Omega) #>      [,1] #> [1,]    1"},{"path":"https://loelschlaeger.de/RprobitB/news/index.html","id":"rprobitb-1009000","dir":"Changelog","previous_headings":"","what":"RprobitB 1.0.0.9000","title":"RprobitB 1.0.0.9000","text":"Added print method RprobitB_parameter. function choice_probs now called choice_probabilities() make functionality clearer. Splitting data set train test part can now done function train_test(). Consequently, argument test_prob removed prepare_data() simulate_choices(). function simulate_choices() argument distr anymore. Instead, covariates can supplied via covariates argument. Consequently, argument standardize removed well. function compare now called model_selection() make functionality clearer. function prepare now called prepare_data() make functionality clearer. function simulate now called simulate_choices() mask stats::simulate(). README file now R Markdown format.","code":""},{"path":"https://loelschlaeger.de/RprobitB/news/index.html","id":"rprobitb-010","dir":"Changelog","previous_headings":"","what":"RprobitB 0.1.0","title":"RprobitB 0.1.0","text":"CRAN release: 2021-05-15 Initial version.","code":""}]
