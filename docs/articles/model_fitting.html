<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="RprobitB">
<title>Model fitting • RprobitB</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.0/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.0/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Model fitting">
<meta property="og:description" content="RprobitB">
<meta property="og:image" content="https://loelschlaeger.de/RprobitB/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">RprobitB</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-2">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/choice_data.html">Choice data</a>
    <a class="dropdown-item" href="../articles/choice_prediction.html">Choice prediction</a>
    <a class="dropdown-item" href="../articles/introduction.html">Introduction</a>
    <a class="dropdown-item" href="../articles/model_definition.html">Model definition</a>
    <a class="dropdown-item" href="../articles/model_fitting.html">Model fitting</a>
    <a class="dropdown-item" href="../articles/model_selection.html">Model selection</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/loelschlaeger/RprobitB/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">

<div class="row">
  <main id="main"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Model fitting</h1>
                        <h4 data-toc-skip class="author">Lennart Oelschläger</h4>
            
            <h4 data-toc-skip class="date">2022-02-05</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/loelschlaeger/RprobitB/blob/HEAD/vignettes/model_fitting.Rmd" class="external-link"><code>vignettes/model_fitting.Rmd</code></a></small>
      <div class="d-none name"><code>model_fitting.Rmd</code></div>
    </div>

    
    
<p><strong>RprobitB</strong> estimates a (latent class) (mixed) (multinomial) probit model in a <a href="#bayes-estimation-of-the-probit-model-via-gibbs-sampling">Bayesian framework via Gibbs sampling</a>.</p>
<p>To fit a model to choice data, apply the function</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">model</span> <span class="op">=</span> <span class="fu"><a href="../reference/mcmc.html">mcmc</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">data</span><span class="op">)</span></code></pre></div>
<p>where <code>data</code> must be the output of either <code>prepare()</code> or <code><a href="https://rdrr.io/r/stats/simulate.html" class="external-link">simulate()</a></code>, see <a href="data_management.html">the vignette about data management</a>.</p>
<p>The function <code><a href="../reference/mcmc.html">mcmc()</a></code> has the following optional arguments:</p>
<ul>
<li>
<p><code>scale</code>: A named list of three elements, determining the parameter normalization with respect to the utility scale (see <a href="introduction_to_RprobitB_and_model_formulation.html">the introductory vignette</a>)<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Per default, the first error-term variance is fixed to 1, i.e. &lt;code&gt;scale = list("parameter" = "s", "index" = 1, "value" = 1)&lt;/code&gt;. Note that you can set &lt;code&gt;"parameter" = "a"&lt;/code&gt; only if the model has parameters with a fixed coefficient (i.e. &lt;code&gt;P_f&amp;gt;0&lt;/code&gt;).&lt;/p&gt;'><sup>1</sup></a>:</p>
<ul>
<li><p><code>parameter</code>: Either <code>"a"</code> (for a linear coefficient of <code>"alpha"</code>) or <code>"s"</code> (for a variance of the error-term covariance matrix <code>"Sigma"</code>).</p></li>
<li><p><code>index</code>: The index of the parameter that gets fixed.</p></li>
<li><p><code>value</code>: The value for the fixed parameter.</p></li>
</ul>
</li>
<li><p><code>R</code>: The number of iterations of the Gibbs sampler.</p></li>
<li><p><code>B</code>: The length of the burn-in period, i.e. a non-negative number of samples to be discarded. See <a href="#burning-and-thinning">below</a> for details.</p></li>
<li><p><code>Q</code>: The thinning factor for the Gibbs samples, i.e. only every <code>Q</code>th sample is kept. See <a href="#burning-and-thinning">below</a> for details.</p></li>
<li><p><code>print_progress</code>: A boolean, determining whether to print the Gibbs sampler progress and the estimated remaining computation time.</p></li>
<li>
<p><code>prior</code>: A named list of parameters for the <a href="#prior-settings">prior distributions of the normalized parameters</a>:</p>
<ul>
<li><p><code>eta</code>: The mean vector of length <code>P_f</code> of the normal prior for <code>alpha</code>.</p></li>
<li><p><code>Psi</code>: The covariance matrix of dimension <code>P_f</code> x <code>P_f</code> of the normal prior for <code>alpha</code>.</p></li>
<li><p><code>delta</code>: The concentration parameter of length 1 of the Dirichlet prior for <code>s</code>.</p></li>
<li><p><code>xi</code>: The mean vector of length <code>P_r</code> of the normal prior for each <code>b_c</code>.</p></li>
<li><p><code>D</code>: The covariance matrix of dimension <code>P_r</code> x <code>P_r</code> of the normal prior for each <code>b_c</code>.</p></li>
<li><p><code>nu</code>: The degrees of freedom (a natural number greater than <code>P_r</code>) of the Inverse Wishart prior for each <code>Omega_c</code>.</p></li>
<li><p><code>Theta</code>: The scale matrix of dimension <code>P_r</code> x <code>P_r</code> of the Inverse Wishart prior for each <code>Omega_c</code>.</p></li>
<li><p><code>kappa</code>: The degrees of freedom (a natural number greater than <code>J-1</code>) of the Inverse Wishart prior for <code>Sigma</code>.</p></li>
<li><p><code>E</code>: The scale matrix of dimension <code>J-1</code> x <code>J-1</code> of the Inverse Wishart prior for <code>Sigma</code>.</p></li>
</ul>
</li>
<li>
<p><code>latent_classes</code>: A list of parameters specifying the number and <a href="#updating-the-number-of-latent-classes">the updating scheme of latent classes</a>:</p>
<ul>
<li><p><code>C</code>: The number (greater or equal 1) of latent classes. Set to 1 per default and is ignored if <code>P_r = 0</code></p></li>
<li><p><code>update</code>: A boolean, determining whether to update <code>C</code>. Ignored if <code>P_r = 0</code>. If <code>update = FALSE</code>, all of the following elements are ignored.</p></li>
<li><p><code>Cmax</code>: The maximum number of latent classes.</p></li>
<li><p><code>buffer</code>: The updating buffer (number of iterations to wait before the next update).</p></li>
<li><p><code>epsmin</code>: The threshold weight for removing latent classes (between 0 and 1).</p></li>
<li><p><code>epsmax</code>: The threshold weight for splitting latent classes (between 0 and 1).</p></li>
<li><p><code>distmin</code>: The threshold difference in means for joining latent classes (non-negative).</p></li>
</ul>
</li>
</ul>
<div class="section level2">
<h2 id="prior-settings">Prior settings<a class="anchor" aria-label="anchor" href="#prior-settings"></a>
</h2>
<p>Bayesian analysis enables to impose prior beliefs on the model parameters. It is possible to either express strong prior knowledge using informative prior distributions or to express vague knowledge using diffuse prior distributions. <strong>RprobitB</strong> applies the following conjugate priors:</p>
<ul>
<li><p><span class="math inline">\((s_1,\dots,s_C)\sim D_C(\delta)\)</span>, where <span class="math inline">\(D_C(\delta)\)</span> denotes the <span class="math inline">\(C\)</span>-dimensional Dirichlet distribution with concentration parameter vector <span class="math inline">\(\delta = (\delta_1,\dots,\delta_C)\)</span>,</p></li>
<li><p><span class="math inline">\(\alpha\sim \text{MVN}{P_f}(\psi,\Psi)\)</span>,</p></li>
<li><p><span class="math inline">\(b_c \sim \text{MVN}{P_r}(\xi,\Xi)\)</span>, independent for all <span class="math inline">\(c\)</span>,</p></li>
<li><p><span class="math inline">\(\Omega_c \sim W^{-1}_{P_r}(\nu,\Theta)\)</span>, independent for all <span class="math inline">\(c\)</span>, where <span class="math inline">\(W^{-1}_{P_r}(\nu,\Theta)\)</span> denotes the <span class="math inline">\(P_r\)</span>-dimensional inverse Wishart distribution with <span class="math inline">\(\nu\)</span> degrees of freedom and scale matrix <span class="math inline">\(\Theta\)</span>,</p></li>
<li><p>and <span class="math inline">\(\Sigma \sim W^{-1}_{J-1}(\kappa,\Lambda)\)</span>.</p></li>
</ul>
<p>Per default, <strong>RprobitB</strong> applies the diffuse prior approach, setting <span class="math inline">\(\delta_1=\dots=\delta_C=1\)</span>; <span class="math inline">\(\psi\)</span> and <span class="math inline">\(\xi\)</span> equal to the zero vector; <span class="math inline">\(\Psi\)</span> and <span class="math inline">\(\Xi\)</span> equal to the identity matrix; <span class="math inline">\(\nu\)</span> and <span class="math inline">\(\kappa\)</span> equal to <span class="math inline">\(P_r+2\)</span> and <span class="math inline">\(J+1\)</span>, respectively (to obtain proper priors); <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(\Lambda\)</span> equal to the identity matrix.</p>
<p>Alternatively, the parameters can be chosen based on estimation results of similar choice settings, resulting in informative priors.</p>
</div>
<div class="section level2">
<h2 id="bayes-estimation-of-the-probit-model-via-gibbs-sampling">Bayes estimation of the probit model via Gibbs sampling<a class="anchor" aria-label="anchor" href="#bayes-estimation-of-the-probit-model-via-gibbs-sampling"></a>
</h2>
<p>The Bayesian analysis of the (latent class) (mixed) (multinomial) probit model builds upon the work of <span class="citation">(McCulloch and Rossi 1994)</span>, <span class="citation">(Nobile 1998)</span>, <span class="citation">(Allenby and Rossi 1998)</span>, and <span class="citation">(Imai and Dyk 2005)</span>. A key ingredient is the concept of data augmentation, cf. <span class="citation">(Albert and Chib 1993)</span>, which treats the latent utilities as parameters themselves. Conditional on the latent utilities, the multinomial probit model constitutes a standard Bayesian linear regression set-up, which renders drawing from the posterior distribution feasible without the need to evaluate any likelihood.</p>
<p>Gibbs sampling from the joint posterior distribution of a latent class mixed multinomial probit model proceeds by iteratively drawing and updating each model parameter conditional on the other parameters.</p>
<ul>
<li><p>The class weights are drawn from the Dirichlet distribution <span class="math display">\[\begin{equation}
(s_1,\dots,s_C)\mid \delta,z \sim D_C(\delta_1+m_1,\dots,\delta_C+m_C),
\end{equation}\]</span> where for <span class="math inline">\(c=1,\dots,C\)</span>, <span class="math inline">\(m_c=\#\{n:z_n=c\}\)</span> denotes the current absolute class size. Mind that the model is invariant to permutations of the class labels <span class="math inline">\(1,\dots,C\)</span>. For that reason, we accept an update only if the ordering <span class="math inline">\(s_1&lt;\dots&lt;s_C\)</span> holds, thereby ensuring a unique labeling of the classes.</p></li>
<li><p>Independently for all <span class="math inline">\(n\)</span>, we update the allocation variables <span class="math inline">\((z_n)_n\)</span> from their conditional distribution <span class="math display">\[\begin{equation}
\text{Prob}(z_n=c\mid s,\beta,b,\Omega )=\frac{s_c\phi_{P_r}(\beta_n\mid b_c,\Omega_c)}{\sum_c s_c\phi_{P_r}(\beta_n\mid b_c,\Omega_c)}.
\end{equation}\]</span></p></li>
<li><p>The class means <span class="math inline">\((b_c)_c\)</span> are updated independently for all <span class="math inline">\(c\)</span> via <span class="math display">\[\begin{equation}
b_c\mid \Xi,\Omega,\xi,z,\beta \sim\text{MVN}{P_r}\left( \mu_{b_c}, \Sigma_{b_c}  \right),
\end{equation}\]</span> where <span class="math inline">\(\mu_{b_c}=(\Xi^{-1}+m_c\Omega_c^{-1})^{-1}(\Xi^{-1}\xi +m_c\Omega_c^{-1}\bar{b}_c)\)</span>, <span class="math inline">\(\Sigma_{b_c}=(\Xi^{-1}+m_c\Omega_c^{-1})^{-1}\)</span>, <span class="math inline">\(\bar{b}_c=m_c^{-1}\sum_{n:z_n=c} \beta_n\)</span>.</p></li>
<li><p>The class covariance matrices <span class="math inline">\((\Omega_c)_c\)</span> are updated independently for all <span class="math inline">\(c\)</span> via <span class="math display">\[\begin{equation}
\Omega_c \mid \nu,\Theta,z,\beta,b \sim W^{-1}_{P_r}(\mu_{\Omega_c},\Sigma_{\Omega_c}),
\end{equation}\]</span> where <span class="math inline">\(\mu_{\Omega_c}=\nu+m_c\)</span> and <span class="math inline">\(\Sigma_{\Omega_c}=\Theta^{-1} + \sum_{n:z_n=c} (\beta_n-b_c)(\beta_n-b_c)'\)</span>.</p></li>
<li><p>Independently for all <span class="math inline">\(n\)</span> and <span class="math inline">\(t\)</span> and conditionally on the other components, the utility vectors <span class="math inline">\((U_{nt:})\)</span> follow a <span class="math inline">\(J-1\)</span>-dimensional truncated multivariate normal distribution, where the truncation points are determined by the choices <span class="math inline">\(y_{nt}\)</span>. To sample from a truncated multivariate normal distribution, we apply a sub-Gibbs sampler, following the approach of : <span class="math display">\[\begin{equation}
U_{ntj} \mid U_{nt(-j)},y_{nt},\Sigma,W,\alpha,X,\beta 
\sim \mathcal{N}(\mu_{U_{ntj}},\Sigma_{U_{ntj}}) \cdot \begin{cases}
1(U_{ntj}&gt;\max(U_{nt(-j)},0) ) &amp; \text{if}~ y_{nt}=j\\
1(U_{ntj}&lt;\max(U_{nt(-j)},0) ) &amp; \text{if}~ y_{nt}\neq j
\end{cases},
\end{equation}\]</span> where <span class="math inline">\(U_{nt(-j)}\)</span> denotes the vector <span class="math inline">\((U_{nt:})\)</span> without the element <span class="math inline">\(U_{ntj}\)</span>, <span class="math inline">\(\mathcal{N}\)</span> denotes the univariate normal distribution, <span class="math inline">\(\Sigma_{U_{ntj}} = 1/(\Sigma^{-1})_{jj}\)</span> and <span class="math display">\[\begin{equation}
\mu_{U_{ntj}} = W_{ntj}'\alpha + X_{ntj}'\beta_n - \Sigma_{U_{ntj}} (\Sigma^{-1})_{j(-j)}   (U_{nt(-j)} - W_{nt(-j)}'\alpha - X_{nt(-j)}' \beta_n ),
\end{equation}\]</span> where <span class="math inline">\((\Sigma^{-1})_{jj}\)</span> denotes the <span class="math inline">\((j,j)\)</span>th element of <span class="math inline">\(\Sigma^{-1}\)</span>, <span class="math inline">\((\Sigma^{-1})_{j(-j)}\)</span> the <span class="math inline">\(j\)</span>th row without the <span class="math inline">\(j\)</span>th entry, <span class="math inline">\(W_{nt(-j)}\)</span> and <span class="math inline">\(X_{nt(-j)}\)</span> the coefficient matrices <span class="math inline">\(W_{nt}\)</span> and <span class="math inline">\(X_{nt}\)</span>, respectively, without the <span class="math inline">\(j\)</span>th column.</p></li>
<li><p>Updating the fixed coefficient vector <span class="math inline">\(\alpha\)</span> is achieved by applying the formula for Bayesian linear regression of the regressors <span class="math inline">\(W_{nt}\)</span> on the regressands <span class="math inline">\((U_{nt:})-X_{nt}'\beta_n\)</span>, i.e. <span class="math display">\[\begin{equation}
\alpha \mid \Psi,\psi,W,\Sigma,U,X,\beta \sim \text{MVN}{P_f}(\mu_\alpha,\Sigma_\alpha),
\end{equation}\]</span> where <span class="math inline">\(\mu_\alpha = \Sigma_\alpha (\Psi^{-1}\psi + \sum_{n=1,t=1}^{N,T} W_{nt} \Sigma^{-1} ((U_{nt:})-X_{nt}'\beta_n) )\)</span> and <span class="math inline">\(\Sigma_\alpha = (\Psi^{-1} + \sum_{n=1,t=1}^{N,T} W_{nt}\Sigma^{-1} W_{nt}^{'} )^{-1}\)</span>.</p></li>
<li><p>Analogously to <span class="math inline">\(\alpha\)</span>, the random coefficients <span class="math inline">\((\beta_n)_n\)</span> are updated independently via <span class="math display">\[\begin{equation}
\beta_n \mid \Omega,b,X,\Sigma,U,W,\alpha \sim \text{MVN}{P_r}(\mu_{\beta_n},\Sigma_{\beta_n}),
\end{equation}\]</span> where <span class="math inline">\(\mu_{\beta_n} = \Sigma_{\beta_n} (\Omega_{z_n}^{-1}b_{z_n} + \sum_{t=1}^{T} X_{nt} \Sigma^{-1} (U_{nt}-W_{nt}'\alpha) )\)</span> and <span class="math inline">\(\Sigma_{\beta_n} = (\Omega_{z_n}^{-1} + \sum_{t=1}^{T} X_{nt}\Sigma^{-1} X_{nt}^{'} )^{-1}\)</span> .</p></li>
<li><p>The error term covariance matrix <span class="math inline">\(\Sigma\)</span> is updated by means of <span class="math display">\[\begin{equation}
\Sigma \mid \kappa,\Lambda,U,W,\alpha,X,\beta \sim W^{-1}_{J-1}(\kappa+NT,\Lambda+S), \\
\end{equation}\]</span> where <span class="math inline">\(S = \sum_{n=1,t=1}^{N,T} \varepsilon_{nt} \varepsilon_{nt}'\)</span> and <span class="math inline">\(\varepsilon_{nt} = (U_{nt:}) - W_{nt}'\alpha - X_{nt}'\beta_n\)</span>.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="parameter-normalization">Parameter normalization<a class="anchor" aria-label="anchor" href="#parameter-normalization"></a>
</h2>
<p>Samples obtained from the scheme described above still lack identification (see <a href="introduction_to_RprobitB_and_model_formulation.html">the introductory vignette</a>). Therefore, subsequent to the sampling, the normalizations</p>
<ul>
<li><p><span class="math inline">\(\alpha^{(i)}/\sqrt{(\Sigma^{(i)})_{11}}\)</span>,</p></li>
<li><p><span class="math inline">\(b_c^{(i)}/\sqrt{(\Sigma^{(i)})_{11}}\)</span>,</p></li>
<li><p><span class="math inline">\(\Omega_c^{(i)}/(\Sigma^{(i)})_{11}\)</span>, <span class="math inline">\(c=1,\dots,C\)</span> and</p></li>
<li><p><span class="math inline">\(\Sigma^{(i)}/(\Sigma^{(i)})_{11}\)</span></p></li>
</ul>
<p>are required for the <span class="math inline">\(i\)</span>th updates in each iterations <span class="math inline">\(i\)</span>, cf. <span class="citation">(Imai and Dyk 2005)</span>, where <span class="math inline">\((\Sigma^{(i)})_{11}\)</span> denotes the top-left element of <span class="math inline">\(\Sigma^{(i)}\)</span>.</p>
<p>The draws for <span class="math inline">\(s\)</span> and <span class="math inline">\(z\)</span> do not need to be normalized. The draws for <span class="math inline">\(U\)</span> and <span class="math inline">\(\beta\)</span> could be normalized if the results are of interest in the analysis.</p>
<p>Alternatively, the samples can be normalized such that any variance of <span class="math inline">\(\Sigma\)</span> or any element of <span class="math inline">\(\alpha\)</span> equals any fixed non-negative value.</p>
<p>The normalization of a fitted model can be changed afterwards via</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/transform.html" class="external-link">transform</a></span><span class="op">(</span><span class="va">model</span>, scale <span class="op">=</span> <span class="va">scale</span><span class="op">)</span></code></pre></div>
<p>where <code>model</code> is the output of <code><a href="../reference/mcmc.html">mcmc()</a></code> and <code>scale</code> is a named list of three elements, determining the parameter normalization, as described above.</p>
</div>
<div class="section level2">
<h2 id="burning-and-thinning">Burning and thinning<a class="anchor" aria-label="anchor" href="#burning-and-thinning"></a>
</h2>
<p>The theory behind Gibbs sampling constitutes that the sequence of samples produced by the updating scheme can be considered as a Markov chain with stationary distribution equal to the desired joint posterior distribution. It takes a certain number of iterations for that stationary distribution to be approximated reasonably well. Therefore, it is common practice to discard the first <span class="math inline">\(B\)</span> out of <span class="math inline">\(R\)</span> samples (the so-called burn-in period). Furthermore, correlation between nearby samples should be expected. In order to obtain independent samples, we consider only every <span class="math inline">\(Q\)</span>th sample when averaging values to compute parameter statistics like expectation and standard deviation.</p>
<p>Adequate values for <span class="math inline">\(R\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(Q\)</span> depend on the complexity of the considered Bayesian framework. Per default, <strong>RprobitB</strong> sets <code>R = 1e4</code>, <code>B = R/2</code> and <code>Q = 10</code>.</p>
<p>The independence of the samples can be verified by computing the serial correlation and the convergence of the Gibbs sampler can be checked by considering trace plots.</p>
</div>
<div class="section level2">
<h2 id="updating-the-number-of-latent-classes">Updating the number of latent classes<a class="anchor" aria-label="anchor" href="#updating-the-number-of-latent-classes"></a>
</h2>
<p>Updating the number <span class="math inline">\(C\)</span> of latent classes is done within the Gibbs sampler by executing the following weight-based updating scheme within the second half of the burn-in period<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;It is reasonable to wait a certain number of iterations before the next update to allow for readjustments, which is implemented via the &lt;code&gt;latent_classes$buffer&lt;/code&gt; argument.&lt;/p&gt;"><sup>2</sup></a>:</p>
<ul>
<li><p>We remove class <span class="math inline">\(c\)</span>, if <span class="math inline">\(s_c&lt;\varepsilon_{\text{min}}\)</span>, i.e. if the class weight <span class="math inline">\(s_c\)</span> drops below some threshold <span class="math inline">\(\varepsilon_{\text{min}}\)</span>. This case indicates that class <span class="math inline">\(c\)</span> has a negligible impact on the mixing distribution.</p></li>
<li><p>We split class <span class="math inline">\(c\)</span> into two classes <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>, if <span class="math inline">\(s_c&gt;\varepsilon_\text{max}\)</span>. This case indicates that class <span class="math inline">\(c\)</span> has a high influence on the mixing distribution whose approximation can potentially be improved by increasing the resolution in directions of high variance. Therefore, the class means <span class="math inline">\(b_{c_1}\)</span> and <span class="math inline">\(b_{c_2}\)</span> of the new classes <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are shifted in opposite directions from the class mean <span class="math inline">\(b_c\)</span> of the old class <span class="math inline">\(c\)</span> in the direction of the highest variance.</p></li>
<li><p>We join two classes <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> to one class <span class="math inline">\(c\)</span>, if <span class="math inline">\(\lVert b_{c_1} - b_{c_2} \rVert&lt;\varepsilon_{\text{distmin}}\)</span>, i.e. if the euclidean distance between the class means <span class="math inline">\(b_{c_1}\)</span> and <span class="math inline">\(b_{c_2}\)</span> drops below some threshold <span class="math inline">\(\varepsilon_{\text{distmin}}\)</span>. This case indicates location redundancy which should be repealed. The parameters of <span class="math inline">\(c\)</span> are assigned by adding the values of <span class="math inline">\(s\)</span> from <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> and averaging the values for <span class="math inline">\(b\)</span> and <span class="math inline">\(\Omega\)</span>.</p></li>
</ul>
<p>These rules contain choices on the values for <span class="math inline">\(\varepsilon_{\text{min}}\)</span>, <span class="math inline">\(\varepsilon_{\text{max}}\)</span> and <span class="math inline">\(\varepsilon_{\text{distmin}}\)</span>. The adequate value for <span class="math inline">\(\varepsilon_{\text{distmin}}\)</span> depends on the scale of the parameters. Per default, <strong>RprobitB</strong> sets</p>
<ul>
<li><p><code>epsmin = 0.01</code>,</p></li>
<li><p><code>epsmax = 0.99</code>, and</p></li>
<li><p><code>distmin = 0.1</code>.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">### probit model</span>
<span class="va">p</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/simulate.html" class="external-link">simulate</a></span><span class="op">(</span>form <span class="op">=</span> <span class="va">choice</span> <span class="op">~</span> <span class="va">var</span> <span class="op">|</span> <span class="fl">0</span>, N <span class="op">=</span> <span class="fl">100</span>, T <span class="op">=</span> <span class="fl">10</span>, J <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">m1</span> <span class="op">=</span> <span class="fu"><a href="../reference/mcmc.html">mcmc</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">p</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">### multinomial probit model</span>
<span class="va">mnp</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/simulate.html" class="external-link">simulate</a></span><span class="op">(</span>form <span class="op">=</span> <span class="va">choice</span> <span class="op">~</span> <span class="va">var</span> <span class="op">|</span> <span class="fl">0</span>, N <span class="op">=</span> <span class="fl">100</span>, T <span class="op">=</span> <span class="fl">10</span>, J <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
<span class="va">m2</span> <span class="op">=</span> <span class="fu"><a href="../reference/mcmc.html">mcmc</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">mnp</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">### mixed multinomial probit model</span>
<span class="va">mmnp</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/simulate.html" class="external-link">simulate</a></span><span class="op">(</span>form <span class="op">=</span> <span class="va">choice</span> <span class="op">~</span> <span class="fl">0</span> <span class="op">|</span> <span class="va">var</span>, N <span class="op">=</span> <span class="fl">100</span>, T <span class="op">=</span> <span class="fl">10</span>, J <span class="op">=</span> <span class="fl">3</span>, re <span class="op">=</span> <span class="st">"var"</span><span class="op">)</span>
<span class="va">m3</span> <span class="op">=</span> <span class="fu"><a href="../reference/mcmc.html">mcmc</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">mmnp</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">### latent classes mixed multinomial probit model</span>
<span class="va">lcmmnp</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/simulate.html" class="external-link">simulate</a></span><span class="op">(</span>form <span class="op">=</span> <span class="va">choice</span> <span class="op">~</span> <span class="fl">0</span> <span class="op">|</span> <span class="va">var</span>, N <span class="op">=</span> <span class="fl">100</span>, T <span class="op">=</span> <span class="fl">10</span>, J <span class="op">=</span> <span class="fl">3</span>, re <span class="op">=</span> <span class="st">"var"</span>,
                  parm <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"C"</span> <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="va">m4</span> <span class="op">=</span> <span class="fu"><a href="../reference/mcmc.html">mcmc</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">lcmmnp</span>, latent_classes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"C"</span> <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">### update of latent classes</span>
<span class="va">m5</span> <span class="op">=</span> <span class="fu"><a href="../reference/mcmc.html">mcmc</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">lcmmnp</span>, latent_classes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"update"</span> <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></code></pre></div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Albert:93" class="csl-entry">
Albert, James H., and Siddhartha Chib. 1993. <span>“Bayesian Analysis of Binary and Polychotomous Response Data.”</span> <em>Journal of the American Statistical Association</em> 88.
</div>
<div id="ref-Allenby:98" class="csl-entry">
Allenby, Greg M., and Peter Rossi. 1998. <span>“Marketing Models of Consumer Heterogeneity.”</span> <em>Journal of Econometrics</em> 89.
</div>
<div id="ref-Imai:05" class="csl-entry">
Imai, Kosuke, and David A. van Dyk. 2005. <span>“A Bayesian Analysis of the Multinomial Probit Model Using Marginal Data Augmentation.”</span> <em>Journal of Econometrics</em> 124.
</div>
<div id="ref-McCulloch:94" class="csl-entry">
McCulloch, Robert, and Peter Rossi. 1994. <span>“An Exact Likelihood Analysis of the Multinomial Probit Model.”</span> <em>Journal of Econometrics</em> 64.
</div>
<div id="ref-Nobile:98" class="csl-entry">
Nobile, Agostino. 1998. <span>“A Hybrid Markov Chain for the Bayesian Analysis of the Multinomial Probit Model.”</span> <em>Statistics and Computing</em> 8.
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by <a href="https://loelschlaeger.de" class="external-link">Lennart Oelschläger</a>, <a href="https://de.wikipedia.org/wiki/Dietmar_Bauer" class="external-link">Dietmar Bauer</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.0.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
